{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "812cb313-3beb-4d5c-8318-f111de14c53b",
   "metadata": {},
   "source": [
    "## Outdated Information For Future Reference\n",
    "This file will attempt to perform sentence completion a little differently by utilizing [numpy](https://numpy-ml.readthedocs.io/en/latest/numpy_ml.ngram.goodturing.html) instead of nltk. Other numpy smoothing algorithms can be found [here](https://numpy-ml.readthedocs.io/en/latest/numpy_ml.ngram.html#:~:text=Laplace%20smoothing%20is%20the%20assumption,time%20than%20it%20actually%20does.&text=where%20c(a)%20denotes%20the,n%20%2Dgrams%20in%20the%20corpus.).\n",
    "\n",
    "## Dataset Processing\n",
    "The main goal of this notebook is now to investigate alternative processing methods for our input data. This is necessary so we can more easily add more information to the model.\n",
    "\n",
    "# Input Creation\n",
    "This file will be used to create and preprocess data used for model training. Our model was originally trained on only transcripts of lectures on Khan Academy. These lectures served us well as a proof of concept but introduced biases and limitations due to being spoken text. We aim to alleviate some of this problem by introducing more information to our model. This will primarily be done by expanding the size of the input corpus used to train the model. Many of the additional dataset sources are too large to include directly into our repository. This is due to the 100mb file size limit and 2GB repository size limit for GitHub. To combat this, while still allowing for more data, we will be using this file to download and setup the files locally. Note that some of the datasets being downloaded as part of this file can reach up to 37GB. This may cause performance issues on slower networks. This file also allows for the addition of ebooks to the model. Ebooks to be used in the model training should be added to the Epubs folder under the Datasets directory.\n",
    "\n",
    "## General Process Order\n",
    "1. Download the datasets and unzip if necessary. Additionlly, store each document to its own .txt file if necessary, as is the case with Khan Academy transcripts.\n",
    "2. Load in the dataset from disk.\n",
    "3. Run the standardized cleaning method over the text in each file.\n",
    "4. Store the updated text to disk for use in the primary model training file.\n",
    "\n",
    "## Datasets\n",
    "- [Khan Academy Transcripts](https://www.khanacademy.org/)\n",
    "- [Kaggle Ted Talks Dataset](https://www.kaggle.com/datasets/rounakbanik/ted-talks)\n",
    "- [1.7GB of Project Gutenberg text files](https://zenodo.org/record/3360392#.ZFU5nXbMIuV)\n",
    "- [Bookcorpus part 1: 2GB](https://the-eye.eu/public/AI/pile_preliminary_components/)\n",
    "- [Bookcorpus part 3: 37GB](https://the-eye.eu/public/AI/pile_preliminary_components/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab855025-6763-43e2-9f8d-10d529782ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package names to /home/fassg/nltk_data...\n",
      "[nltk_data]   Package names is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/fassg/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package state_union to\n",
      "[nltk_data]     /home/fassg/nltk_data...\n",
      "[nltk_data]   Package state_union is already up-to-date!\n",
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     /home/fassg/nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n",
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     /home/fassg/nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/fassg/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/fassg/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/fassg/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict\n",
    "import pandas as pd\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "import nltk.data\n",
    "import re\n",
    "import contractions\n",
    "from bs4 import BeautifulSoup\n",
    "import unidecode\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score\n",
    "from sklearn.metrics import f1_score, recall_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import math\n",
    "import numpy as np\n",
    "import copy\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import pickle\n",
    "from nltk.lm.preprocessing import flatten\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "from nltk.util import pad_sequence\n",
    "from nltk.util import everygrams\n",
    "from nltk.lm import MLE\n",
    "from functools import partial\n",
    "import requests\n",
    "import sys\n",
    "import time\n",
    "import urllib.request\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "nltk.download([\n",
    "\"names\",\n",
    "\"stopwords\",\n",
    "\"state_union\",\n",
    "\"twitter_samples\",\n",
    "\"movie_reviews\",\n",
    "\"averaged_perceptron_tagger\",\n",
    "\"vader_lexicon\",\n",
    "\"punkt\",\n",
    "])\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "encoder = LabelEncoder()\n",
    "sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "tqdm.pandas()\n",
    "n = 3\n",
    "input_path = \"Datasets/Input\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72889e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_base(x: str):\n",
    "        # remove any html tags\n",
    "        x = BeautifulSoup(x, \"html.parser\").get_text(separator=\" \")\n",
    "        # # set all to lower\n",
    "        # x = x.lower()\n",
    "        # clean up the contractions\n",
    "        x = contractions.fix(x)\n",
    "        # remove accended characters\n",
    "        x = unidecode.unidecode(x)\n",
    "        # # remove stopwords: https://stackoverflow.com/questions/19560498/faster-way-to-remove-stop-words-in-python\n",
    "        # x = ' '.join([word for word in x.split() if word not in cachedStopWords]) # slower to use word tokenize\n",
    "        # # fix punctuation spacing\n",
    "        # x = re.sub(r'(?<=[\\.\\,\\?])(?=[^\\s])', r' ', x)\n",
    "        # # strip punctuation\n",
    "        # x = re.sub(r'[\\.\\,\\?\\\\\\/\\<\\>\\;\\:\\[\\]\\{\\}]', r'', x)\n",
    "        # strip quotes\n",
    "        # x = x.replace('\\'', '').replace('\\\"', '')\n",
    "        x = x.replace('\\\"', '')\n",
    "        # # remove some actions\n",
    "        # remove_list = ['(Laughter)', '(laughter)', '(Music)', '(music)', '(Music ends)', '(Audience cheers)', '(Applause)', '(Applause ends)', '(Applause continues)', '(Bells)', '(Trumpet)', '(Clears throat)']\n",
    "        # x = ' '.join([word for word in x.split() if word not in remove_list])\n",
    "        # remove extraneous items\n",
    "        x = x.replace(' -- ', '').replace(' .. ', ' ').replace(' ... ', ' ')\n",
    "        # remove extra whitespace\n",
    "        x = ' '.join(x.strip().split())\n",
    "        # # may want to add lematization\n",
    "        # x = ' '.join([lemmatizer.lemmatize(word) for word in x.split()])\n",
    "        # remove some of the extra bracket tags\n",
    "        x = re.sub(r\"\\s{2,}\", \" \", re.sub(r\"[\\(\\[\\{][^\\)\\]\\}]*[\\)\\]\\}]\", \"\", x))\n",
    "        # # Strip newlines\n",
    "        x = re.sub(r\"\\n\", \" \", x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5891ad",
   "metadata": {},
   "source": [
    "## Khan Academy Transcripts\n",
    "These transcripts are currently stored as five separate CSV files. Each CSV file represents a different domain of lectures on the site. Each domain contains multiple lectures, from a variety of courses and subjects. The primary step in this section is to store cleaned versions of each individual transcript to its own text file. The names of each file will be based on the video title and the course of the lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4c6e450-ea38-4118-a66f-1f070b9b9d3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 8261 entries, 0 to 2789\n",
      "Data columns (total 6 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   course       8261 non-null   object\n",
      " 1   unit         8261 non-null   object\n",
      " 2   lesson       8261 non-null   object\n",
      " 3   video_title  8261 non-null   object\n",
      " 4   about        8261 non-null   object\n",
      " 5   transcript   8261 non-null   object\n",
      "dtypes: object(6)\n",
      "memory usage: 451.8+ KB\n"
     ]
    }
   ],
   "source": [
    "computing_df = pd.read_csv(Path(\"Datasets/KhanAcademy/Computing.csv\"))\n",
    "computing_df = computing_df.dropna()\n",
    "\n",
    "economics_df = pd.read_csv(Path(\"Datasets/KhanAcademy/Economics.csv\"))\n",
    "economics_df = economics_df.dropna()\n",
    "\n",
    "humanities_df = pd.read_csv(Path(\"Datasets/KhanAcademy/Humanities.csv\"))\n",
    "humanities_df = humanities_df.dropna()\n",
    "\n",
    "math_df = pd.read_csv(Path(\"Datasets/KhanAcademy/Math.csv\"))\n",
    "math_df = math_df.dropna()\n",
    "\n",
    "science_df = pd.read_csv(Path(\"Datasets/KhanAcademy/Science.csv\"))\n",
    "science_df = science_df.dropna()\n",
    "\n",
    "khan_dfs = [computing_df, economics_df, humanities_df, math_df, science_df]\n",
    "khan = pd.concat(khan_dfs, axis=0)\n",
    "khan.info(verbose=True, show_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5e142aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76101875d34548bd9b538843b017ed6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8261 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "def store_khan_lecture(row):\n",
    "    new_title = f\"khan - {row['course']} - {row['video_title']}\"\n",
    "    new_title = re.sub(r'[\\.\\,\\?\\\\\\/\\<\\>\\;\\:\\[\\]\\{\\}\\!\\\"®︎\\|\\*\\(\\)]', r'', new_title)\n",
    "    new_fp = Path(f\"{input_path}/Khan/{new_title}.txt\")\n",
    "    with open(new_fp, 'w', encoding=\"utf-8\") as f:\n",
    "        # f.write(row['transcript'])\n",
    "        f.write(clean_base(row['transcript']))\n",
    "\n",
    "khan.progress_apply(lambda row: store_khan_lecture(row), axis=1)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f13b44",
   "metadata": {},
   "source": [
    "## Ted Talk Transcripts\n",
    "The Ted Talk dataset consists of two CSV files. Ted main contains information about each of the talks while the ted transcripts contains the transcript information as well as the url to the talk. As with the Khan Academy dataset, we will need to clean each transcript and store it to its own text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94095b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2461 entries, 0 to 2466\n",
      "Data columns (total 19 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   comments            2461 non-null   int64 \n",
      " 1   description         2461 non-null   object\n",
      " 2   duration            2461 non-null   int64 \n",
      " 3   event               2461 non-null   object\n",
      " 4   film_date           2461 non-null   int64 \n",
      " 5   languages           2461 non-null   int64 \n",
      " 6   main_speaker        2461 non-null   object\n",
      " 7   name                2461 non-null   object\n",
      " 8   num_speaker         2461 non-null   int64 \n",
      " 9   published_date      2461 non-null   int64 \n",
      " 10  ratings             2461 non-null   object\n",
      " 11  related_talks       2461 non-null   object\n",
      " 12  speaker_occupation  2461 non-null   object\n",
      " 13  tags                2461 non-null   object\n",
      " 14  title               2461 non-null   object\n",
      " 15  urlurl              2461 non-null   object\n",
      " 16  views               2461 non-null   int64 \n",
      " 17  transcript          2461 non-null   object\n",
      " 18  urlurl              2461 non-null   object\n",
      "dtypes: int64(7), object(12)\n",
      "memory usage: 384.5+ KB\n"
     ]
    }
   ],
   "source": [
    "ted_main = pd.read_csv(\"Datasets/TEDTalksDataset/ted_main.csv\")\n",
    "transcripts = pd.read_csv(\"Datasets/TEDTalksDataset/transcripts.csv\")\n",
    "ted = ted_main.join(transcripts, lsuffix='url', rsuffix='url', sort=True)\n",
    "ted = ted.dropna()\n",
    "ted.info(verbose=True, show_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9612f81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47eb49f4feca4ba9bcc4c4f400af41b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2461 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.7/site-packages/bs4/__init__.py:439: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  MarkupResemblesLocatorWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "def store_ted_lecture(row):\n",
    "    new_title = f\"ted - {row['title']}\"\n",
    "    new_title = re.sub(r'[\\.\\,\\?\\\\\\/\\<\\>\\;\\:\\[\\]\\{\\}\\!\\\"®︎\\|\\*\\(\\)]', r'', new_title)\n",
    "    new_fp = Path(f\"{input_path}/Ted/{new_title}.txt\")\n",
    "    with open(new_fp, 'w', encoding=\"utf-8\") as f:\n",
    "        # f.write(row['transcript'])\n",
    "        f.write(clean_base(row['transcript']))\n",
    "\n",
    "ted.progress_apply(lambda row: store_ted_lecture(row), axis=1)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af157d3",
   "metadata": {},
   "source": [
    "## Method for Reporting Downloads\n",
    "The below method is used to give progress updates on the download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d5b9e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://blog.shichao.io/2012/10/04/progress_speed_indicator_for_urlretrieve_in_python.html\n",
    "def reporthook(count, block_size, total_size):\n",
    "    global start_time\n",
    "    if count == 0:\n",
    "        start_time = time.time()\n",
    "        return\n",
    "    duration = time.time() - start_time\n",
    "    duration = 1 if duration == 0 else duration\n",
    "    progress_size = int(count * block_size)\n",
    "    speed = int(progress_size / (1024 * duration))\n",
    "    percent = int(count * block_size * 100 / total_size)\n",
    "    sys.stdout.write(\"\\r...%d%%, %d MB, %d KB/s, %d seconds passed\" %\n",
    "                     (percent, progress_size / (1024 * 1024), speed, duration))\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0765978e",
   "metadata": {},
   "source": [
    "## Project Gutenberg\n",
    "These files come from Zenodo. More text files can be manually added to this directory as wanted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de1ec882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...6%, 33 MB, 1715 KB/s, 19 seconds passed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...11%, 62 MB, 2759 KB/s, 23 seconds passed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...66%, 357 MB, 7507 KB/s, 48 seconds passed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...78%, 419 MB, 7977 KB/s, 53 seconds passed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...84%, 452 MB, 8191 KB/s, 56 seconds passed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...97%, 524 MB, 8600 KB/s, 62 seconds passed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...100%, 537 MB, 8658 KB/s, 63 seconds passed"
     ]
    }
   ],
   "source": [
    "new_file_path = Path(\"Downloads/D1.7GB.zip\")\n",
    "if not os.path.exists(new_file_path):\n",
    "    url = 'https://zenodo.org/record/3360392/files/D1.7GB.zip?download=1'\n",
    "    urllib.request.urlretrieve(url, new_file_path, reporthook=reporthook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f219a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = 'https://zenodo.org/record/3360392/files/D1.7GB.zip?download=1'\n",
    "# r = requests.get(url, allow_redirects=True)\n",
    "# filename = getFilename_fromCd(r.headers.get('content-disposition'))\n",
    "# # print(f\"downloading {filename}\")\n",
    "# open(Path(f\"Downloads/{filename}\"), 'wb').write(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e90a936",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_dir = Path(\"Datasets/Input/\")\n",
    "shutil.unpack_archive(new_file_path, extract_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adcddcb8",
   "metadata": {},
   "source": [
    "## Bookcorpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36e9da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...13%, 303 MB, 6184 KB/s, 50 seconds passed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...14%, 333 MB, 6440 KB/s, 52 seconds passed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...16%, 369 MB, 6527 KB/s, 57 seconds passed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...17%, 399 MB, 6642 KB/s, 61 seconds passed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...18%, 429 MB, 6659 KB/s, 66 seconds passed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...22%, 512 MB, 6214 KB/s, 84 seconds passed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...25%, 590 MB, 6390 KB/s, 94 seconds passed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...28%, 660 MB, 6420 KB/s, 105 seconds passed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...31%, 722 MB, 6319 KB/s, 116 seconds passed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...34%, 783 MB, 6211 KB/s, 129 seconds passed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...36%, 842 MB, 5941 KB/s, 145 seconds passed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...38%, 893 MB, 5976 KB/s, 153 seconds passed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...63%, 1453 MB, 5685 KB/s, 261 seconds passed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...67%, 1538 MB, 5444 KB/s, 289 seconds passed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...70%, 1614 MB, 5477 KB/s, 301 seconds passed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...81%, 1873 MB, 4864 KB/s, 394 seconds passed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...85%, 1967 MB, 4760 KB/s, 423 seconds passed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...89%, 2054 MB, 4860 KB/s, 432 seconds passed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...93%, 2133 MB, 4834 KB/s, 451 seconds passed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...94%, 2172 MB, 4855 KB/s, 458 seconds passed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...96%, 2209 MB, 4895 KB/s, 462 seconds passed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...97%, 2241 MB, 4934 KB/s, 465 seconds passed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...99%, 2271 MB, 4967 KB/s, 468 seconds passed"
     ]
    }
   ],
   "source": [
    "new_file_path = Path(\"Downloads/books1.tar.gz\")\n",
    "if not os.path.exists(new_file_path):\n",
    "    url = 'https://the-eye.eu/public/AI/pile_preliminary_components/books1.tar.gz'\n",
    "    urllib.request.urlretrieve(url, new_file_path, reporthook=reporthook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e553442",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_dir = Path(\"Datasets/Input/\")\n",
    "shutil.unpack_archive(new_file_path, extract_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "27e40666",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4919fcc21bf84eee9a8f3abd3d7a2e29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17868 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets/Input/books1/epubtxt/the-microworld-miracle.epub.txt failed with exception string index out of range\n",
      "Datasets/Input/books1/epubtxt/a-helping-hand-for-refugees.epub.txt failed with exception string index out of range\n",
      "Datasets/Input/books1/epubtxt/20-soruda-evrim-teorisinin-cokusu.epub.txt failed with exception string index out of range\n",
      "Datasets/Input/books1/epubtxt/the-pkks-treachery-and-oppression.epub.txt failed with exception string index out of range\n",
      "Datasets/Input/books1/epubtxt/gunumuz-cahiliye-toplumunun-adi-konmamis-karanlik-dini-adaml.epub.txt failed with exception string index out of range\n",
      "Datasets/Input/books1/epubtxt/70th-aacc-annual-scientific-meeting.epub.txt failed with exception string index out of range\n",
      "Datasets/Input/books1/epubtxt/the-voyage-edited-by-chandani-lokuge-david-morley.epub.txt failed with exception string index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.7/site-packages/bs4/builder/__init__.py:546: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  XMLParsedAsHTMLWarning.MESSAGE, XMLParsedAsHTMLWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets/Input/books1/epubtxt/sufism.epub.txt failed with exception string index out of range\n",
      "Datasets/Input/books1/epubtxt/allahin-renk-sanati.epub.txt failed with exception string index out of range\n",
      "Datasets/Input/books1/epubtxt/kuran-ile-hayat-nasil-yasanir.epub.txt failed with exception string index out of range\n",
      "Datasets/Input/books1/epubtxt/finance-guide.epub.txt failed with exception string index out of range\n",
      "Datasets/Input/books1/epubtxt/has-the-bible-been-changed-the-reliability-of-the-scriptures.epub.txt failed with exception string index out of range\n",
      "Datasets/Input/books1/epubtxt/the-error-of-the-evolution-of-species.epub.txt failed with exception string index out of range\n",
      "Datasets/Input/books1/epubtxt/istanbul-intrigues.epub.txt failed with exception string index out of range\n",
      "Datasets/Input/books1/epubtxt/allahin-detay-sanati.epub.txt failed with exception string index out of range\n",
      "Datasets/Input/books1/epubtxt/americas-failure-to-perceive-the-pkk.epub.txt failed with exception string index out of range\n",
      "Datasets/Input/books1/epubtxt/200-questions-about-the-bible-and-the-quran-a-comparison-of-.epub.txt failed with exception string index out of range\n",
      "Datasets/Input/books1/epubtxt/200-most-frequently-used-turkish-words-2000-example-sentence.epub.txt failed with exception string index out of range\n",
      "Datasets/Input/books1/epubtxt/mektubat-tercemesi.epub.txt failed with exception string index out of range\n",
      "Datasets/Input/books1/epubtxt/exploring-religion.epub.txt failed with exception string index out of range\n",
      "Datasets/Input/books1/epubtxt/hzrt-mhmmd-mustafa-ss.epub.txt failed with exception string index out of range\n",
      "Datasets/Input/books1/epubtxt/the-mediterranean-reset-geopolitics-in-a-new-age.epub.txt failed with exception string index out of range\n",
      "Datasets/Input/books1/epubtxt/40-konuda-hucre.epub.txt failed with exception string index out of range\n",
      "Datasets/Input/books1/epubtxt/contemplation-in-islam.epub.txt failed with exception string index out of range\n"
     ]
    }
   ],
   "source": [
    "directory_path = Path('Datasets/Input/books1/epubtxt')\n",
    "# documents = []\n",
    "for data_path in tqdm(list(directory_path.glob(\"**/*.txt\"))):\n",
    "    text = \"\"\n",
    "    try:\n",
    "        with open(Path(data_path), \"r\", encoding=\"utf-8\") as f:\n",
    "            text = clean_base(f.read())\n",
    "            # f.write(text)\n",
    "            # documents.append(text)\n",
    "        with open(Path(data_path), \"w\", encoding=\"utf-8\") as f:\n",
    "            # text = clean_base(f.read())\n",
    "            f.write(text)\n",
    "    except Exception as e:\n",
    "        print(f\"{data_path} failed with exception {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dc5cb981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_file_path = Path(\"Downloads/books3.tar.gz\")\n",
    "# if not os.path.exists(new_file_path):\n",
    "#     url = 'https://the-eye.eu/public/AI/pile_preliminary_components/books3.tar.gz'\n",
    "#     urllib.request.urlretrieve(url, new_file_path, reporthook=reporthook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "664b53e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract_dir = Path(\"Datasets/Input/\")\n",
    "# shutil.unpack_archive(new_file_path, extract_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c9b59ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tarfile\n",
    "# tarfile.open(new_file_path).extractall(extract_dir).close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a1adc2",
   "metadata": {},
   "source": [
    "## Determine how to read data back in\n",
    "This section investigates how we can read this data back into the program for use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2d086eb8-5987-443c-ba28-12c3a0a92501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory_path = Path(new_dir)\n",
    "# documents = []\n",
    "# for data_path in tqdm(list(directory_path.glob(\"**/*.txt\"))):\n",
    "#     text = \"\"\n",
    "#     with open(Path(data_path), \"r\", encoding=\"utf-8\") as f:\n",
    "#         text = clean_base(f.read())\n",
    "#         documents.append(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249314aa-9df0-4077-8c31-310720aa613d",
   "metadata": {},
   "source": [
    "Additional text files were sourced from [Project Gutenberg](https://www.gutenberg.org/) indirectly through [Zenodo](https://zenodo.org/record/3360392#.ZFUirnbMIuU).\n",
    "\n",
    "## Ebooks:\n",
    "the below section investigates how to read ebooks with an OCHEM ebook I found online. I investigated two options for this. The first option is more concise, but has issues where it removes the spaces between some words during html parsing. The second option is a little slower but more accurate so we will go with that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1d8fc62f-1c1b-45f3-bfea-108f31fe8598",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fassg/.local/lib/python3.7/site-packages/ebooklib/epub.py:1395: UserWarning: In the future version we will turn default option ignore_ncx to True.\n",
      "  warnings.warn('In the future version we will turn default option ignore_ncx to True.')\n"
     ]
    }
   ],
   "source": [
    "import ebooklib\n",
    "from ebooklib import epub\n",
    "\n",
    "book = epub.read_epub(Path(\"Datasets/Epubs/Organic-Chemistry-I-1639153167.epub\"))\n",
    "\n",
    "text = \"\"\n",
    "for doc in book.get_items_of_type(ebooklib.ITEM_DOCUMENT):\n",
    "    # print(doc.content)\n",
    "    text += clean_base(doc.content)\n",
    "# print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "666ed10d-1d5a-458b-bec2-b237b86b66fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install epub-conversion ebooklib xml_cleaner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d8fd325f-b076-4640-9a57-024f0e82e3e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad19a1c2e1224995bc235d10b76974a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fassg/.local/lib/python3.7/site-packages/epub/__init__.py:140: SyntaxWarning: The ePub does not define any NCX file\n",
      "  SyntaxWarning)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Datasets/Input/Epubs/Organic-Chemistry-I-1639153167.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2840833/761630396.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{input_path}/Epubs/{data_path.stem}.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Datasets/Input/Epubs/Organic-Chemistry-I-1639153167.txt'"
     ]
    }
   ],
   "source": [
    "# from epub_conversion import Converter\n",
    "# converter = Converter(Path(\"Datasets/Epubs/\"))\n",
    "# converter.convert(\"epubs.txt\")\n",
    "from epub_conversion.utils import open_book, convert_epub_to_lines\n",
    "\n",
    "directory_path = Path(\"Datasets/Epubs\")\n",
    "documents = []\n",
    "for data_path in tqdm(list(directory_path.glob(\"**/*.epub\"))):\n",
    "    book = open_book(Path(data_path))\n",
    "    lines = [clean_base(line) for line in convert_epub_to_lines(book)]\n",
    "    lines = [i for i in lines if i != '']\n",
    "    text = '\\n'.join(lines)\n",
    "    with open(Path(f'{input_path}/Epubs/{data_path.stem}.txt'), 'w', encoding='utf-8') as f:\n",
    "        f.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bcd788-8746-493e-ab78-d6e6adfcb83e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fccf102-87e2-4fdc-8545-504c3012c2cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04439fcb-427e-486a-b80c-dbc0abd2cc4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e845f5b8-d661-46fa-ada4-418e3e9bf842",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316cd327-4109-479d-88af-2cd8cfa995e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66c0d8c-f3ae-4425-a176-4dc7bd8a978d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c87e41-5b5a-4a8f-8c18-94771c628e51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6294aaa-c537-4e0d-94bd-31046ecc5c37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe3d3d0-af0d-48f6-acc2-0bdf2a7fd4a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265f9dae-2c20-4767-b104-759b5c99073e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
