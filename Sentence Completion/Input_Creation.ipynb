{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "812cb313-3beb-4d5c-8318-f111de14c53b",
   "metadata": {},
   "source": [
    "## Outdated Information For Future Reference\n",
    "This file will attempt to perform sentence completion a little differently by utilizing [numpy](https://numpy-ml.readthedocs.io/en/latest/numpy_ml.ngram.goodturing.html) instead of nltk. Other numpy smoothing algorithms can be found [here](https://numpy-ml.readthedocs.io/en/latest/numpy_ml.ngram.html#:~:text=Laplace%20smoothing%20is%20the%20assumption,time%20than%20it%20actually%20does.&text=where%20c(a)%20denotes%20the,n%20%2Dgrams%20in%20the%20corpus.).\n",
    "\n",
    "## Dataset Processing\n",
    "The main goal of this notebook is now to investigate alternative processing methods for our input data. This is necessary so we can more easily add more information to the model.\n",
    "\n",
    "# Input Creation\n",
    "This file will be used to create and preprocess data used for model training. Our model was originally trained on only transcripts of lectures on Khan Academy. These lectures served us well as a proof of concept but introduced biases and limitations due to being spoken text. We aim to alleviate some of this problem by introducing more information to our model. This will primarily be done by expanding the size of the input corpus used to train the model. Many of the additional dataset sources are too large to include directly into our repository. This is due to the 100mb file size limit and 2GB repository size limit for GitHub. To combat this, while still allowing for more data, we will be using this file to download and setup the files locally. Note that some of the datasets being downloaded as part of this file can reach up to 37GB. This may cause performance issues on slower networks. This file also allows for the addition of ebooks to the model. Ebooks to be used in the model training should be added to the Epubs folder under the Datasets directory.\n",
    "\n",
    "## General Process Order\n",
    "1. Download the datasets and unzip if necessary. Additionlly, store each document to its own .txt file if necessary, as is the case with Khan Academy transcripts.\n",
    "2. Load in the dataset from disk.\n",
    "3. Run the standardized cleaning method over the text in each file.\n",
    "4. Store the updated text to disk for use in the primary model training file.\n",
    "\n",
    "## Datasets\n",
    "- [Khan Academy Transcripts](https://www.khanacademy.org/)\n",
    "- [Kaggle Ted Talks Dataset](https://www.kaggle.com/datasets/rounakbanik/ted-talks)\n",
    "- [1.7GB of Project Gutenberg text files](https://zenodo.org/record/3360392#.ZFU5nXbMIuV)\n",
    "- [Bookcorpus part 1: 2GB](https://the-eye.eu/public/AI/pile_preliminary_components/)\n",
    "- [Bookcorpus part 3: 37GB](https://the-eye.eu/public/AI/pile_preliminary_components/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ab855025-6763-43e2-9f8d-10d529782ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package names to\n",
      "[nltk_data]     C:\\Users\\Grant\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package names is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Grant\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package state_union to\n",
      "[nltk_data]     C:\\Users\\Grant\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package state_union is already up-to-date!\n",
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     C:\\Users\\Grant\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n",
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\Grant\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Grant\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Grant\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Grant\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict\n",
    "import pandas as pd\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "import nltk.data\n",
    "import re\n",
    "import contractions\n",
    "from bs4 import BeautifulSoup\n",
    "import unidecode\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score\n",
    "from sklearn.metrics import f1_score, recall_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import math\n",
    "import numpy as np\n",
    "import copy\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import pickle\n",
    "from nltk.lm.preprocessing import flatten\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "from nltk.util import pad_sequence\n",
    "from nltk.util import everygrams\n",
    "from nltk.lm import MLE\n",
    "from functools import partial\n",
    "import requests\n",
    "import sys\n",
    "import time\n",
    "import urllib.request\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "nltk.download([\n",
    "\"names\",\n",
    "\"stopwords\",\n",
    "\"state_union\",\n",
    "\"twitter_samples\",\n",
    "\"movie_reviews\",\n",
    "\"averaged_perceptron_tagger\",\n",
    "\"vader_lexicon\",\n",
    "\"punkt\",\n",
    "])\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "encoder = LabelEncoder()\n",
    "sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "tqdm.pandas()\n",
    "n = 3\n",
    "input_path = \"Datasets/Input\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "72889e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_base(x: str):\n",
    "        # remove any html tags\n",
    "        x = BeautifulSoup(x, \"html.parser\").get_text(separator=\" \")\n",
    "        # # set all to lower\n",
    "        # x = x.lower()\n",
    "        # clean up the contractions\n",
    "        x = contractions.fix(x)\n",
    "        # remove accended characters\n",
    "        x = unidecode.unidecode(x)\n",
    "        # # remove stopwords: https://stackoverflow.com/questions/19560498/faster-way-to-remove-stop-words-in-python\n",
    "        # x = ' '.join([word for word in x.split() if word not in cachedStopWords]) # slower to use word tokenize\n",
    "        # # fix punctuation spacing\n",
    "        # x = re.sub(r'(?<=[\\.\\,\\?])(?=[^\\s])', r' ', x)\n",
    "        # # strip punctuation\n",
    "        # x = re.sub(r'[\\.\\,\\?\\\\\\/\\<\\>\\;\\:\\[\\]\\{\\}]', r'', x)\n",
    "        # strip quotes\n",
    "        # x = x.replace('\\'', '').replace('\\\"', '')\n",
    "        x = x.replace('\\\"', '')\n",
    "        # # remove some actions\n",
    "        # remove_list = ['(Laughter)', '(laughter)', '(Music)', '(music)', '(Music ends)', '(Audience cheers)', '(Applause)', '(Applause ends)', '(Applause continues)', '(Bells)', '(Trumpet)', '(Clears throat)']\n",
    "        # x = ' '.join([word for word in x.split() if word not in remove_list])\n",
    "        # remove extraneous items\n",
    "        x = x.replace(' -- ', '').replace(' .. ', ' ').replace(' ... ', ' ')\n",
    "        # remove extra whitespace\n",
    "        x = ' '.join(x.strip().split())\n",
    "        # # may want to add lematization\n",
    "        # x = ' '.join([lemmatizer.lemmatize(word) for word in x.split()])\n",
    "        # remove some of the extra bracket tags\n",
    "        x = re.sub(r\"\\s{2,}\", \" \", re.sub(r\"[\\(\\[\\{][^\\)\\]\\}]*[\\)\\]\\}]\", \"\", x))\n",
    "        # # Strip newlines\n",
    "        x = re.sub(r\"\\n\", \" \", x)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7f5891ad",
   "metadata": {},
   "source": [
    "## Khan Academy Transcripts\n",
    "These transcripts are currently stored as five separate CSV files. Each CSV file represents a different domain of lectures on the site. Each domain contains multiple lectures, from a variety of courses and subjects. The primary step in this section is to store cleaned versions of each individual transcript to its own text file. The names of each file will be based on the video title and the course of the lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f4c6e450-ea38-4118-a66f-1f070b9b9d3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 8261 entries, 0 to 2789\n",
      "Data columns (total 6 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   course       8261 non-null   object\n",
      " 1   unit         8261 non-null   object\n",
      " 2   lesson       8261 non-null   object\n",
      " 3   video_title  8261 non-null   object\n",
      " 4   about        8261 non-null   object\n",
      " 5   transcript   8261 non-null   object\n",
      "dtypes: object(6)\n",
      "memory usage: 451.8+ KB\n"
     ]
    }
   ],
   "source": [
    "computing_df = pd.read_csv(Path(\"Datasets/KhanAcademy/Computing.csv\"))\n",
    "computing_df = computing_df.dropna()\n",
    "\n",
    "economics_df = pd.read_csv(Path(\"Datasets/KhanAcademy/Economics.csv\"))\n",
    "economics_df = economics_df.dropna()\n",
    "\n",
    "humanities_df = pd.read_csv(Path(\"Datasets/KhanAcademy/Humanities.csv\"))\n",
    "humanities_df = humanities_df.dropna()\n",
    "\n",
    "math_df = pd.read_csv(Path(\"Datasets/KhanAcademy/Math.csv\"))\n",
    "math_df = math_df.dropna()\n",
    "\n",
    "science_df = pd.read_csv(Path(\"Datasets/KhanAcademy/Science.csv\"))\n",
    "science_df = science_df.dropna()\n",
    "\n",
    "khan_dfs = [computing_df, economics_df, humanities_df, math_df, science_df]\n",
    "khan = pd.concat(khan_dfs, axis=0)\n",
    "khan.info(verbose=True, show_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f5e142aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a902c74faca242e6b27afea3598032c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8261 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0       None\n",
       "1       None\n",
       "5       None\n",
       "6       None\n",
       "7       None\n",
       "        ... \n",
       "2785    None\n",
       "2786    None\n",
       "2787    None\n",
       "2788    None\n",
       "2789    None\n",
       "Length: 8261, dtype: object"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def store_khan_lecture(row):\n",
    "    new_title = f\"khan - {row['course']} - {row['video_title']}\"\n",
    "    new_title = re.sub(r'[\\.\\,\\?\\\\\\/\\<\\>\\;\\:\\[\\]\\{\\}\\!\\\"®︎\\|\\*\\(\\)]', r'', new_title)\n",
    "    new_fp = Path(f\"{input_path}/Khan/{new_title}.txt\")\n",
    "    with open(new_fp, 'w', encoding=\"utf-8\") as f:\n",
    "        # f.write(row['transcript'])\n",
    "        f.write(clean_base(row['transcript']))\n",
    "\n",
    "khan.progress_apply(lambda row: store_khan_lecture(row), axis=1)\n",
    "print(\"Done\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "06f13b44",
   "metadata": {},
   "source": [
    "## Ted Talk Transcripts\n",
    "The Ted Talk dataset consists of two CSV files. Ted main contains information about each of the talks while the ted transcripts contains the transcript information as well as the url to the talk. As with the Khan Academy dataset, we will need to clean each transcript and store it to its own text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "94095b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2461 entries, 0 to 2466\n",
      "Data columns (total 19 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   comments            2461 non-null   int64 \n",
      " 1   description         2461 non-null   object\n",
      " 2   duration            2461 non-null   int64 \n",
      " 3   event               2461 non-null   object\n",
      " 4   film_date           2461 non-null   int64 \n",
      " 5   languages           2461 non-null   int64 \n",
      " 6   main_speaker        2461 non-null   object\n",
      " 7   name                2461 non-null   object\n",
      " 8   num_speaker         2461 non-null   int64 \n",
      " 9   published_date      2461 non-null   int64 \n",
      " 10  ratings             2461 non-null   object\n",
      " 11  related_talks       2461 non-null   object\n",
      " 12  speaker_occupation  2461 non-null   object\n",
      " 13  tags                2461 non-null   object\n",
      " 14  title               2461 non-null   object\n",
      " 15  urlurl              2461 non-null   object\n",
      " 16  views               2461 non-null   int64 \n",
      " 17  transcript          2461 non-null   object\n",
      " 18  urlurl              2461 non-null   object\n",
      "dtypes: int64(7), object(12)\n",
      "memory usage: 384.5+ KB\n"
     ]
    }
   ],
   "source": [
    "ted_main = pd.read_csv(\"Datasets\\\\TEDTalksDataset\\\\ted_main.csv\")\n",
    "transcripts = pd.read_csv(\"Datasets\\\\TEDTalksDataset\\\\transcripts.csv\")\n",
    "ted = ted_main.join(transcripts, lsuffix='url', rsuffix='url', sort=True)\n",
    "ted = ted.dropna()\n",
    "ted.info(verbose=True, show_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a9612f81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7ba402ae8c945669b785afea5e9c535",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2461 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Grant\\AppData\\Local\\Temp\\ipykernel_9896\\2807408775.py:3: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  x = BeautifulSoup(x, \"html.parser\").get_text(separator=\" \")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "def store_ted_lecture(row):\n",
    "    new_title = f\"ted - {row['title']}\"\n",
    "    new_title = re.sub(r'[\\.\\,\\?\\\\\\/\\<\\>\\;\\:\\[\\]\\{\\}\\!\\\"®︎\\|\\*\\(\\)]', r'', new_title)\n",
    "    new_fp = Path(f\"{input_path}/Ted/{new_title}.txt\")\n",
    "    with open(new_fp, 'w', encoding=\"utf-8\") as f:\n",
    "        # f.write(row['transcript'])\n",
    "        f.write(clean_base(row['transcript']))\n",
    "\n",
    "ted.progress_apply(lambda row: store_ted_lecture(row), axis=1)\n",
    "print(\"Done\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6af157d3",
   "metadata": {},
   "source": [
    "## Method for Reporting Downloads\n",
    "The below method is used to give progress updates on the download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6d5b9e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://blog.shichao.io/2012/10/04/progress_speed_indicator_for_urlretrieve_in_python.html\n",
    "def reporthook(count, block_size, total_size):\n",
    "    global start_time\n",
    "    if count == 0:\n",
    "        start_time = time.time()\n",
    "        return\n",
    "    duration = time.time() - start_time\n",
    "    duration = 1 if duration == 0 else duration\n",
    "    progress_size = int(count * block_size)\n",
    "    speed = int(progress_size / (1024 * duration))\n",
    "    percent = int(count * block_size * 100 / total_size)\n",
    "    sys.stdout.write(\"\\r...%d%%, %d MB, %d KB/s, %d seconds passed\" %\n",
    "                     (percent, progress_size / (1024 * 1024), speed, duration))\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0765978e",
   "metadata": {},
   "source": [
    "## Project Gutenberg\n",
    "These files come from Zenodo. More text files can be manually added to this directory as wanted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "de1ec882",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_file_path = Path(\"Downloads/D1.7GB.zip\")\n",
    "if not os.path.exists(new_file_path):\n",
    "    url = 'https://zenodo.org/record/3360392/files/D1.7GB.zip?download=1'\n",
    "    urllib.request.urlretrieve(url, new_file_path, reporthook=reporthook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3f219a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = 'https://zenodo.org/record/3360392/files/D1.7GB.zip?download=1'\n",
    "# r = requests.get(url, allow_redirects=True)\n",
    "# filename = getFilename_fromCd(r.headers.get('content-disposition'))\n",
    "# # print(f\"downloading {filename}\")\n",
    "# open(Path(f\"Downloads/{filename}\"), 'wb').write(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8e90a936",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_dir = Path(\"Datasets/Input/\")\n",
    "shutil.unpack_archive(new_file_path, extract_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "adcddcb8",
   "metadata": {},
   "source": [
    "## Bookcorpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b36e9da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_file_path = Path(\"Downloads/books1.tar.gz\")\n",
    "if not os.path.exists(new_file_path):\n",
    "    url = 'https://the-eye.eu/public/AI/pile_preliminary_components/books1.tar.gz'\n",
    "    urllib.request.urlretrieve(url, new_file_path, reporthook=reporthook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8e553442",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_dir = Path(\"Datasets/Input/\")\n",
    "shutil.unpack_archive(new_file_path, extract_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "27e40666",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ceabdd5a61141d283b2bef4bc96c347",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17868 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets\\Input\\books1\\epubtxt\\20-soruda-evrim-teorisinin-cokusu.epub.txt failed with exception string index out of range\n",
      "Datasets\\Input\\books1\\epubtxt\\200-most-frequently-used-turkish-words-2000-example-sentence.epub.txt failed with exception string index out of range\n",
      "Datasets\\Input\\books1\\epubtxt\\200-questions-about-the-bible-and-the-quran-a-comparison-of-.epub.txt failed with exception string index out of range\n",
      "Datasets\\Input\\books1\\epubtxt\\40-konuda-hucre.epub.txt failed with exception string index out of range\n",
      "Datasets\\Input\\books1\\epubtxt\\70th-aacc-annual-scientific-meeting.epub.txt failed with exception string index out of range\n",
      "Datasets\\Input\\books1\\epubtxt\\a-helping-hand-for-refugees.epub.txt failed with exception string index out of range\n",
      "Datasets\\Input\\books1\\epubtxt\\allahin-detay-sanati.epub.txt failed with exception string index out of range\n",
      "Datasets\\Input\\books1\\epubtxt\\allahin-renk-sanati.epub.txt failed with exception string index out of range\n",
      "Datasets\\Input\\books1\\epubtxt\\americas-failure-to-perceive-the-pkk.epub.txt failed with exception string index out of range\n",
      "Datasets\\Input\\books1\\epubtxt\\contemplation-in-islam.epub.txt failed with exception string index out of range\n",
      "Datasets\\Input\\books1\\epubtxt\\exploring-religion.epub.txt failed with exception string index out of range\n",
      "Datasets\\Input\\books1\\epubtxt\\finance-guide.epub.txt failed with exception string index out of range\n",
      "Datasets\\Input\\books1\\epubtxt\\gunumuz-cahiliye-toplumunun-adi-konmamis-karanlik-dini-adaml.epub.txt failed with exception string index out of range\n",
      "Datasets\\Input\\books1\\epubtxt\\has-the-bible-been-changed-the-reliability-of-the-scriptures.epub.txt failed with exception string index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets\\Input\\books1\\epubtxt\\hzrt-mhmmd-mustafa-ss.epub.txt failed with exception string index out of range\n",
      "Datasets\\Input\\books1\\epubtxt\\istanbul-intrigues.epub.txt failed with exception string index out of range\n",
      "Datasets\\Input\\books1\\epubtxt\\kuran-ile-hayat-nasil-yasanir.epub.txt failed with exception string index out of range\n",
      "Datasets\\Input\\books1\\epubtxt\\mektubat-tercemesi.epub.txt failed with exception string index out of range\n",
      "Datasets\\Input\\books1\\epubtxt\\sufism.epub.txt failed with exception string index out of range\n",
      "Datasets\\Input\\books1\\epubtxt\\the-error-of-the-evolution-of-species.epub.txt failed with exception string index out of range\n",
      "Datasets\\Input\\books1\\epubtxt\\the-mediterranean-reset-geopolitics-in-a-new-age.epub.txt failed with exception string index out of range\n",
      "Datasets\\Input\\books1\\epubtxt\\the-microworld-miracle.epub.txt failed with exception string index out of range\n",
      "Datasets\\Input\\books1\\epubtxt\\the-pkks-treachery-and-oppression.epub.txt failed with exception string index out of range\n",
      "Datasets\\Input\\books1\\epubtxt\\the-voyage-edited-by-chandani-lokuge-david-morley.epub.txt failed with exception string index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Grant\\AppData\\Local\\Temp\\ipykernel_9896\\2807408775.py:3: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  x = BeautifulSoup(x, \"html.parser\").get_text(separator=\" \")\n"
     ]
    }
   ],
   "source": [
    "directory_path = Path('Datasets/Input/books1/epubtxt')\n",
    "# documents = []\n",
    "for data_path in tqdm(list(directory_path.glob(\"**/*.txt\"))):\n",
    "    text = \"\"\n",
    "    try:\n",
    "        with open(Path(data_path), \"r\", encoding=\"utf-8\") as f:\n",
    "            text = clean_base(f.read())\n",
    "            # f.write(text)\n",
    "            # documents.append(text)\n",
    "        with open(Path(data_path), \"w\", encoding=\"utf-8\") as f:\n",
    "            # text = clean_base(f.read())\n",
    "            f.write(text)\n",
    "    except Exception as e:\n",
    "        print(f\"{data_path} failed with exception {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "dc5cb981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_file_path = Path(\"Downloads/books3.tar.gz\")\n",
    "# if not os.path.exists(new_file_path):\n",
    "#     url = 'https://the-eye.eu/public/AI/pile_preliminary_components/books3.tar.gz'\n",
    "#     urllib.request.urlretrieve(url, new_file_path, reporthook=reporthook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "664b53e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract_dir = Path(\"Datasets/Input/\")\n",
    "# shutil.unpack_archive(new_file_path, extract_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c9b59ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tarfile\n",
    "# tarfile.open(new_file_path).extractall(extract_dir).close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "61a1adc2",
   "metadata": {},
   "source": [
    "## Determine how to read data back in\n",
    "This section investigates how we can read this data back into the program for use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d086eb8-5987-443c-ba28-12c3a0a92501",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bd1f48d4545492e890c96d6c72f4341",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8067 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "directory_path = Path(new_dir)\n",
    "documents = []\n",
    "for data_path in tqdm(list(directory_path.glob(\"**/*.txt\"))):\n",
    "    text = \"\"\n",
    "    with open(Path(data_path), \"r\", encoding=\"utf-8\") as f:\n",
    "        text = clean_base(f.read())\n",
    "        documents.append(text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "249314aa-9df0-4077-8c31-310720aa613d",
   "metadata": {},
   "source": [
    "Additional text files were sourced from [Project Gutenberg](https://www.gutenberg.org/) indirectly through [Zenodo](https://zenodo.org/record/3360392#.ZFUirnbMIuU).\n",
    "\n",
    "## Ebooks:\n",
    "the below section investigates how to read ebooks with an OCHEM ebook I found online. I investigated two options for this. The first option is more concise, but has issues where it removes the spaces between some words during html parsing. The second option is a little slower but more accurate so we will go with that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1d8fc62f-1c1b-45f3-bfea-108f31fe8598",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\ebooklib\\epub.py:1395: UserWarning: In the future version we will turn default option ignore_ncx to True.\n",
      "  warnings.warn('In the future version we will turn default option ignore_ncx to True.')\n"
     ]
    }
   ],
   "source": [
    "import ebooklib\n",
    "from ebooklib import epub\n",
    "\n",
    "book = epub.read_epub(Path(\"Datasets/Epubs/Organic-Chemistry-I-1639153167.epub\"))\n",
    "\n",
    "text = \"\"\n",
    "for doc in book.get_items_of_type(ebooklib.ITEM_DOCUMENT):\n",
    "    # print(doc.content)\n",
    "    text += clean_base(doc.content)\n",
    "# print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "666ed10d-1d5a-458b-bec2-b237b86b66fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: epub-conversion in c:\\python310\\lib\\site-packages (1.0.15)\n",
      "Requirement already satisfied: ebooklib in c:\\python310\\lib\\site-packages (0.18)\n",
      "Collecting xml_cleaner\n",
      "  Downloading xml-cleaner-2.0.4.tar.gz (10 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: bz2file in c:\\python310\\lib\\site-packages (from epub-conversion) (0.98)\n",
      "Requirement already satisfied: epub in c:\\python310\\lib\\site-packages (from epub-conversion) (0.5.2)\n",
      "Requirement already satisfied: ciseau in c:\\python310\\lib\\site-packages (from epub-conversion) (1.0.1)\n",
      "Requirement already satisfied: lxml in c:\\python310\\lib\\site-packages (from ebooklib) (4.9.2)\n",
      "Requirement already satisfied: six in c:\\python310\\lib\\site-packages (from ebooklib) (1.16.0)\n",
      "Installing collected packages: xml_cleaner\n",
      "  Running setup.py install for xml_cleaner: started\n",
      "  Running setup.py install for xml_cleaner: finished with status 'done'\n",
      "Successfully installed xml_cleaner-2.0.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  DEPRECATION: xml_cleaner is being installed using the legacy 'setup.py install' method, because it does not have a 'pyproject.toml' and the 'wheel' package is not installed. pip 23.1 will enforce this behaviour change. A possible replacement is to enable the '--use-pep517' option. Discussion can be found at https://github.com/pypa/pip/issues/8559\n",
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 23.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# !pip install epub-conversion ebooklib xml_cleaner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d8fd325f-b076-4640-9a57-024f0e82e3e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2360190444f40af907f4c91e057bed2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\epub\\__init__.py:139: SyntaxWarning: The ePub does not define any NCX file\n",
      "  warnings.warn('The ePub does not define any NCX file',\n"
     ]
    }
   ],
   "source": [
    "# from epub_conversion import Converter\n",
    "# converter = Converter(Path(\"Datasets/Epubs/\"))\n",
    "# converter.convert(\"epubs.txt\")\n",
    "from epub_conversion.utils import open_book, convert_epub_to_lines\n",
    "\n",
    "directory_path = Path(\"Datasets/Epubs\")\n",
    "documents = []\n",
    "for data_path in tqdm(list(directory_path.glob(\"**/*.epub\"))):\n",
    "    book = open_book(Path(data_path))\n",
    "    lines = [clean_base(line) for line in convert_epub_to_lines(book)]\n",
    "    lines = [i for i in lines if i != '']\n",
    "    text = '\\n'.join(lines)\n",
    "    with open(Path(f'{input_path}/Epubs/{data_path.stem}.txt'), 'w', encoding='utf-8') as f:\n",
    "        f.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1f3b74-625d-4004-95a0-25cf6b87da01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bcd788-8746-493e-ab78-d6e6adfcb83e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fccf102-87e2-4fdc-8545-504c3012c2cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04439fcb-427e-486a-b80c-dbc0abd2cc4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e845f5b8-d661-46fa-ada4-418e3e9bf842",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316cd327-4109-479d-88af-2cd8cfa995e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66c0d8c-f3ae-4425-a176-4dc7bd8a978d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c87e41-5b5a-4a8f-8c18-94771c628e51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6294aaa-c537-4e0d-94bd-31046ecc5c37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe3d3d0-af0d-48f6-acc2-0bdf2a7fd4a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265f9dae-2c20-4767-b104-759b5c99073e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
