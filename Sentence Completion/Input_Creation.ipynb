{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "812cb313-3beb-4d5c-8318-f111de14c53b",
   "metadata": {},
   "source": [
    "## Outdated Information For Future Reference\n",
    "This file will attempt to perform sentence completion a little differently by utilizing [numpy](https://numpy-ml.readthedocs.io/en/latest/numpy_ml.ngram.goodturing.html) instead of nltk. Other numpy smoothing algorithms can be found [here](https://numpy-ml.readthedocs.io/en/latest/numpy_ml.ngram.html#:~:text=Laplace%20smoothing%20is%20the%20assumption,time%20than%20it%20actually%20does.&text=where%20c(a)%20denotes%20the,n%20%2Dgrams%20in%20the%20corpus.).\n",
    "\n",
    "## Dataset Processing\n",
    "The main goal of this notebook is now to investigate alternative processing methods for our input data. This is necessary so we can more easily add more information to the model.\n",
    "\n",
    "# Input Creation\n",
    "This file will be used to create and preprocess data used for model training. Our model was originally trained on only transcripts of lectures on Khan Academy. These lectures served us well as a proof of concept but introduced biases and limitations due to being spoken text. We aim to alleviate some of this problem by introducing more information to our model. This will primarily be done by expanding the size of the input corpus used to train the model. Many of the additional dataset sources are too large to include directly into our repository. This is due to the 100mb file size limit and 2GB repository size limit for GitHub. To combat this, while still allowing for more data, we will be using this file to download and setup the files locally. Note that some of the datasets being downloaded as part of this file can reach up to 37GB. This may cause performance issues on slower networks. This file also allows for the addition of ebooks to the model. Ebooks to be used in the model training should be added to the Epubs folder under the Datasets directory.\n",
    "\n",
    "## General Process Order\n",
    "1. Download the datasets and unzip if necessary. Additionlly, store each document to its own .txt file if necessary, as is the case with Khan Academy transcripts.\n",
    "2. Load in the dataset from disk.\n",
    "3. Run the standardized cleaning method over the text in each file.\n",
    "4. Store the updated text to disk for use in the primary model training file.\n",
    "\n",
    "## Datasets\n",
    "- [Khan Academy Transcripts](https://www.khanacademy.org/)\n",
    "- [Kaggle Ted Talks Dataset](https://www.kaggle.com/datasets/rounakbanik/ted-talks)\n",
    "- [1.7GB of Project Gutenberg text files](https://zenodo.org/record/3360392#.ZFU5nXbMIuV)\n",
    "- [Bookcorpus part 1: 2GB](https://the-eye.eu/public/AI/pile_preliminary_components/)\n",
    "- [Bookcorpus part 3: 37GB](https://the-eye.eu/public/AI/pile_preliminary_components/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab855025-6763-43e2-9f8d-10d529782ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package names to\n",
      "[nltk_data]     C:\\Users\\Grant\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package names is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Grant\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package state_union to\n",
      "[nltk_data]     C:\\Users\\Grant\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package state_union is already up-to-date!\n",
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     C:\\Users\\Grant\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n",
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\Grant\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Grant\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Grant\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Grant\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict\n",
    "import pandas as pd\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "import nltk.data\n",
    "import re\n",
    "import contractions\n",
    "from bs4 import BeautifulSoup\n",
    "import unidecode\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score\n",
    "from sklearn.metrics import f1_score, recall_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import math\n",
    "import numpy as np\n",
    "import copy\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import pickle\n",
    "from nltk.lm.preprocessing import flatten\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "from nltk.util import pad_sequence\n",
    "from nltk.util import everygrams\n",
    "from nltk.lm import MLE\n",
    "from functools import partial\n",
    "\n",
    "nltk.download([\n",
    "\"names\",\n",
    "\"stopwords\",\n",
    "\"state_union\",\n",
    "\"twitter_samples\",\n",
    "\"movie_reviews\",\n",
    "\"averaged_perceptron_tagger\",\n",
    "\"vader_lexicon\",\n",
    "\"punkt\",\n",
    "])\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "encoder = LabelEncoder()\n",
    "sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "tqdm.pandas()\n",
    "n = 3\n",
    "new_dir = \"Datasets/Input\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "72889e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_base(x: str):\n",
    "        # remove any html tags\n",
    "        x = BeautifulSoup(x, \"html.parser\").get_text(separator=\" \")\n",
    "        # # set all to lower\n",
    "        # x = x.lower()\n",
    "        # clean up the contractions\n",
    "        x = contractions.fix(x)\n",
    "        # remove accended characters\n",
    "        x = unidecode.unidecode(x)\n",
    "        # # remove stopwords: https://stackoverflow.com/questions/19560498/faster-way-to-remove-stop-words-in-python\n",
    "        # x = ' '.join([word for word in x.split() if word not in cachedStopWords]) # slower to use word tokenize\n",
    "        # # fix punctuation spacing\n",
    "        # x = re.sub(r'(?<=[\\.\\,\\?])(?=[^\\s])', r' ', x)\n",
    "        # # strip punctuation\n",
    "        # x = re.sub(r'[\\.\\,\\?\\\\\\/\\<\\>\\;\\:\\[\\]\\{\\}]', r'', x)\n",
    "        # strip quotes\n",
    "        # x = x.replace('\\'', '').replace('\\\"', '')\n",
    "        x = x.replace('\\\"', '')\n",
    "        # # remove some actions\n",
    "        # remove_list = ['(Laughter)', '(laughter)', '(Music)', '(music)', '(Music ends)', '(Audience cheers)', '(Applause)', '(Applause ends)', '(Applause continues)', '(Bells)', '(Trumpet)', '(Clears throat)']\n",
    "        # x = ' '.join([word for word in x.split() if word not in remove_list])\n",
    "        # remove extraneous items\n",
    "        x = x.replace(' -- ', '').replace(' .. ', ' ').replace(' ... ', ' ')\n",
    "        # remove extra whitespace\n",
    "        x = ' '.join(x.strip().split())\n",
    "        # # may want to add lematization\n",
    "        # x = ' '.join([lemmatizer.lemmatize(word) for word in x.split()])\n",
    "        # remove some of the extra bracket tags\n",
    "        x = re.sub(r\"\\s{2,}\", \" \", re.sub(r\"[\\(\\[\\{][^\\)\\]\\}]*[\\)\\]\\}]\", \"\", x))\n",
    "        # # Strip newlines\n",
    "        x = re.sub(r\"\\n\", \" \", x)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7f5891ad",
   "metadata": {},
   "source": [
    "## Convert Khan\n",
    "The first step is to change how we are currently storing the khan academy data. Instead of storing the transcripts inside of csv files, or as one newline delmited text file, we instead will be creating a new text file for each transcript. These files will all be stored in the same directory so we can load them all with glob. The names of each file will be based on the video title and the course of the lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4c6e450-ea38-4118-a66f-1f070b9b9d3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 8261 entries, 0 to 2789\n",
      "Data columns (total 6 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   course       8261 non-null   object\n",
      " 1   unit         8261 non-null   object\n",
      " 2   lesson       8261 non-null   object\n",
      " 3   video_title  8261 non-null   object\n",
      " 4   about        8261 non-null   object\n",
      " 5   transcript   8261 non-null   object\n",
      "dtypes: object(6)\n",
      "memory usage: 451.8+ KB\n"
     ]
    }
   ],
   "source": [
    "computing_df = pd.read_csv(Path(\"Datasets/KhanAcademy/Computing.csv\"))\n",
    "computing_df = computing_df.dropna()\n",
    "\n",
    "economics_df = pd.read_csv(Path(\"Datasets/KhanAcademy/Economics.csv\"))\n",
    "economics_df = economics_df.dropna()\n",
    "\n",
    "humanities_df = pd.read_csv(Path(\"Datasets/KhanAcademy/Humanities.csv\"))\n",
    "humanities_df = humanities_df.dropna()\n",
    "\n",
    "math_df = pd.read_csv(Path(\"Datasets/KhanAcademy/Math.csv\"))\n",
    "math_df = math_df.dropna()\n",
    "\n",
    "science_df = pd.read_csv(Path(\"Datasets/KhanAcademy/Science.csv\"))\n",
    "science_df = science_df.dropna()\n",
    "\n",
    "khan_dfs = [computing_df, economics_df, humanities_df, math_df, science_df]\n",
    "khan = pd.concat(khan_dfs, axis=0)\n",
    "khan.info(verbose=True, show_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5e142aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f79f56ea4b64e04adebb15a24ba5a71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8261 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0       None\n",
       "1       None\n",
       "5       None\n",
       "6       None\n",
       "7       None\n",
       "        ... \n",
       "2785    None\n",
       "2786    None\n",
       "2787    None\n",
       "2788    None\n",
       "2789    None\n",
       "Length: 8261, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def store_khan_lecture(row):\n",
    "    new_title = f\"khan - {row['course']} - {row['video_title']}\"\n",
    "    new_title = re.sub(r'[\\.\\,\\?\\\\\\/\\<\\>\\;\\:\\[\\]\\{\\}\\!\\\"®︎\\|\\*\\(\\)]', r'', new_title)\n",
    "    new_fp = Path(f\"{new_dir}/{new_title}.txt\")\n",
    "    with open(new_fp, 'w', encoding=\"utf-8\") as f:\n",
    "        f.write(row['transcript'])\n",
    "        # f.write(clean_base(lecture['transcript']))\n",
    "\n",
    "khan.progress_apply(lambda row: store_khan_lecture(row), axis=1)\n",
    "# for lecture in khan.iloc:\n",
    "#     store_khan_lecture(lecture)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "61a1adc2",
   "metadata": {},
   "source": [
    "## Determine how to read data back in\n",
    "This section investigates how we can read this data back into the program for use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d086eb8-5987-443c-ba28-12c3a0a92501",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bd1f48d4545492e890c96d6c72f4341",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8067 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "directory_path = Path(new_dir)\n",
    "documents = []\n",
    "for data_path in tqdm(list(directory_path.glob(\"**/*.txt\"))):\n",
    "    text = \"\"\n",
    "    with open(Path(data_path), \"r\", encoding=\"utf-8\") as f:\n",
    "        text = clean_base(f.read())\n",
    "        documents.append(text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "249314aa-9df0-4077-8c31-310720aa613d",
   "metadata": {},
   "source": [
    "Additional text files were sourced from [Project Gutenberg](https://www.gutenberg.org/) indirectly through [Zenodo](https://zenodo.org/record/3360392#.ZFUirnbMIuU).\n",
    "\n",
    "## Ebooks:\n",
    "the below section investigates how to read ebooks with an OCHEM ebook I found online. I investigated two options for this. The first option is more concise, but has issues where it removes the spaces between some words during html parsing. The second option is a little slower but more accurate so we will go with that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1d8fc62f-1c1b-45f3-bfea-108f31fe8598",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\ebooklib\\epub.py:1395: UserWarning: In the future version we will turn default option ignore_ncx to True.\n",
      "  warnings.warn('In the future version we will turn default option ignore_ncx to True.')\n"
     ]
    }
   ],
   "source": [
    "import ebooklib\n",
    "from ebooklib import epub\n",
    "\n",
    "book = epub.read_epub(Path(\"Datasets/Epubs/Organic-Chemistry-I-1639153167.epub\"))\n",
    "\n",
    "text = \"\"\n",
    "for doc in book.get_items_of_type(ebooklib.ITEM_DOCUMENT):\n",
    "    # print(doc.content)\n",
    "    text += clean_base(doc.content)\n",
    "# print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "666ed10d-1d5a-458b-bec2-b237b86b66fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: epub-conversion in c:\\python310\\lib\\site-packages (1.0.15)\n",
      "Requirement already satisfied: ebooklib in c:\\python310\\lib\\site-packages (0.18)\n",
      "Collecting xml_cleaner\n",
      "  Downloading xml-cleaner-2.0.4.tar.gz (10 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: bz2file in c:\\python310\\lib\\site-packages (from epub-conversion) (0.98)\n",
      "Requirement already satisfied: epub in c:\\python310\\lib\\site-packages (from epub-conversion) (0.5.2)\n",
      "Requirement already satisfied: ciseau in c:\\python310\\lib\\site-packages (from epub-conversion) (1.0.1)\n",
      "Requirement already satisfied: lxml in c:\\python310\\lib\\site-packages (from ebooklib) (4.9.2)\n",
      "Requirement already satisfied: six in c:\\python310\\lib\\site-packages (from ebooklib) (1.16.0)\n",
      "Installing collected packages: xml_cleaner\n",
      "  Running setup.py install for xml_cleaner: started\n",
      "  Running setup.py install for xml_cleaner: finished with status 'done'\n",
      "Successfully installed xml_cleaner-2.0.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  DEPRECATION: xml_cleaner is being installed using the legacy 'setup.py install' method, because it does not have a 'pyproject.toml' and the 'wheel' package is not installed. pip 23.1 will enforce this behaviour change. A possible replacement is to enable the '--use-pep517' option. Discussion can be found at https://github.com/pypa/pip/issues/8559\n",
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 23.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# !pip install epub-conversion ebooklib xml_cleaner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d8fd325f-b076-4640-9a57-024f0e82e3e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3108332cce3e4c8d9267706f21e2ccae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\epub\\__init__.py:139: SyntaxWarning: The ePub does not define any NCX file\n",
      "  warnings.warn('The ePub does not define any NCX file',\n"
     ]
    }
   ],
   "source": [
    "# from epub_conversion import Converter\n",
    "# converter = Converter(Path(\"Datasets/Epubs/\"))\n",
    "# converter.convert(\"epubs.txt\")\n",
    "from epub_conversion.utils import open_book, convert_epub_to_lines\n",
    "\n",
    "directory_path = Path(\"Datasets/Epubs\")\n",
    "documents = []\n",
    "for data_path in tqdm(list(directory_path.glob(\"**/*.epub\"))):\n",
    "    book = open_book(Path(data_path))\n",
    "    lines = [clean_base(line) for line in convert_epub_to_lines(book)]\n",
    "    lines = [i for i in lines if i != '']\n",
    "    text = '\\n'.join(lines)\n",
    "    with open(Path(f'{new_dir}/{data_path.stem}.txt'), 'w', encoding='utf-8') as f:\n",
    "        f.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1f3b74-625d-4004-95a0-25cf6b87da01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bcd788-8746-493e-ab78-d6e6adfcb83e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fccf102-87e2-4fdc-8545-504c3012c2cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04439fcb-427e-486a-b80c-dbc0abd2cc4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e845f5b8-d661-46fa-ada4-418e3e9bf842",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316cd327-4109-479d-88af-2cd8cfa995e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66c0d8c-f3ae-4425-a176-4dc7bd8a978d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c87e41-5b5a-4a8f-8c18-94771c628e51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6294aaa-c537-4e0d-94bd-31046ecc5c37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe3d3d0-af0d-48f6-acc2-0bdf2a7fd4a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265f9dae-2c20-4767-b104-759b5c99073e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
