- [Voiceover] Hello everyone. So here, I'd like to talk
about harmonic functions. Now, harmonic functions are a very special kind of multivariable function, and they're defined in
terms of the Laplacian, which I've been talking about in the last few videos. So the Laplacian, which we denote with this upper right-side-up triangle, is an operator that you might take on a multivariable function. So it might have two inputs, it could have, you know, a hundred inputs, just some kind of multivariable function with a scalar output. And I talked about it
in the last few videos, but as a reminder, it's defined as the divergence of the gradient of F, and it's kind of like
the second derivative. It's sort of the way to extend the idea of the second derivative into multiple dimensions. Now, what a harmonic function is, is one where the Laplacian
is equal to zero. And it's equal to zero at every possible input point. And sometimes the way that people write this to distinguish it, they'll make a kind of triple equals sign, maybe saying like, "Equivalent to zero." And this is really just
a way of emphasizing that it's equal to zero at all possible input points. It's, you know, not an equation that you're solving for
the specific X and Y, where it equals zero. It's a statement about the function. And to get our head around this, because it's kind of a, you know, as you're just starting to learn about the Laplacian, it's hard to just immediately see the intuition for what this means, let's think about what it means for a single variable function. If you just have some single
variable function of X, and you're looking at
its second derivative, which is kind of the
analog of the Laplacian, what does it mean if that's equal to zero? Well, we can integrate it, we can take the antiderivative, and say, that means that
the single derivative of F, well let's see, what functions have a derivative that's zero? The only functions are the constant ones, so C is just gonna mean
some constant here. And if you integrate that again, say what function has, as its derivative, a constant, well, it's gonna be that constant times X, plus some other constant, some other constant K. So basically, linear functions. So if you're thinking of a graph, it's just something that's got a line, line passing through it like that. And this should kind of make sense if you think of the
geometric interpretation for the second derivative. Because if you're just
looking at a random, you know, arbitrary function that's kind of curving as it does, the second derivative is negative when this concavity is down. So this right here would be a point where the second derivative, it's not zero, it's negative. And over here, when the concavity is up, and it's got a sort of bowl shape, that's where the second
derivative is positive. So if we're saying that
the second derivative has to always be zero, then it can't curve down
and it can't curve up, and it can't do that anywhere, so basically there's no curving allowed, so whatever direction it starts at, it's not allowed to curve, so it just sticks straight like that. But once we extend this to the idea of a multivariable function, things can get a lot more interesting than just a straight line. So an example, I've got the graph here of a multivariable function that happens to be harmonic. So the graph that you're looking at, this is of a two variable function, and the function specifically is F of XY, XY, is equal to E to the X multiplied by sine of Y. And as we're looking at the graph here, hopefully it makes a little bit of sense why this is sort of an E to the X sine of Y pattern. 'Cause as we're moving in
the positive X direction, this here is the positive X direction, you have this exponential shape, and this corresponds with
the fact that over here we've got an E to the X, so as you move X, it kinda looks like E to the X, and it's being multiplied by something that is a function of Y. So if you're holding Y constant, this just looks like a constant. But notice, if that was a, if that was a negative constant, if sine of Y at some point happens to be negative, then your whole exponential function actually kind of goes down. It's sort of like a
negative E to the X look. But if you imagine moving
in the Y direction, so instead of the pure
X direction like that, if we imagine ourselves moving with the input going along, let's see what it would be, it would be this way, positive Y direction, you have this sort of sinusoidal shape, and that should make sense because you've got this sine of Y. And depending on what E to the X is, the amplitude of that sine wave is gonna get, you know, really high at some points here. It's going way up and way down. But if E to the X was
really small, you know, it hardly, hardly even looks like it's wiggling over here. It pretty much looks flat. So that's the graph that we're looking at. And I'm telling you right now, I claim that this is harmonic. This is a function whose Laplacian is equal to zero. And what that would mean is that as we go over here and we say, we evaluate the Laplacian of F, which, just to remind you, there's a different formula rather than thinking
divergence of gradient, that turns out to be completely the same as saying, you take the second derivative of that function with respect to X, that's its first input, and you add that, let's see, second derivative
with respect to X, you add that to the second derivative of your function with respect to the next variable. And you keep doing this for all of the different
variables that there are, but this is just a two variable function, so you do this twice. The claim is that this is always equal to zero. So I might say, kind of equivalent, at every possible input, it's equal to zero. And I think I'll leave that as something for you to compute. It might be kind of good practice to kind of get a feel for computing the Laplacian. But what I wanna do is interpret what does this actually mean, right? 'Cause you can plug it through, and you can see, ah yes, at all possible inputs, it will be zero. But what does that mean? Because in the single variable context, once we started thinking about the geometric interpretation of a second derivative as this concavity, it sort of made sense that forcing it to be zero will give us a straight line. But clearly, that's not the case. This is much more complicated than a straight line. And for that, I want to give a kind of a different way that you can think about the single variable second derivative. On the one hand, you can think of, let's say, negative second derivative, as being this concavity where it's kind of frowning down. But another way you can
maybe think about this is saying that all of the
neighbors of your point, if you go a little bit to the left, right, you've got an input point here, and if you go a little bit to the left, the neighbor is less than it, and if you go a little bit to the right, that other neighbor is also less than it. So it's kind of a way of saying, hey, if you look at the
neighbors of your input, so if you, if you happen
to be making the claim that F double prime, at
some particular input, like X sub O, is less than zero, it's saying that all of the neighbors of X sub O, all of the neighbors of that point, are less than it. And if you do a similar thing over at a positive concavity point where it's kind of smiling up, you say, well its neighbor to the right has a greater value, and its neighbor to the
left has a greater value. So at some point, where
the second derivative, instead of being less than zero, happens to be, happens to be greater than zero, that means that the neighbors tend to be greater than the point itself. And even if you're
looking at a circumstance that isn't this idealized, you know, it happens to be a local minimum, but let's say you're looking at a graph. Let's say you're looking at a function at a point where it's concave up, right? It's concave up, but
it's not this idealized, local minimum kind of circumstance. So instead, you might be looking at a point like this, and if you look at, you know, its neighbor to the left, that'll have some value
that's actually less than your original guy, so the neighbor looks like it's less than it on the left, but if you move that same
distance to the right, its neighbor is greater. But you would say, on average, if you took the average
value of the neighbors, the neighbor on the
right kind of outbalances the neighbor on the left, and you would say, on average, its neighbors are greater
than the point itself. So let's say that input point there was like X sub O, that would mean that the second derivative of your function at that point, you know, is greater than zero. So this positive concavity, you can also think of it as a measure of, on average, are the neighbors greater
than your original point or less than it? And the reason I'm saying this is because this idea, of kind of comparing your neighbors to the original point is a much better way to contemplate the Laplacian in the multivariable world. So if we look at a function like this, and let's say we're looking at it kind of from a bird's eye view. So we've got our XY plane, right? This over here is the X-axis, and this up here is the Y-axis. And let's say that we're looking at some specific input point. With the Laplacian, you wanna start thinking about a circle of points around it, all of its neighbors, and in fact, think of a perfect circle, so all of the points that are a specified distance away. The question the Laplacian is asking is, "Hey, are those neighbor
points, on average, "greater than or less
than your original point?" And this is actually, this is how I introduced the Laplacian in the original video where I was giving kind of the intuition for the Laplacian, you're asking, "Do the
points around a given input "happen to be greater
than it or less than it?" And if you're looking at a point where the Laplacian of your function happens to be greater
than zero at some point, that would mean all of the neighbors tend to be, on average,
greater than your point. Whereas if you're looking at a point where the Laplacian of your function is less than zero, then all of those neighbors, on average, would be less than your point. So in particular, you know, if the Laplacian was less than zero, your point might look
like a local maximum. Or if the Laplacian was greater than zero, it might look like a local minimum, 'cause all of its neighbors would be greater than where it is. But for harmonic functions, what makes them so special is that you're saying the value of the function itself, or the value of the
Laplacian of the function at every possible point is equal to zero. So no matter what point you choose, those neighbors are gonna be, on average, the same value as this guy. So the height of the graph
above those neighbors will, on average, be the same. So, so if we kind of look around the graph, what that should mean is, let's say you're looking
at an input point, you know, the output of this guy. And if we looked at all of its, the circle of its neighbors, and kind of projected
them up onto the graph, what it should mean is that the height of all the points on this circle, on average, are the same as that. And no matter where you look, that should kind of average out. And again, I encourage you to take a look at, take a look at this function and actually evaluate the Laplacian to see that it's zero. But what's interesting
is it's not at all clear, just looking at this E to the X times sine of Y formula, that the average value of
a circle of input points is always gonna kind of equal the value of the point at the center. That's not something you can easily tell just looking at that formula. But with what's not
that hard a computation, you can make this conclusion, which is pretty far-reaching. And this comes up all the time in physics. You know, for example, heat is one where maybe you wanna describe how the heat at a certain point in a room is related to the
average value of the heat of all of the points kind of around it. And in fact, it comes up in
all sorts of circumstances where you have some
point in physical space and something about that point, maybe like the rate at which some property of it is changing corresponds to the average value at points around it. So whenever you're sort
of relating neighbors to your original point,
the Laplacian comes in, and harmonic functions have this tendency to correspond to some notion of stability. And I won't go deeper into that now. This is really, that really starts to get into the topic of partial differential equations. But at least in the context of just multivariable calculus, I wanted to shed some light on interpreting this operator, and kind of interpreting the physical and geometric properties that that implies about a function. And with that, I will see you next video.