{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipywidgets in c:\\python310\\lib\\site-packages (8.0.6)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\grant\\appdata\\roaming\\python\\python310\\site-packages (from ipywidgets) (5.9.0)\n",
      "Requirement already satisfied: ipython>=6.1.0 in c:\\users\\grant\\appdata\\roaming\\python\\python310\\site-packages (from ipywidgets) (8.12.0)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.7 in c:\\python310\\lib\\site-packages (from ipywidgets) (3.0.7)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in c:\\users\\grant\\appdata\\roaming\\python\\python310\\site-packages (from ipywidgets) (6.22.0)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.7 in c:\\python310\\lib\\site-packages (from ipywidgets) (4.0.7)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in c:\\users\\grant\\appdata\\roaming\\python\\python310\\site-packages (from ipykernel>=4.5.1->ipywidgets) (5.3.0)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in c:\\users\\grant\\appdata\\roaming\\python\\python310\\site-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: tornado>=6.1 in c:\\users\\grant\\appdata\\roaming\\python\\python310\\site-packages (from ipykernel>=4.5.1->ipywidgets) (6.2)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in c:\\users\\grant\\appdata\\roaming\\python\\python310\\site-packages (from ipykernel>=4.5.1->ipywidgets) (1.6.7)\n",
      "Requirement already satisfied: packaging in c:\\python310\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets) (23.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\grant\\appdata\\roaming\\python\\python310\\site-packages (from ipykernel>=4.5.1->ipywidgets) (5.9.4)\n",
      "Requirement already satisfied: pyzmq>=20 in c:\\users\\grant\\appdata\\roaming\\python\\python310\\site-packages (from ipykernel>=4.5.1->ipywidgets) (25.0.2)\n",
      "Requirement already satisfied: nest-asyncio in c:\\users\\grant\\appdata\\roaming\\python\\python310\\site-packages (from ipykernel>=4.5.1->ipywidgets) (1.5.6)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in c:\\users\\grant\\appdata\\roaming\\python\\python310\\site-packages (from ipykernel>=4.5.1->ipywidgets) (8.1.0)\n",
      "Requirement already satisfied: comm>=0.1.1 in c:\\users\\grant\\appdata\\roaming\\python\\python310\\site-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.3)\n",
      "Requirement already satisfied: stack-data in c:\\users\\grant\\appdata\\roaming\\python\\python310\\site-packages (from ipython>=6.1.0->ipywidgets) (0.6.2)\n",
      "Requirement already satisfied: decorator in c:\\users\\grant\\appdata\\roaming\\python\\python310\\site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: backcall in c:\\users\\grant\\appdata\\roaming\\python\\python310\\site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\grant\\appdata\\roaming\\python\\python310\\site-packages (from ipython>=6.1.0->ipywidgets) (2.14.0)\n",
      "Requirement already satisfied: colorama in c:\\python310\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.4.6)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\grant\\appdata\\roaming\\python\\python310\\site-packages (from ipython>=6.1.0->ipywidgets) (0.18.2)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in c:\\users\\grant\\appdata\\roaming\\python\\python310\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.38)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\grant\\appdata\\roaming\\python\\python310\\site-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in c:\\users\\grant\\appdata\\roaming\\python\\python310\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\python310\\lib\\site-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (2.8.2)\n",
      "Requirement already satisfied: platformdirs>=2.5 in c:\\users\\grant\\appdata\\roaming\\python\\python310\\site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel>=4.5.1->ipywidgets) (3.2.0)\n",
      "Requirement already satisfied: pywin32>=300 in c:\\users\\grant\\appdata\\roaming\\python\\python310\\site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel>=4.5.1->ipywidgets) (306)\n",
      "Requirement already satisfied: wcwidth in c:\\python310\\lib\\site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=6.1.0->ipywidgets) (0.2.6)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\grant\\appdata\\roaming\\python\\python310\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\grant\\appdata\\roaming\\python\\python310\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.2.1)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\grant\\appdata\\roaming\\python\\python310\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (1.2.0)\n",
      "Requirement already satisfied: six in c:\\python310\\lib\\site-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 23.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package names to\n",
      "[nltk_data]     C:\\Users\\Grant\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package names is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Grant\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package state_union to\n",
      "[nltk_data]     C:\\Users\\Grant\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package state_union is already up-to-date!\n",
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     C:\\Users\\Grant\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n",
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\Grant\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Grant\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Grant\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Grant\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict\n",
    "import pandas as pd\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "import nltk.data\n",
    "import re\n",
    "import contractions\n",
    "from bs4 import BeautifulSoup\n",
    "import unidecode\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score\n",
    "from sklearn.metrics import f1_score, recall_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import math\n",
    "import numpy as np\n",
    "import copy\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "nltk.download([\n",
    "\"names\",\n",
    "\"stopwords\",\n",
    "\"state_union\",\n",
    "\"twitter_samples\",\n",
    "\"movie_reviews\",\n",
    "\"averaged_perceptron_tagger\",\n",
    "\"vader_lexicon\",\n",
    "\"punkt\",\n",
    "])\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "encoder = LabelEncoder()\n",
    "sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "computing = pd.read_csv(\"Datasets\\\\KhanAcademy\\\\Computing.csv\")\n",
    "computing = computing.dropna()\n",
    "\n",
    "economics = pd.read_csv(\"Datasets\\\\KhanAcademy\\\\Economics.csv\")\n",
    "economics = economics.dropna()\n",
    "\n",
    "humanities = pd.read_csv(\"Datasets\\\\KhanAcademy\\\\Humanities.csv\")\n",
    "humanities = humanities.dropna()\n",
    "\n",
    "math = pd.read_csv(\"Datasets\\\\KhanAcademy\\\\Math.csv\")\n",
    "math = math.dropna()\n",
    "\n",
    "science = pd.read_csv(\"Datasets\\\\KhanAcademy\\\\Science.csv\")\n",
    "science = science.dropna()\n",
    "\n",
    "khan_dfs = [computing, economics, humanities, math, science]\n",
    "khan = pd.concat(khan_dfs, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8df11958ff884a56b4ad3fd80a469f91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8261 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a1ce1e91f114e1b9c4e82fec154c138",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8261 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>course</th>\n",
       "      <th>unit</th>\n",
       "      <th>lesson</th>\n",
       "      <th>video_title</th>\n",
       "      <th>about</th>\n",
       "      <th>transcript</th>\n",
       "      <th>clean_transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Computer programming</td>\n",
       "      <td>Intro to JS: Drawing &amp; Animation</td>\n",
       "      <td>Intro to programming</td>\n",
       "      <td>What is Programming?</td>\n",
       "      <td>Programming is the process of creating a set o...</td>\n",
       "      <td>Hi, welcome to programming! If you've never le...</td>\n",
       "      <td>If you have never learned to program before, y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Computer programming</td>\n",
       "      <td>Intro to JS: Drawing &amp; Animation</td>\n",
       "      <td>Coloring</td>\n",
       "      <td>The Power of the Docs</td>\n",
       "      <td>Created by Pamela Fox.</td>\n",
       "      <td>Voiceover: Ok so you've\\r\\nmade a few programs...</td>\n",
       "      <td>Is it width and height, or is it height and wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Computer programming</td>\n",
       "      <td>Intro to HTML/CSS: Making webpages</td>\n",
       "      <td>Further learning</td>\n",
       "      <td>HTML validation</td>\n",
       "      <td>Learn how to validate your webpages with the W...</td>\n",
       "      <td>- [Voiceover] On Khan Academy, we pop up the o...</td>\n",
       "      <td>But we only tell you about the big things. The...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Computer programming</td>\n",
       "      <td>Intro to SQL: Querying and managing data</td>\n",
       "      <td>SQL basics</td>\n",
       "      <td>Welcome to SQL</td>\n",
       "      <td>SQL is useful for creating and querying relati...</td>\n",
       "      <td>- [Instructor] The world is full of data. Ever...</td>\n",
       "      <td>Every app that you use is full of data. On Kha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Computer programming</td>\n",
       "      <td>Intro to SQL: Querying and managing data</td>\n",
       "      <td>SQL basics</td>\n",
       "      <td>S-Q-L or SEQUEL?</td>\n",
       "      <td>How is it pronounced? Why? Let's discuss...</td>\n",
       "      <td>At this point, you've probably heard me\\r\\npro...</td>\n",
       "      <td>Some of you might even be mad that I am pronou...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 course                                      unit  \\\n",
       "0  Computer programming          Intro to JS: Drawing & Animation   \n",
       "1  Computer programming          Intro to JS: Drawing & Animation   \n",
       "5  Computer programming        Intro to HTML/CSS: Making webpages   \n",
       "6  Computer programming  Intro to SQL: Querying and managing data   \n",
       "7  Computer programming  Intro to SQL: Querying and managing data   \n",
       "\n",
       "                 lesson            video_title  \\\n",
       "0  Intro to programming   What is Programming?   \n",
       "1              Coloring  The Power of the Docs   \n",
       "5      Further learning        HTML validation   \n",
       "6            SQL basics         Welcome to SQL   \n",
       "7            SQL basics       S-Q-L or SEQUEL?   \n",
       "\n",
       "                                               about  \\\n",
       "0  Programming is the process of creating a set o...   \n",
       "1                             Created by Pamela Fox.   \n",
       "5  Learn how to validate your webpages with the W...   \n",
       "6  SQL is useful for creating and querying relati...   \n",
       "7       How is it pronounced? Why? Let's discuss...    \n",
       "\n",
       "                                          transcript  \\\n",
       "0  Hi, welcome to programming! If you've never le...   \n",
       "1  Voiceover: Ok so you've\\r\\nmade a few programs...   \n",
       "5  - [Voiceover] On Khan Academy, we pop up the o...   \n",
       "6  - [Instructor] The world is full of data. Ever...   \n",
       "7  At this point, you've probably heard me\\r\\npro...   \n",
       "\n",
       "                                    clean_transcript  \n",
       "0  If you have never learned to program before, y...  \n",
       "1  Is it width and height, or is it height and wi...  \n",
       "5  But we only tell you about the big things. The...  \n",
       "6  Every app that you use is full of data. On Kha...  \n",
       "7  Some of you might even be mad that I am pronou...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_base(x: str):\n",
    "        # remove any html tags\n",
    "        x = BeautifulSoup(x, \"html.parser\").get_text(separator=\" \")\n",
    "        # # set all to lower\n",
    "        # x = x.lower()\n",
    "        # clean up the contractions\n",
    "        x = contractions.fix(x)\n",
    "        # remove accended characters\n",
    "        x = unidecode.unidecode(x)\n",
    "        # # remove stopwords: https://stackoverflow.com/questions/19560498/faster-way-to-remove-stop-words-in-python\n",
    "        # x = ' '.join([word for word in x.split() if word not in cachedStopWords]) # slower to use word tokenize\n",
    "        # # fix punctuation spacing\n",
    "        # x = re.sub(r'(?<=[\\.\\,\\?])(?=[^\\s])', r' ', x)\n",
    "        # # strip punctuation\n",
    "        # x = re.sub(r'[\\.\\,\\?\\\\\\/\\<\\>\\;\\:\\[\\]\\{\\}]', r'', x)\n",
    "        # strip quotes\n",
    "        x = x.replace('\\'', '').replace('\\\"', '')\n",
    "        # remove some actions\n",
    "        remove_list = ['(Laughter)', '(laughter)', '(Music)', '(music)', '(Music ends)', '(Audience cheers)', '(Applause)', '(Applause ends)', '(Applause continues)', '(Bells)', '(Trumpet)', '(Clears throat)']\n",
    "        x = ' '.join([word for word in x.split() if word not in remove_list])\n",
    "        # remove extraneous items\n",
    "        x = x.replace(' -- ', '').replace(' .. ', ' ').replace(' ... ', ' ')\n",
    "        # remove extra whitespace\n",
    "        x = ' '.join(x.strip().split())\n",
    "        # # may want to add lematization\n",
    "        # x = ' '.join([lemmatizer.lemmatize(word) for word in x.split()])\n",
    "        # remove some of the extra bracket tags\n",
    "        x = re.sub(r\"\\s{2,}\", \" \", re.sub(r\"[\\(\\[\\{][^\\)\\]\\}]*[\\)\\]\\}]\", \"\", x))\n",
    "        return x\n",
    "\n",
    "def remove_first_sentence(doc):\n",
    "    \"\"\"\n",
    "    This removes the first sentence of a document. We use this to remove all narrator / speaker tags, and\n",
    "    to remove unnecessary introductory sentences that most transcripts have\n",
    "    \"\"\"\n",
    "    return ' '.join(nltk.sent_tokenize(doc)[1:])\n",
    "\n",
    "transcripts = khan['transcript']\n",
    "transcripts = transcripts.progress_apply(remove_first_sentence)\n",
    "transcripts = transcripts.progress_apply(clean_base)\n",
    "khan['clean_transcript'] = transcripts\n",
    "khan.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 152 entries, 0 to 151\n",
      "Data columns (total 13 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        152 non-null    int64 \n",
      " 1   ans       152 non-null    object\n",
      " 2   question  152 non-null    object\n",
      " 3   a)        152 non-null    object\n",
      " 4   b)        152 non-null    object\n",
      " 5   c)        152 non-null    object\n",
      " 6   d)        152 non-null    object\n",
      " 7   e)        152 non-null    object\n",
      " 8   year      152 non-null    int64 \n",
      " 9   sec       152 non-null    int64 \n",
      " 10  num       152 non-null    int64 \n",
      " 11  diff      152 non-null    object\n",
      " 12  blanks    152 non-null    int64 \n",
      "dtypes: int64(5), object(8)\n",
      "memory usage: 15.6+ KB\n"
     ]
    }
   ],
   "source": [
    "sat = pd.read_csv(\"Datasets\\\\SAT_Question_Dataset.csv\")\n",
    "sat.info(verbose=True, show_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24bf98840f974b1582fd938c7887e3c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/152 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "226bb66043c44a9890a143e34cb03465",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/152 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cb23ed5cd70412c8645e9a705233d3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/152 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a28c245879b243218bb4ce4c965a6580",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/152 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d22cf498eae246c2b66ff945110327e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/152 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02c7d79bc0c94dd08ed8dfa17477ed82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/152 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>ans</th>\n",
       "      <th>question</th>\n",
       "      <th>a)</th>\n",
       "      <th>b)</th>\n",
       "      <th>c)</th>\n",
       "      <th>d)</th>\n",
       "      <th>e)</th>\n",
       "      <th>year</th>\n",
       "      <th>sec</th>\n",
       "      <th>num</th>\n",
       "      <th>diff</th>\n",
       "      <th>blanks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>e</td>\n",
       "      <td>Much of our knowledge of dinosaurs comes from ...</td>\n",
       "      <td>Much of our knowledge of dinosaurs comes from ...</td>\n",
       "      <td>Much of our knowledge of dinosaurs comes from ...</td>\n",
       "      <td>Much of our knowledge of dinosaurs comes from ...</td>\n",
       "      <td>Much of our knowledge of dinosaurs comes from ...</td>\n",
       "      <td>Much of our knowledge of dinosaurs comes from ...</td>\n",
       "      <td>2001</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>c</td>\n",
       "      <td>Responding to criticism that the script was ra...</td>\n",
       "      <td>Responding to criticism that the script was ra...</td>\n",
       "      <td>Responding to criticism that the script was ra...</td>\n",
       "      <td>Responding to criticism that the script was ra...</td>\n",
       "      <td>Responding to criticism that the script was ra...</td>\n",
       "      <td>Responding to criticism that the script was ra...</td>\n",
       "      <td>2001</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>e</td>\n",
       "      <td>Vernal pools are among the most _____ of ponds...</td>\n",
       "      <td>Vernal pools are among the most transitory of ...</td>\n",
       "      <td>Vernal pools are among the most anachronistic ...</td>\n",
       "      <td>Vernal pools are among the most immutable of p...</td>\n",
       "      <td>Vernal pools are among the most itinerant of p...</td>\n",
       "      <td>Vernal pools are among the most ephemeral of p...</td>\n",
       "      <td>2001</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>e</td>\n",
       "      <td>During the 1990s, Shanghai benefited from an a...</td>\n",
       "      <td>During the 1990s, Shanghai benefited from an a...</td>\n",
       "      <td>During the 1990s, Shanghai benefited from an a...</td>\n",
       "      <td>During the 1990s, Shanghai benefited from an a...</td>\n",
       "      <td>During the 1990s, Shanghai benefited from an a...</td>\n",
       "      <td>During the 1990s, Shanghai benefited from an a...</td>\n",
       "      <td>2001</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>b</td>\n",
       "      <td>Many subatomic nuclear particles are _____ and...</td>\n",
       "      <td>Many subatomic nuclear particles are unstable ...</td>\n",
       "      <td>Many subatomic nuclear particles are elusive a...</td>\n",
       "      <td>Many subatomic nuclear particles are minute an...</td>\n",
       "      <td>Many subatomic nuclear particles are charged a...</td>\n",
       "      <td>Many subatomic nuclear particles are tenuous a...</td>\n",
       "      <td>2001</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id ans                                           question  \\\n",
       "0   1   e  Much of our knowledge of dinosaurs comes from ...   \n",
       "1   2   c  Responding to criticism that the script was ra...   \n",
       "2   3   e  Vernal pools are among the most _____ of ponds...   \n",
       "3   4   e  During the 1990s, Shanghai benefited from an a...   \n",
       "4   5   b  Many subatomic nuclear particles are _____ and...   \n",
       "\n",
       "                                                  a)  \\\n",
       "0  Much of our knowledge of dinosaurs comes from ...   \n",
       "1  Responding to criticism that the script was ra...   \n",
       "2  Vernal pools are among the most transitory of ...   \n",
       "3  During the 1990s, Shanghai benefited from an a...   \n",
       "4  Many subatomic nuclear particles are unstable ...   \n",
       "\n",
       "                                                  b)  \\\n",
       "0  Much of our knowledge of dinosaurs comes from ...   \n",
       "1  Responding to criticism that the script was ra...   \n",
       "2  Vernal pools are among the most anachronistic ...   \n",
       "3  During the 1990s, Shanghai benefited from an a...   \n",
       "4  Many subatomic nuclear particles are elusive a...   \n",
       "\n",
       "                                                  c)  \\\n",
       "0  Much of our knowledge of dinosaurs comes from ...   \n",
       "1  Responding to criticism that the script was ra...   \n",
       "2  Vernal pools are among the most immutable of p...   \n",
       "3  During the 1990s, Shanghai benefited from an a...   \n",
       "4  Many subatomic nuclear particles are minute an...   \n",
       "\n",
       "                                                  d)  \\\n",
       "0  Much of our knowledge of dinosaurs comes from ...   \n",
       "1  Responding to criticism that the script was ra...   \n",
       "2  Vernal pools are among the most itinerant of p...   \n",
       "3  During the 1990s, Shanghai benefited from an a...   \n",
       "4  Many subatomic nuclear particles are charged a...   \n",
       "\n",
       "                                                  e)  year  sec  num diff  \\\n",
       "0  Much of our knowledge of dinosaurs comes from ...  2001    1    1    1   \n",
       "1  Responding to criticism that the script was ra...  2001    1    2    1   \n",
       "2  Vernal pools are among the most ephemeral of p...  2001    1    3    2   \n",
       "3  During the 1990s, Shanghai benefited from an a...  2001    1    4    3   \n",
       "4  Many subatomic nuclear particles are tenuous a...  2001    1    5    3   \n",
       "\n",
       "   blanks  \n",
       "0       2  \n",
       "1       2  \n",
       "2       2  \n",
       "3       1  \n",
       "4       2  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for column_name in ['question', 'a)', 'b)', 'c)', 'd)', 'e)']:\n",
    "    sat[column_name] = sat[column_name].progress_apply(clean_base)\n",
    "sat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f25325042ea411b8bfb0ae06b46eefa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8261 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "603547\n"
     ]
    }
   ],
   "source": [
    "# create a list of all of the sentences with each sentence tokenized.\n",
    "def tokenize_transcript(transcript):\n",
    "    token_document = [word_tokenize(t) for t in sent_detector.tokenize(transcript)]\n",
    "    sents = []\n",
    "    start_of_sentence = \"<sent>\"\n",
    "    end_of_sentence = \"<\\\\sent>\"\n",
    "    for sent in token_document:\n",
    "        sent.insert(0, start_of_sentence)\n",
    "        sent.append(end_of_sentence)\n",
    "        sents.append(sent)\n",
    "    return sents\n",
    "\n",
    "sents = []\n",
    "for transcript in tqdm(khan['clean_transcript']):\n",
    "    sents += tokenize_transcript(transcript)\n",
    "\n",
    "print(len(sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing all N-grams where N is 3 or less\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b735df857a324302930a4b64a64540bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/603547 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing frequency distribution\n"
     ]
    }
   ],
   "source": [
    "n = 3\n",
    "print(\"Computing all N-grams where N is %d or less\" % n)\n",
    "grams = []\n",
    "for sent in tqdm(sents):\n",
    "    for i in range(0, 3):\n",
    "        grams += list(ngrams(sent, i + 1))\n",
    "    \n",
    "print(\"Computing frequency distribution\")\n",
    "freq_dist = nltk.FreqDist(grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute the windows\n",
      "mask indicies = [10, 22]\n",
      "[('engaging', 'simplicity'), ('subjective', 'ambiguity'), ('muddled', 'clarity'), ('terse', 'emptiness'), ('difficult', 'abstraction')]\n",
      "[['rambling', 'and', '_____0'], ['and', '_____0', ','], ['_____0', ',', 'the'], ['and', '_____0'], ['_____0', ','], ['_____0'], ['succinctness', 'and', '_____1'], ['and', '_____1', '.'], ['and', '_____1'], ['_____1', '.'], ['_____1']]\n",
      "['<sent>', 'Responding', 'to', 'criticism', 'that', 'the', 'script', 'was', 'rambling', 'and', '_____0', ',', 'the', 'new', 'screenwriter', 'revised', 'the', 'dialogue', 'for', 'greater', 'succinctness', 'and', '_____1', '.', '<\\\\sent>']\n"
     ]
    }
   ],
   "source": [
    "def tokenize_question(question):\n",
    "    return tokenize_transcript(question)[0]\n",
    "\n",
    "def get_mask_indices(row):\n",
    "    mask_indices = []\n",
    "    previous_idx = 0\n",
    "    for blank_id in range(row['blanks']):\n",
    "        idx = tokenize_question(row['question']).index(mask, previous_idx)\n",
    "        mask_indices.append(idx)\n",
    "        previous_idx = idx + 1\n",
    "    return mask_indices\n",
    "\n",
    "def extract_possible_solutions(row, mask_indices):\n",
    "    possible_solutions = []\n",
    "    for i in ['a)', 'b)', 'c)', 'd)', 'e)']:\n",
    "        # print(test[i])\n",
    "        tokens = tokenize_question(row[i])\n",
    "        possible_solution = []\n",
    "        for mask_idx in mask_indices:\n",
    "            possible_solution.append(tokens[mask_idx])\n",
    "        possible_solutions.append(tuple(possible_solution))\n",
    "    return possible_solutions\n",
    "\n",
    "def get_ngrams_with_mask(mask_indices, tokenized_question, n):\n",
    "    \"\"\"This method is used to compute the sliding window for the original n-gram size as well\n",
    "    as each of the subsequently smaller sizes of n. This method requires the variable 'n' to\n",
    "    be defined globally for the maximum n-gram size to look for.\n",
    "\n",
    "    Args:\n",
    "        row : a row from a pandas dataframe of the SAT dataset to create the masks from.\n",
    "\n",
    "    Returns:\n",
    "        list: A list containing tuples of each n-gram is returned. If more than one \n",
    "    \"\"\"\n",
    "    ranges = []\n",
    "    count = 0\n",
    "    for mask_idx in mask_indices:\n",
    "        remask = mask + str(count)\n",
    "        tokenized_question[mask_idx] = remask\n",
    "        count += 1\n",
    "        # print(\"\\nmask_idx = %d\" % mask_idx)\n",
    "        for i in range(n, 0, -1):  # for each successively smaller n-gram size\n",
    "            # print(\"n = %d\" % i)\n",
    "            upper_bound = mask_idx + 1\n",
    "            lower_bound = mask_idx - i + 1\n",
    "            if lower_bound >= 0 and upper_bound < len(tokenized_question):\n",
    "                ranges.append((lower_bound, upper_bound))\n",
    "                # print(\"range(%d, %d)\" % (lower_bound, upper_bound))\n",
    "            for j in range(1, i):  # already processed first n-gram for size i. Now process the rest where j is the offset.\n",
    "                upper_bound += 1\n",
    "                lower_bound += 1\n",
    "                if lower_bound >= 0 and upper_bound < len(tokenized_question):\n",
    "                    ranges.append((lower_bound, upper_bound))\n",
    "                    # print(\"range(%d, %d)\" % (lower_bound, upper_bound))\n",
    "    n_grams = []\n",
    "    for indices in ranges:\n",
    "        n_gram = []\n",
    "        for i in range(indices[0], indices[1]):\n",
    "            n_gram.append(tokenized_question[i])\n",
    "        n_grams.append(n_gram)\n",
    "        \n",
    "    return n_grams\n",
    "    \n",
    "    \n",
    "print(\"compute the windows\")\n",
    "test = sat.iloc[1]\n",
    "mask = \"_____\"\n",
    "mask_indices = get_mask_indices(test)\n",
    "tokenized_question = tokenize_question(test['question'])\n",
    "possible_solutions = extract_possible_solutions(test, mask_indices)\n",
    "windows = get_ngrams_with_mask(mask_indices, tokenized_question, n=3)\n",
    "print(\"mask indicies = %s\" % str(mask_indices))\n",
    "print(possible_solutions)\n",
    "print(windows)\n",
    "print(tokenized_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
