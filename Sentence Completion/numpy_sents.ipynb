{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "812cb313-3beb-4d5c-8318-f111de14c53b",
   "metadata": {},
   "source": [
    "This file will attempt to perform sentence completion a little differently by utilizing [numpy](https://numpy-ml.readthedocs.io/en/latest/numpy_ml.ngram.goodturing.html) instead of nltk. Other numpy smoothing algorithms can be found [here](https://numpy-ml.readthedocs.io/en/latest/numpy_ml.ngram.html#:~:text=Laplace%20smoothing%20is%20the%20assumption,time%20than%20it%20actually%20does.&text=where%20c(a)%20denotes%20the,n%20%2Dgrams%20in%20the%20corpus.).\n",
    "\n",
    "## Step 1: Create corpus_fp\n",
    "Numpy requires a variable called corpus_fp in order to train their good turing ngram model. This corpus_fp variable needs to be a filepath to a newline separated text file of the documents to use for the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab855025-6763-43e2-9f8d-10d529782ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package names to /home/fassg/nltk_data...\n",
      "[nltk_data]   Package names is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/fassg/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package state_union to\n",
      "[nltk_data]     /home/fassg/nltk_data...\n",
      "[nltk_data]   Package state_union is already up-to-date!\n",
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     /home/fassg/nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n",
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     /home/fassg/nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/fassg/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/fassg/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/fassg/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict\n",
    "import pandas as pd\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "import nltk.data\n",
    "import re\n",
    "import contractions\n",
    "from bs4 import BeautifulSoup\n",
    "import unidecode\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score\n",
    "from sklearn.metrics import f1_score, recall_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import math\n",
    "import numpy as np\n",
    "import copy\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import pickle\n",
    "from nltk.lm.preprocessing import flatten\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "from nltk.util import pad_sequence\n",
    "from nltk.util import everygrams\n",
    "from nltk.lm import MLE\n",
    "from functools import partial\n",
    "\n",
    "nltk.download([\n",
    "\"names\",\n",
    "\"stopwords\",\n",
    "\"state_union\",\n",
    "\"twitter_samples\",\n",
    "\"movie_reviews\",\n",
    "\"averaged_perceptron_tagger\",\n",
    "\"vader_lexicon\",\n",
    "\"punkt\",\n",
    "])\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "encoder = LabelEncoder()\n",
    "sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "tqdm.pandas()\n",
    "n = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4c6e450-ea38-4118-a66f-1f070b9b9d3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 8261 entries, 0 to 2789\n",
      "Data columns (total 6 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   course       8261 non-null   object\n",
      " 1   unit         8261 non-null   object\n",
      " 2   lesson       8261 non-null   object\n",
      " 3   video_title  8261 non-null   object\n",
      " 4   about        8261 non-null   object\n",
      " 5   transcript   8261 non-null   object\n",
      "dtypes: object(6)\n",
      "memory usage: 451.8+ KB\n"
     ]
    }
   ],
   "source": [
    "computing_df = pd.read_csv(Path(\"Datasets/KhanAcademy/Computing.csv\"))\n",
    "computing_df = computing_df.dropna()\n",
    "\n",
    "economics_df = pd.read_csv(Path(\"Datasets/KhanAcademy/Economics.csv\"))\n",
    "economics_df = economics_df.dropna()\n",
    "\n",
    "humanities_df = pd.read_csv(Path(\"Datasets/KhanAcademy/Humanities.csv\"))\n",
    "humanities_df = humanities_df.dropna()\n",
    "\n",
    "math_df = pd.read_csv(Path(\"Datasets/KhanAcademy/Math.csv\"))\n",
    "math_df = math_df.dropna()\n",
    "\n",
    "science_df = pd.read_csv(Path(\"Datasets/KhanAcademy/Science.csv\"))\n",
    "science_df = science_df.dropna()\n",
    "\n",
    "khan_dfs = [computing_df, economics_df, humanities_df, math_df, science_df]\n",
    "khan = pd.concat(khan_dfs, axis=0)\n",
    "khan.info(verbose=True, show_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20855666-51dc-43fc-bfc0-08f61dcf7079",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e02d99999bb453c8c642b5885d95059",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8261 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cd13d5b361f43358830d9346b8de88e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8261 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>course</th>\n",
       "      <th>unit</th>\n",
       "      <th>lesson</th>\n",
       "      <th>video_title</th>\n",
       "      <th>about</th>\n",
       "      <th>transcript</th>\n",
       "      <th>clean_transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Computer programming</td>\n",
       "      <td>Intro to JS: Drawing &amp; Animation</td>\n",
       "      <td>Intro to programming</td>\n",
       "      <td>What is Programming?</td>\n",
       "      <td>Programming is the process of creating a set o...</td>\n",
       "      <td>Hi, welcome to programming! If you've never le...</td>\n",
       "      <td>If you have never learned to program before, y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Computer programming</td>\n",
       "      <td>Intro to JS: Drawing &amp; Animation</td>\n",
       "      <td>Coloring</td>\n",
       "      <td>The Power of the Docs</td>\n",
       "      <td>Created by Pamela Fox.</td>\n",
       "      <td>Voiceover: Ok so you've\\nmade a few programs, ...</td>\n",
       "      <td>Is it width and height, or is it height and wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Computer programming</td>\n",
       "      <td>Intro to HTML/CSS: Making webpages</td>\n",
       "      <td>Further learning</td>\n",
       "      <td>HTML validation</td>\n",
       "      <td>Learn how to validate your webpages with the W...</td>\n",
       "      <td>- [Voiceover] On Khan Academy, we pop up the o...</td>\n",
       "      <td>But we only tell you about the big things. The...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Computer programming</td>\n",
       "      <td>Intro to SQL: Querying and managing data</td>\n",
       "      <td>SQL basics</td>\n",
       "      <td>Welcome to SQL</td>\n",
       "      <td>SQL is useful for creating and querying relati...</td>\n",
       "      <td>- [Instructor] The world is full of data. Ever...</td>\n",
       "      <td>Every app that you use is full of data. On Kha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Computer programming</td>\n",
       "      <td>Intro to SQL: Querying and managing data</td>\n",
       "      <td>SQL basics</td>\n",
       "      <td>S-Q-L or SEQUEL?</td>\n",
       "      <td>How is it pronounced? Why? Let's discuss...</td>\n",
       "      <td>At this point, you've probably heard me\\nprono...</td>\n",
       "      <td>Some of you might even be mad that I am pronou...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 course                                      unit  \\\n",
       "0  Computer programming          Intro to JS: Drawing & Animation   \n",
       "1  Computer programming          Intro to JS: Drawing & Animation   \n",
       "5  Computer programming        Intro to HTML/CSS: Making webpages   \n",
       "6  Computer programming  Intro to SQL: Querying and managing data   \n",
       "7  Computer programming  Intro to SQL: Querying and managing data   \n",
       "\n",
       "                 lesson            video_title  \\\n",
       "0  Intro to programming   What is Programming?   \n",
       "1              Coloring  The Power of the Docs   \n",
       "5      Further learning        HTML validation   \n",
       "6            SQL basics         Welcome to SQL   \n",
       "7            SQL basics       S-Q-L or SEQUEL?   \n",
       "\n",
       "                                               about  \\\n",
       "0  Programming is the process of creating a set o...   \n",
       "1                             Created by Pamela Fox.   \n",
       "5  Learn how to validate your webpages with the W...   \n",
       "6  SQL is useful for creating and querying relati...   \n",
       "7       How is it pronounced? Why? Let's discuss...    \n",
       "\n",
       "                                          transcript  \\\n",
       "0  Hi, welcome to programming! If you've never le...   \n",
       "1  Voiceover: Ok so you've\\nmade a few programs, ...   \n",
       "5  - [Voiceover] On Khan Academy, we pop up the o...   \n",
       "6  - [Instructor] The world is full of data. Ever...   \n",
       "7  At this point, you've probably heard me\\nprono...   \n",
       "\n",
       "                                    clean_transcript  \n",
       "0  If you have never learned to program before, y...  \n",
       "1  Is it width and height, or is it height and wi...  \n",
       "5  But we only tell you about the big things. The...  \n",
       "6  Every app that you use is full of data. On Kha...  \n",
       "7  Some of you might even be mad that I am pronou...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_base(x: str):\n",
    "        # remove any html tags\n",
    "        x = BeautifulSoup(x, \"html.parser\").get_text(separator=\" \")\n",
    "        # # set all to lower\n",
    "        # x = x.lower()\n",
    "        # clean up the contractions\n",
    "        x = contractions.fix(x)\n",
    "        # remove accended characters\n",
    "        x = unidecode.unidecode(x)\n",
    "        # # remove stopwords: https://stackoverflow.com/questions/19560498/faster-way-to-remove-stop-words-in-python\n",
    "        # x = ' '.join([word for word in x.split() if word not in cachedStopWords]) # slower to use word tokenize\n",
    "        # # fix punctuation spacing\n",
    "        # x = re.sub(r'(?<=[\\.\\,\\?])(?=[^\\s])', r' ', x)\n",
    "        # # strip punctuation\n",
    "        # x = re.sub(r'[\\.\\,\\?\\\\\\/\\<\\>\\;\\:\\[\\]\\{\\}]', r'', x)\n",
    "        # strip quotes\n",
    "        x = x.replace('\\'', '').replace('\\\"', '')\n",
    "        # remove some actions\n",
    "        remove_list = ['(Laughter)', '(laughter)', '(Music)', '(music)', '(Music ends)', '(Audience cheers)', '(Applause)', '(Applause ends)', '(Applause continues)', '(Bells)', '(Trumpet)', '(Clears throat)']\n",
    "        x = ' '.join([word for word in x.split() if word not in remove_list])\n",
    "        # remove extraneous items\n",
    "        x = x.replace(' -- ', '').replace(' .. ', ' ').replace(' ... ', ' ')\n",
    "        # remove extra whitespace\n",
    "        x = ' '.join(x.strip().split())\n",
    "        # # may want to add lematization\n",
    "        # x = ' '.join([lemmatizer.lemmatize(word) for word in x.split()])\n",
    "        # remove some of the extra bracket tags\n",
    "        x = re.sub(r\"\\s{2,}\", \" \", re.sub(r\"[\\(\\[\\{][^\\)\\]\\}]*[\\)\\]\\}]\", \"\", x))\n",
    "        return x\n",
    "\n",
    "def remove_first_sentence(doc):\n",
    "    \"\"\"\n",
    "    This removes the first sentence of a document. We use this to remove all narrator / speaker tags, and\n",
    "    to remove unnecessary introductory sentences that most transcripts have\n",
    "    \"\"\"\n",
    "    return ' '.join(nltk.sent_tokenize(doc)[1:])\n",
    "\n",
    "transcripts = khan['transcript']\n",
    "transcripts = transcripts.progress_apply(remove_first_sentence)\n",
    "transcripts = transcripts.progress_apply(clean_base)\n",
    "khan['clean_transcript'] = transcripts\n",
    "khan.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cae23140-a94c-48e5-ad64-d309ef9d3e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_write = \"\"\n",
    "for transcript in khan['clean_transcript']:\n",
    "    text_to_write += transcript.replace(\"\\n\", \" \") + \"\\n\"\n",
    "corpus_fp = Path(\"transcripts.txt\")\n",
    "with open(corpus_fp, 'w') as f:\n",
    "    f.write(text_to_write)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12127263-91e8-42cf-9da0-3db8e3479054",
   "metadata": {},
   "source": [
    "## Step 2 is to try and train the numpy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c3467c0d-c3b8-4ed6-8d91-ddeb080a2c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy_ml.ngram import GoodTuringNGram\n",
    "lm = GoodTuringNGram(N=3, conf=1.96, unk=True, filter_stopwords=False, filter_punctuation=False)\n",
    "lm.train(corpus_fp=corpus_fp, vocab=None, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8ec0f534-fe39-4831-a372-42ec12f63d95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-11.238087595339199"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.log_prob([\"but\", \"how\", \"do\"], N=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1ea5c064-b1d4-464f-84ab-fcf18123ee25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-14.421354281382715"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.log_prob([\"another\", \"two\", \"electrons\"], N=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c51aeaa4-7246-4616-b430-e1fbec78b6db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.completions([\"do\", \"morning\"], N=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d9a5279d-5eca-4229-a577-5e23f631fd09",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2132336/1757118717.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'<bol>'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_sentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/numpy_ml/ngram/ngram.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, N, seed_words, n_sentences)\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseed_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mcounter\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mn_sentences\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             \u001b[0mnextw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompletions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m             \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# renormalize probs if smoothed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0mnext_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnextw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/numpy_ml/ngram/ngram.py\u001b[0m in \u001b[0;36mcompletions\u001b[0;34m(self, words, N)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mbase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mN\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcounts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m                 \u001b[0mc_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_ngram_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0mprobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lm.generate(N=3, seed_words=['<bol>'], n_sentences=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d086eb8-5987-443c-ba28-12c3a0a92501",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20560f91-e97c-42dd-a489-de4ae5b07f13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2330315-39a8-4705-9e05-570d9660009f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249314aa-9df0-4077-8c31-310720aa613d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8fc62f-1c1b-45f3-bfea-108f31fe8598",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71260d29-6912-4b54-8cca-69310e567d7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666ed10d-1d5a-458b-bec2-b237b86b66fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fd325f-b076-4640-9a57-024f0e82e3e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1f3b74-625d-4004-95a0-25cf6b87da01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bcd788-8746-493e-ab78-d6e6adfcb83e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fccf102-87e2-4fdc-8545-504c3012c2cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04439fcb-427e-486a-b80c-dbc0abd2cc4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e845f5b8-d661-46fa-ada4-418e3e9bf842",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316cd327-4109-479d-88af-2cd8cfa995e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66c0d8c-f3ae-4425-a176-4dc7bd8a978d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c87e41-5b5a-4a8f-8c18-94771c628e51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6294aaa-c537-4e0d-94bd-31046ecc5c37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe3d3d0-af0d-48f6-acc2-0bdf2a7fd4a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265f9dae-2c20-4767-b104-759b5c99073e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
