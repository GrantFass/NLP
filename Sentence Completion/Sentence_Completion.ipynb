{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Completion\n",
    "This notebook is used for a final project as a part of the CS 4980 Natural Language Processing course at the Milwaukee School of Engineering during the spring term of 2023. This notebook was created by [Grant Fass](grantfass@gmail.com) and [Nicholas Kaja](kajan@msoe.edu). The following notebook will explore the problem of natural language sentence completion. This notebook can also be reached directly through the following GitHub repository: [https://github.com/GrantFass/NLP/tree/main/Sentence%20Completion](https://github.com/GrantFass/NLP/tree/main/Sentence%20Completion)\n",
    "\n",
    "Natural language sentence completion involves determining what word best fits in a blank present in a sentence. This type of question is typically found on the Scholastic Aptitude Test (SAT). It is useful because it can measure the performance of a language model (LM) on questions that educational experts deem important. LMs that perform well on this type of problem will likely perform better on broader tasks.\n",
    "\n",
    "The primary method for forming a sentence completion model is to compute the probability of each possible sentence tehn choose the most probable option. This probability computation can be done using n-grams, Latent Semantic Analysis (LSA), and Syntactic Dependency Trees. Some research has also been done into combining these with methods of preserving long-range dependencies in the sentences. Of these options, N-grams is one of the easiest starting points due to its ease of implementation and understanding. N-grams also allow for sufficient variations over the base model such as different N-gram algorithms, different values of N, and different methods of tokenization.\n",
    "\n",
    "We will be constructing our models from a dataset of Khan Academy lecture transcripts. This dataset was scraped using BeautifulSoup during January of 2023 by Nicholas Kaja as a part of a senior design project. The performance of our model will be evaluated on a [SAT Question Dataset](https://github.com/ctr4si/sentence-completion/tree/master/data/completion). After evaluation, we plan to take our best performing model and apply it to sentence generation. This will allow us to get a better feel for how well it performs. Once our model is working, we also plan to implement some additional features such as Named Entity Recognition (NER). If there is enough time we also plan to investigate using the [OpenAI tokenizer](https://platform.openai.com/tokenizer) instead of the [NLTK Word Tokenizer](https://www.nltk.org/api/nltk.tokenize.html) or the [SpaCy Tokenizer](https://spacy.io/api/tokenizer). The OpenAI tokenizer has a [python package found on github](https://github.com/openai/tiktoken).\n",
    "\n",
    "For more information please see the Data Collection And Processing document as well as the Project Background document. Both of these documents can be found in the repository under the Sentence Completion directory.\n",
    "\n",
    "## Project Demo:\n",
    "Note that this file is mainly used for training and testing the model we build. It also includes the explanations of processes. To view an interactive demo please visit the website located at [grantfass.org](http://grantfass.org).\n",
    "\n",
    "## Running this Notebook:\n",
    "DO NOT RUN THIS NOTEBOOK LOCALLY. This notebook will require over 32gb of memory and a lot of CPU time. Estimated runtime for this notebook is around 15 hours.\n",
    "\n",
    "## Pip Installations\n",
    "The below magic command should install all of the required python packages to run this project. If it does not run successfully please try to run the command through an admin shell instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in /usr/local/anaconda3/lib/python3.7/site-packages (from -r requirements.txt (line 1)) (4.11.1)\n",
      "Requirement already satisfied: contractions in /home/fassg/.local/lib/python3.7/site-packages (from -r requirements.txt (line 2)) (0.1.73)\n",
      "Requirement already satisfied: matplotlib in /usr/local/anaconda3/lib/python3.7/site-packages (from -r requirements.txt (line 3)) (3.5.1)\n",
      "Requirement already satisfied: nltk in /usr/local/anaconda3/lib/python3.7/site-packages (from -r requirements.txt (line 4)) (3.7)\n",
      "Requirement already satisfied: numpy in /home/fassg/.local/lib/python3.7/site-packages (from -r requirements.txt (line 5)) (1.21.5)\n",
      "Requirement already satisfied: pandas in /home/fassg/.local/lib/python3.7/site-packages (from -r requirements.txt (line 6)) (1.3.5)\n",
      "Requirement already satisfied: scikit_learn in /home/fassg/.local/lib/python3.7/site-packages (from -r requirements.txt (line 7)) (1.0.2)\n",
      "Requirement already satisfied: seaborn in /home/fassg/.local/lib/python3.7/site-packages (from -r requirements.txt (line 8)) (0.11.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/anaconda3/lib/python3.7/site-packages (from -r requirements.txt (line 9)) (4.64.0)\n",
      "Requirement already satisfied: Unidecode in /usr/local/anaconda3/lib/python3.7/site-packages (from -r requirements.txt (line 10)) (1.2.0)\n",
      "Requirement already satisfied: ipywidgets in /usr/local/anaconda3/lib/python3.7/site-packages (from -r requirements.txt (line 11)) (7.6.5)\n",
      "Requirement already satisfied: numpy_ml in /home/fassg/.local/lib/python3.7/site-packages (from -r requirements.txt (line 12)) (0.1.2)\n",
      "Requirement already satisfied: gym in /home/fassg/.local/lib/python3.7/site-packages (from -r requirements.txt (line 13)) (0.26.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/anaconda3/lib/python3.7/site-packages (from beautifulsoup4->-r requirements.txt (line 1)) (2.3.1)\n",
      "Requirement already satisfied: textsearch>=0.0.21 in /home/fassg/.local/lib/python3.7/site-packages (from contractions->-r requirements.txt (line 2)) (0.0.24)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/anaconda3/lib/python3.7/site-packages (from matplotlib->-r requirements.txt (line 3)) (2.8.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/fassg/.local/lib/python3.7/site-packages (from matplotlib->-r requirements.txt (line 3)) (9.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/anaconda3/lib/python3.7/site-packages (from matplotlib->-r requirements.txt (line 3)) (21.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/anaconda3/lib/python3.7/site-packages (from matplotlib->-r requirements.txt (line 3)) (1.4.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/anaconda3/lib/python3.7/site-packages (from matplotlib->-r requirements.txt (line 3)) (4.25.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/anaconda3/lib/python3.7/site-packages (from matplotlib->-r requirements.txt (line 3)) (0.11.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/anaconda3/lib/python3.7/site-packages (from matplotlib->-r requirements.txt (line 3)) (3.0.4)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/anaconda3/lib/python3.7/site-packages (from nltk->-r requirements.txt (line 4)) (2022.7.9)\n",
      "Requirement already satisfied: joblib in /usr/local/anaconda3/lib/python3.7/site-packages (from nltk->-r requirements.txt (line 4)) (1.1.0)\n",
      "Requirement already satisfied: click in /usr/local/anaconda3/lib/python3.7/site-packages (from nltk->-r requirements.txt (line 4)) (8.0.4)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/anaconda3/lib/python3.7/site-packages (from pandas->-r requirements.txt (line 6)) (2022.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/fassg/.local/lib/python3.7/site-packages (from scikit_learn->-r requirements.txt (line 7)) (3.1.0)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /usr/local/anaconda3/lib/python3.7/site-packages (from scikit_learn->-r requirements.txt (line 7)) (1.7.3)\n",
      "Requirement already satisfied: ipython>=4.0.0 in /usr/local/anaconda3/lib/python3.7/site-packages (from ipywidgets->-r requirements.txt (line 11)) (7.31.1)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/anaconda3/lib/python3.7/site-packages (from ipywidgets->-r requirements.txt (line 11)) (6.9.1)\n",
      "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/anaconda3/lib/python3.7/site-packages (from ipywidgets->-r requirements.txt (line 11)) (0.2.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/anaconda3/lib/python3.7/site-packages (from ipywidgets->-r requirements.txt (line 11)) (5.1.1)\n",
      "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/anaconda3/lib/python3.7/site-packages (from ipywidgets->-r requirements.txt (line 11)) (3.5.2)\n",
      "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/anaconda3/lib/python3.7/site-packages (from ipywidgets->-r requirements.txt (line 11)) (5.3.0)\n",
      "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/anaconda3/lib/python3.7/site-packages (from ipywidgets->-r requirements.txt (line 11)) (1.0.0)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in /home/fassg/.local/lib/python3.7/site-packages (from gym->-r requirements.txt (line 13)) (0.0.8)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in /home/fassg/.local/lib/python3.7/site-packages (from gym->-r requirements.txt (line 13)) (6.6.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/anaconda3/lib/python3.7/site-packages (from gym->-r requirements.txt (line 13)) (2.0.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/fassg/.local/lib/python3.7/site-packages (from importlib-metadata>=4.8.0->gym->-r requirements.txt (line 13)) (3.15.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/fassg/.local/lib/python3.7/site-packages (from importlib-metadata>=4.8.0->gym->-r requirements.txt (line 13)) (4.5.0)\n",
      "Requirement already satisfied: nest-asyncio in /usr/local/anaconda3/lib/python3.7/site-packages (from ipykernel>=4.5.1->ipywidgets->-r requirements.txt (line 11)) (1.5.5)\n",
      "Requirement already satisfied: matplotlib-inline<0.2.0,>=0.1.0 in /usr/local/anaconda3/lib/python3.7/site-packages (from ipykernel>=4.5.1->ipywidgets->-r requirements.txt (line 11)) (0.1.2)\n",
      "Requirement already satisfied: jupyter-client<8.0 in /usr/local/anaconda3/lib/python3.7/site-packages (from ipykernel>=4.5.1->ipywidgets->-r requirements.txt (line 11)) (7.2.2)\n",
      "Requirement already satisfied: debugpy<2.0,>=1.0.0 in /usr/local/anaconda3/lib/python3.7/site-packages (from ipykernel>=4.5.1->ipywidgets->-r requirements.txt (line 11)) (1.5.1)\n",
      "Requirement already satisfied: tornado<7.0,>=4.2 in /usr/local/anaconda3/lib/python3.7/site-packages (from ipykernel>=4.5.1->ipywidgets->-r requirements.txt (line 11)) (6.1)\n",
      "Requirement already satisfied: pickleshare in /usr/local/anaconda3/lib/python3.7/site-packages (from ipython>=4.0.0->ipywidgets->-r requirements.txt (line 11)) (0.7.5)\n",
      "Requirement already satisfied: pygments in /usr/local/anaconda3/lib/python3.7/site-packages (from ipython>=4.0.0->ipywidgets->-r requirements.txt (line 11)) (2.11.2)\n",
      "Requirement already satisfied: setuptools>=18.5 in /usr/local/anaconda3/lib/python3.7/site-packages (from ipython>=4.0.0->ipywidgets->-r requirements.txt (line 11)) (63.4.1)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/anaconda3/lib/python3.7/site-packages (from ipython>=4.0.0->ipywidgets->-r requirements.txt (line 11)) (3.0.20)\n",
      "Requirement already satisfied: decorator in /usr/local/anaconda3/lib/python3.7/site-packages (from ipython>=4.0.0->ipywidgets->-r requirements.txt (line 11)) (5.1.1)\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/local/anaconda3/lib/python3.7/site-packages (from ipython>=4.0.0->ipywidgets->-r requirements.txt (line 11)) (4.8.0)\n",
      "Requirement already satisfied: jedi>=0.16 in /usr/local/anaconda3/lib/python3.7/site-packages (from ipython>=4.0.0->ipywidgets->-r requirements.txt (line 11)) (0.18.1)\n",
      "Requirement already satisfied: backcall in /usr/local/anaconda3/lib/python3.7/site-packages (from ipython>=4.0.0->ipywidgets->-r requirements.txt (line 11)) (0.2.0)\n",
      "Requirement already satisfied: fastjsonschema in /usr/local/anaconda3/lib/python3.7/site-packages (from nbformat>=4.2.0->ipywidgets->-r requirements.txt (line 11)) (2.15.1)\n",
      "Requirement already satisfied: jsonschema>=2.6 in /usr/local/anaconda3/lib/python3.7/site-packages (from nbformat>=4.2.0->ipywidgets->-r requirements.txt (line 11)) (4.4.0)\n",
      "Requirement already satisfied: jupyter-core in /usr/local/anaconda3/lib/python3.7/site-packages (from nbformat>=4.2.0->ipywidgets->-r requirements.txt (line 11)) (4.10.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/anaconda3/lib/python3.7/site-packages (from python-dateutil>=2.7->matplotlib->-r requirements.txt (line 3)) (1.16.0)\n",
      "Requirement already satisfied: anyascii in /home/fassg/.local/lib/python3.7/site-packages (from textsearch>=0.0.21->contractions->-r requirements.txt (line 2)) (0.3.2)\n",
      "Requirement already satisfied: pyahocorasick in /home/fassg/.local/lib/python3.7/site-packages (from textsearch>=0.0.21->contractions->-r requirements.txt (line 2)) (2.0.0)\n",
      "Requirement already satisfied: notebook>=4.4.1 in /usr/local/anaconda3/lib/python3.7/site-packages (from widgetsnbextension~=3.5.0->ipywidgets->-r requirements.txt (line 11)) (6.4.12)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/anaconda3/lib/python3.7/site-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets->-r requirements.txt (line 11)) (0.8.3)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /usr/local/anaconda3/lib/python3.7/site-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets->-r requirements.txt (line 11)) (21.4.0)\n",
      "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/anaconda3/lib/python3.7/site-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets->-r requirements.txt (line 11)) (5.2.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/anaconda3/lib/python3.7/site-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets->-r requirements.txt (line 11)) (0.18.0)\n",
      "Requirement already satisfied: pyzmq>=22.3 in /usr/local/anaconda3/lib/python3.7/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets->-r requirements.txt (line 11)) (23.2.0)\n",
      "Requirement already satisfied: entrypoints in /usr/local/anaconda3/lib/python3.7/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets->-r requirements.txt (line 11)) (0.4)\n",
      "Requirement already satisfied: argon2-cffi in /usr/local/anaconda3/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->-r requirements.txt (line 11)) (21.3.0)\n",
      "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/anaconda3/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->-r requirements.txt (line 11)) (1.8.0)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /usr/local/anaconda3/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->-r requirements.txt (line 11)) (0.13.1)\n",
      "Requirement already satisfied: prometheus-client in /usr/local/anaconda3/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->-r requirements.txt (line 11)) (0.14.1)\n",
      "Requirement already satisfied: nbconvert>=5 in /usr/local/anaconda3/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->-r requirements.txt (line 11)) (6.4.4)\n",
      "Requirement already satisfied: jinja2 in /usr/local/anaconda3/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->-r requirements.txt (line 11)) (2.10.1)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/anaconda3/lib/python3.7/site-packages (from pexpect>4.3->ipython>=4.0.0->ipywidgets->-r requirements.txt (line 11)) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/anaconda3/lib/python3.7/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets->-r requirements.txt (line 11)) (0.2.5)\n",
      "Requirement already satisfied: defusedxml in /usr/local/anaconda3/lib/python3.7/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->-r requirements.txt (line 11)) (0.7.1)\n",
      "Requirement already satisfied: jupyterlab-pygments in /usr/local/anaconda3/lib/python3.7/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->-r requirements.txt (line 11)) (0.1.2)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/anaconda3/lib/python3.7/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->-r requirements.txt (line 11)) (0.8.4)\n",
      "Requirement already satisfied: bleach in /usr/local/anaconda3/lib/python3.7/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->-r requirements.txt (line 11)) (4.1.0)\n",
      "Requirement already satisfied: testpath in /usr/local/anaconda3/lib/python3.7/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->-r requirements.txt (line 11)) (0.6.0)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/anaconda3/lib/python3.7/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->-r requirements.txt (line 11)) (1.5.0)\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in /usr/local/anaconda3/lib/python3.7/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->-r requirements.txt (line 11)) (0.5.13)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/anaconda3/lib/python3.7/site-packages (from jinja2->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->-r requirements.txt (line 11)) (2.0.1)\n",
      "Requirement already satisfied: argon2-cffi-bindings in /usr/local/anaconda3/lib/python3.7/site-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->-r requirements.txt (line 11)) (21.2.0)\n",
      "Requirement already satisfied: cffi>=1.0.1 in /usr/local/anaconda3/lib/python3.7/site-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->-r requirements.txt (line 11)) (1.15.1)\n",
      "Requirement already satisfied: webencodings in /usr/local/anaconda3/lib/python3.7/site-packages (from bleach->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->-r requirements.txt (line 11)) (0.5.1)\n",
      "Requirement already satisfied: pycparser in /usr/local/anaconda3/lib/python3.7/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->-r requirements.txt (line 11)) (2.21)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "This cell contains the imports and some setup functions. This is mostly generalized which means there are extra imports as of now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package names to /home/fassg/nltk_data...\n",
      "[nltk_data]   Package names is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/fassg/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package state_union to\n",
      "[nltk_data]     /home/fassg/nltk_data...\n",
      "[nltk_data]   Package state_union is already up-to-date!\n",
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     /home/fassg/nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n",
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     /home/fassg/nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/fassg/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/fassg/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/fassg/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict\n",
    "import pandas as pd\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "import nltk.data\n",
    "import re\n",
    "import contractions\n",
    "from bs4 import BeautifulSoup\n",
    "import unidecode\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score\n",
    "from sklearn.metrics import f1_score, recall_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import math\n",
    "import numpy as np\n",
    "import copy\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import pickle\n",
    "from nltk.lm.preprocessing import flatten\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "from nltk.util import pad_sequence\n",
    "from nltk.util import everygrams\n",
    "from nltk.lm import MLE, KneserNeyInterpolated, Laplace, AbsoluteDiscountingInterpolated\n",
    "from functools import partial\n",
    "\n",
    "nltk.download([\n",
    "\"names\",\n",
    "\"stopwords\",\n",
    "\"state_union\",\n",
    "\"twitter_samples\",\n",
    "\"movie_reviews\",\n",
    "\"averaged_perceptron_tagger\",\n",
    "\"vader_lexicon\",\n",
    "\"punkt\",\n",
    "])\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "encoder = LabelEncoder()\n",
    "sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "tqdm.pandas()\n",
    "n = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading  \n",
    "This section is used to load in all of the training data for the vocabulary. This comes from the 5 csv files of Khan Academy lectures. These files are all combined into a single dataframe for easier use and analysis. In the future we may need to include other sources of data such as ted talks or even ebooks in order to widen our vocabulary.\n",
    "\n",
    "We also load in the SAT dataset here. This is a short dataset that we are primarily using as our testing set. The goal here is to guess which response option, out of five, is the correct response for a fill-in-the-blank SAT question. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading books1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6645e0785634bf3b387858188ab4fc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17868 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Project Gutenberg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3dfcc388f2b495d84e9038f731d7a12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2808 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error reading Datasets/Input/D1.7GB/999.txt with exception: 'utf-8' codec can't decode byte 0xab in position 211859: invalid start byte\n",
      "Reading Epubs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54e5802847fd4249b91e86471390b42a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Khan\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc71a6301c694e7cafb8bfa8b03abd05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8044 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Ted\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12643784d1c84da9bb034b1a6cd99cac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2461 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31180 documents\n"
     ]
    }
   ],
   "source": [
    "def read_in_txt_file(fp):\n",
    "    try:\n",
    "        with open(Path(data_path), 'r', encoding='utf8') as f:\n",
    "            documents.append(f.read())\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {fp} with exception: {e}\")\n",
    "\n",
    "documents = []\n",
    "print(\"Reading books1\")\n",
    "for data_path in tqdm(list(Path('Datasets/Input/books1/epubtxt').glob('**/*.txt'))):\n",
    "    read_in_txt_file(data_path)\n",
    "print(\"Reading Project Gutenberg\")\n",
    "for data_path in tqdm(list(Path('Datasets/Input/D1.7GB').glob('**/*.txt'))):\n",
    "    read_in_txt_file(data_path)\n",
    "print(\"Reading Epubs\")\n",
    "for data_path in tqdm(list(Path('Datasets/Input/Epubs').glob('**/*.txt'))):\n",
    "    read_in_txt_file(data_path)\n",
    "print(\"Reading Khan\")\n",
    "for data_path in tqdm(list(Path('Datasets/Input/Khan').glob('**/*.txt'))):\n",
    "    read_in_txt_file(data_path)\n",
    "print(\"Reading Ted\")\n",
    "for data_path in tqdm(list(Path('Datasets/Input/Ted').glob('**/*.txt'))):\n",
    "    read_in_txt_file(data_path)\n",
    "print(f\"{len(documents)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing_df = pd.read_csv(Path(\"Datasets/KhanAcademy/Computing.csv\"))\n",
    "# computing_df = computing_df.dropna()\n",
    "\n",
    "# economics_df = pd.read_csv(Path(\"Datasets/KhanAcademy/Economics.csv\"))\n",
    "# economics_df = economics_df.dropna()\n",
    "\n",
    "# humanities_df = pd.read_csv(Path(\"Datasets/KhanAcademy/Humanities.csv\"))\n",
    "# humanities_df = humanities_df.dropna()\n",
    "\n",
    "# math_df = pd.read_csv(Path(\"Datasets/KhanAcademy/Math.csv\"))\n",
    "# math_df = math_df.dropna()\n",
    "\n",
    "# science_df = pd.read_csv(Path(\"Datasets/KhanAcademy/Science.csv\"))\n",
    "# science_df = science_df.dropna()\n",
    "\n",
    "# khan_dfs = [computing_df, economics_df, humanities_df, math_df, science_df]\n",
    "# khan = pd.concat(khan_dfs, axis=0)\n",
    "# khan.info(verbose=True, show_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 152 entries, 0 to 151\n",
      "Data columns (total 13 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        152 non-null    int64 \n",
      " 1   ans       152 non-null    object\n",
      " 2   question  152 non-null    object\n",
      " 3   a)        152 non-null    object\n",
      " 4   b)        152 non-null    object\n",
      " 5   c)        152 non-null    object\n",
      " 6   d)        152 non-null    object\n",
      " 7   e)        152 non-null    object\n",
      " 8   year      152 non-null    int64 \n",
      " 9   sec       152 non-null    int64 \n",
      " 10  num       152 non-null    int64 \n",
      " 11  diff      152 non-null    object\n",
      " 12  blanks    152 non-null    int64 \n",
      "dtypes: int64(5), object(8)\n",
      "memory usage: 15.6+ KB\n"
     ]
    }
   ],
   "source": [
    "sat = pd.read_csv(Path(\"Datasets/SAT_Question_Dataset.csv\"))\n",
    "sat.info(verbose=True, show_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning\n",
    "This section is dedicated to cleaning up some of our vocabulary dataset before it is used to create the vocabulary. We have decided, for simplicity, to remove the first sentence of every transcript in the dataset. This will allow us to easily remove all of the tagging information such as \"[Instructor]\" and \"- Speaker 1:\". Most of the first sentences are greetings and introductions. As such they are not necessarily as important to answering the SAT question dataset. We have also elected to expand contractions, remove html tags, remove quotes, and replace accented characters.\n",
    "\n",
    "The same general cleaning method is run over our testing data as well. The first sentence removal is not performed on the testing data.\n",
    "\n",
    "[Note] Before this section, as a part of the cleaning, we will want to perform SpaCy NER. Use SpaCy NER to identify the multi-word-expressions then use the [nltk.tokenize.mwe module](https://www.nltk.org/api/nltk.tokenize.mwe.html) to combine the tokens - Grant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_base(x: str):\n",
    "        # remove any html tags\n",
    "        x = BeautifulSoup(x, \"html.parser\").get_text(separator=\" \")\n",
    "        # # set all to lower\n",
    "        # x = x.lower()\n",
    "        # clean up the contractions\n",
    "        x = contractions.fix(x)\n",
    "        # remove accended characters\n",
    "        x = unidecode.unidecode(x)\n",
    "        # # remove stopwords: https://stackoverflow.com/questions/19560498/faster-way-to-remove-stop-words-in-python\n",
    "        # x = ' '.join([word for word in x.split() if word not in cachedStopWords]) # slower to use word tokenize\n",
    "        # # fix punctuation spacing\n",
    "        # x = re.sub(r'(?<=[\\.\\,\\?])(?=[^\\s])', r' ', x)\n",
    "        # # strip punctuation\n",
    "        # x = re.sub(r'[\\.\\,\\?\\\\\\/\\<\\>\\;\\:\\[\\]\\{\\}]', r'', x)\n",
    "        # strip quotes\n",
    "        # x = x.replace('\\'', '').replace('\\\"', '')\n",
    "        x = x.replace('\\\"', '')\n",
    "        # # remove some actions\n",
    "        # remove_list = ['(Laughter)', '(laughter)', '(Music)', '(music)', '(Music ends)', '(Audience cheers)', '(Applause)', '(Applause ends)', '(Applause continues)', '(Bells)', '(Trumpet)', '(Clears throat)']\n",
    "        # x = ' '.join([word for word in x.split() if word not in remove_list])\n",
    "        # remove extraneous items\n",
    "        x = x.replace(' -- ', '').replace(' .. ', ' ').replace(' ... ', ' ')\n",
    "        # remove extra whitespace\n",
    "        x = ' '.join(x.strip().split())\n",
    "        # # may want to add lematization\n",
    "        # x = ' '.join([lemmatizer.lemmatize(word) for word in x.split()])\n",
    "        # remove some of the extra bracket tags\n",
    "        x = re.sub(r\"\\s{2,}\", \" \", re.sub(r\"[\\(\\[\\{][^\\)\\]\\}]*[\\)\\]\\}]\", \"\", x))\n",
    "        return x\n",
    "\n",
    "def remove_first_sentence(doc):\n",
    "    \"\"\"\n",
    "    This removes the first sentence of a document. We use this to remove all narrator / speaker tags, and\n",
    "    to remove unnecessary introductory sentences that most transcripts have\n",
    "    \"\"\"\n",
    "    return ' '.join(nltk.sent_tokenize(doc)[1:])\n",
    "\n",
    "# transcripts = khan['transcript']\n",
    "# # transcripts = transcripts.progress_apply(remove_first_sentence)\n",
    "# transcripts = transcripts.progress_apply(clean_base)\n",
    "# khan['clean_transcript'] = transcripts\n",
    "# khan.head()\n",
    "\n",
    "# directory_path = Path(new_dir)\n",
    "# documents = []\n",
    "# for data_path in tqdm(list(directory_path.glob(\"**/*.txt\"))):\n",
    "#     text = \"\"\n",
    "#     with open(Path(data_path), \"r\", encoding=\"utf-8\") as f:\n",
    "#         text = clean_base(f.read())\n",
    "#         documents.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "410253de29e942e1b361b10ed64581f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/152 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eeb11b16b34a47dc842d6ba808786103",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/152 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bdeab002c4940cb9dcb6dd290aa4ca6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/152 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cecc9efeb998407b8f980b2a1d094f36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/152 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b43fb04f6a2143848f5eb4c018804157",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/152 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa5c1f41a026410da0fe4c16ce830ad6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/152 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>ans</th>\n",
       "      <th>question</th>\n",
       "      <th>a)</th>\n",
       "      <th>b)</th>\n",
       "      <th>c)</th>\n",
       "      <th>d)</th>\n",
       "      <th>e)</th>\n",
       "      <th>year</th>\n",
       "      <th>sec</th>\n",
       "      <th>num</th>\n",
       "      <th>diff</th>\n",
       "      <th>blanks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>e</td>\n",
       "      <td>Much of our knowledge of dinosaurs comes from ...</td>\n",
       "      <td>Much of our knowledge of dinosaurs comes from ...</td>\n",
       "      <td>Much of our knowledge of dinosaurs comes from ...</td>\n",
       "      <td>Much of our knowledge of dinosaurs comes from ...</td>\n",
       "      <td>Much of our knowledge of dinosaurs comes from ...</td>\n",
       "      <td>Much of our knowledge of dinosaurs comes from ...</td>\n",
       "      <td>2001</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>c</td>\n",
       "      <td>Responding to criticism that the script was ra...</td>\n",
       "      <td>Responding to criticism that the script was ra...</td>\n",
       "      <td>Responding to criticism that the script was ra...</td>\n",
       "      <td>Responding to criticism that the script was ra...</td>\n",
       "      <td>Responding to criticism that the script was ra...</td>\n",
       "      <td>Responding to criticism that the script was ra...</td>\n",
       "      <td>2001</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>e</td>\n",
       "      <td>Vernal pools are among the most _____ of ponds...</td>\n",
       "      <td>Vernal pools are among the most transitory of ...</td>\n",
       "      <td>Vernal pools are among the most anachronistic ...</td>\n",
       "      <td>Vernal pools are among the most immutable of p...</td>\n",
       "      <td>Vernal pools are among the most itinerant of p...</td>\n",
       "      <td>Vernal pools are among the most ephemeral of p...</td>\n",
       "      <td>2001</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>e</td>\n",
       "      <td>During the 1990's, Shanghai benefited from an ...</td>\n",
       "      <td>During the 1990's, Shanghai benefited from an ...</td>\n",
       "      <td>During the 1990's, Shanghai benefited from an ...</td>\n",
       "      <td>During the 1990's, Shanghai benefited from an ...</td>\n",
       "      <td>During the 1990's, Shanghai benefited from an ...</td>\n",
       "      <td>During the 1990's, Shanghai benefited from an ...</td>\n",
       "      <td>2001</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>b</td>\n",
       "      <td>Many subatomic nuclear particles are _____ and...</td>\n",
       "      <td>Many subatomic nuclear particles are unstable ...</td>\n",
       "      <td>Many subatomic nuclear particles are elusive a...</td>\n",
       "      <td>Many subatomic nuclear particles are minute an...</td>\n",
       "      <td>Many subatomic nuclear particles are charged a...</td>\n",
       "      <td>Many subatomic nuclear particles are tenuous a...</td>\n",
       "      <td>2001</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id ans                                           question  \\\n",
       "0   1   e  Much of our knowledge of dinosaurs comes from ...   \n",
       "1   2   c  Responding to criticism that the script was ra...   \n",
       "2   3   e  Vernal pools are among the most _____ of ponds...   \n",
       "3   4   e  During the 1990's, Shanghai benefited from an ...   \n",
       "4   5   b  Many subatomic nuclear particles are _____ and...   \n",
       "\n",
       "                                                  a)  \\\n",
       "0  Much of our knowledge of dinosaurs comes from ...   \n",
       "1  Responding to criticism that the script was ra...   \n",
       "2  Vernal pools are among the most transitory of ...   \n",
       "3  During the 1990's, Shanghai benefited from an ...   \n",
       "4  Many subatomic nuclear particles are unstable ...   \n",
       "\n",
       "                                                  b)  \\\n",
       "0  Much of our knowledge of dinosaurs comes from ...   \n",
       "1  Responding to criticism that the script was ra...   \n",
       "2  Vernal pools are among the most anachronistic ...   \n",
       "3  During the 1990's, Shanghai benefited from an ...   \n",
       "4  Many subatomic nuclear particles are elusive a...   \n",
       "\n",
       "                                                  c)  \\\n",
       "0  Much of our knowledge of dinosaurs comes from ...   \n",
       "1  Responding to criticism that the script was ra...   \n",
       "2  Vernal pools are among the most immutable of p...   \n",
       "3  During the 1990's, Shanghai benefited from an ...   \n",
       "4  Many subatomic nuclear particles are minute an...   \n",
       "\n",
       "                                                  d)  \\\n",
       "0  Much of our knowledge of dinosaurs comes from ...   \n",
       "1  Responding to criticism that the script was ra...   \n",
       "2  Vernal pools are among the most itinerant of p...   \n",
       "3  During the 1990's, Shanghai benefited from an ...   \n",
       "4  Many subatomic nuclear particles are charged a...   \n",
       "\n",
       "                                                  e)  year  sec  num diff  \\\n",
       "0  Much of our knowledge of dinosaurs comes from ...  2001    1    1    1   \n",
       "1  Responding to criticism that the script was ra...  2001    1    2    1   \n",
       "2  Vernal pools are among the most ephemeral of p...  2001    1    3    2   \n",
       "3  During the 1990's, Shanghai benefited from an ...  2001    1    4    3   \n",
       "4  Many subatomic nuclear particles are tenuous a...  2001    1    5    3   \n",
       "\n",
       "   blanks  \n",
       "0       2  \n",
       "1       2  \n",
       "2       2  \n",
       "3       1  \n",
       "4       2  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for column_name in ['question', 'a)', 'b)', 'c)', 'd)', 'e)']:\n",
    "    sat[column_name] = sat[column_name].progress_apply(clean_base)\n",
    "sat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-Grams\n",
    "This section is used for the tokenization of our text into N-Grams. Various tokenization approaches may yield different results. As such, it is important to test different approaches here. We begin by dividing each of the transcripts into its constituent sentences. These sentences are then each tokenized using the word tokenizer provided by nltk. A start and end of sentence tag is then added to each list of tokens. Next, these tokens are used to generate all possible n-grams below a certain size threshold. Lastly, we create a frequency distribution dictionary for the n-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing Documents...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e0601728dda45148f266ca132ddda88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31180 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Custom Everygram Pipeline...\n",
      "Fitting Language Model...\n",
      "<Vocabulary with cutoff=1 unk_label='<UNK>' and 13652284 items>\n"
     ]
    }
   ],
   "source": [
    "def tokenize_document(document):\n",
    "    return [word_tokenize(t) for t in sent_detector.tokenize(document)]\n",
    "\n",
    "def tokenize_documents(documents):\n",
    "    sents = []\n",
    "    for doc in tqdm(documents):\n",
    "        for sent in tokenize_document(doc):\n",
    "            sents.append(sent)\n",
    "    return sents\n",
    "\n",
    "def pad_sentence(tokens):\n",
    "    return pad_both_ends(tokens, n=2)\n",
    "\n",
    "def pad_sentences(sents):\n",
    "    return [pad_sentence(sent) for sent in sents]\n",
    "\n",
    "def custom_padded_everygam_pipeline(n, tokenized_sents):\n",
    "    # these few lines come directly from the padded_everygram_pipeline source code\n",
    "    padding_fn = partial(pad_both_ends, n=2)\n",
    "    return (\n",
    "        (everygrams(list(padding_fn(sent)), max_len=n) for sent in tokenized_sents),\n",
    "        flatten(map(padding_fn, tokenized_sents)),\n",
    "    )\n",
    "\n",
    "print(\"Tokenizing Documents...\")\n",
    "# sents = tokenize_documents(khan['clean_transcript'])\n",
    "sents = tokenize_documents(documents)\n",
    "print(\"Building Custom Everygram Pipeline...\")\n",
    "train, vocab = custom_padded_everygam_pipeline(n, sents)\n",
    "lm = MLE(n)\n",
    "# lm = Laplace(n)\n",
    "print(\"Fitting Language Model...\")\n",
    "lm.fit(train, vocab)\n",
    "print(lm.vocab)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store Model\n",
    "The next step is to store the freq_dist model for use in future prediction methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved lm.pkl\n"
     ]
    }
   ],
   "source": [
    "def to_pickle(filename, model):\n",
    "    with open(filename, 'wb') as fp:\n",
    "        pickle.dump(model, fp)\n",
    "        print(\"Saved %s\" % (filename))\n",
    "        \n",
    "def from_pickle(filename):\n",
    "    model = []\n",
    "    with open(filename, 'rb') as fp:\n",
    "        model = pickle.load(fp)\n",
    "    return model\n",
    "\n",
    "        \n",
    "to_pickle(filename='lm.pkl', model=lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAT Dataset\n",
    "This section focuses on the processing of individual SAT questions as well as defining a prediction method for the dataset. Below is a sample approach for the prediction method.\n",
    "\n",
    "### SAT Question Prediction Approach\n",
    "Note that this approach is slightly different from what was implemented. One of the main differences is that our tokenization currently includes punctuation as its own token whereas the below example removes punctuation.\n",
    "\n",
    "\"\"\"  \n",
    "Question:  \n",
    "Responding to criticism that the script was rambling and _____, the new screenwriter revised the dialogue for greater succinctness and _____.\n",
    "\n",
    "Possible Solutions:  \n",
    "[('engaging', 'simplicity'), ('subjective', 'abiguity'), ('muddled', 'clarity'), ('terse', 'emptiness'), ('difficult', 'abstraction')]  \n",
    "\"\"\"\n",
    "\n",
    "N-gram size of 4 max\n",
    "1. Change the blanks out to something we can use to mask the individual ones later. For example, [BLANK1] and [BLANK2].\n",
    "    - esponding to criticism that the script was rambling and [BLANK1], the new screenwriter revised the dialogue for greater succinctness and [BLANK2].<\\sent>\n",
    "2. Pull out the possible sliding windows centered around each blank.\n",
    "    - blank 1\n",
    "        - n=4\n",
    "            - ('was', 'rambling', 'and', [BLANK1])\n",
    "            - ('rambling', 'and', [BLANK1], 'the')\n",
    "            - ('and', [BLANK1], 'the', 'new')\n",
    "            - ([BLANK1], 'the', 'new', 'screenwriter')\n",
    "        - n=3\n",
    "            - ('rambling', 'and', [BLANK1])\n",
    "            - ('and', [BLANK1], 'the')\n",
    "            - ([BLANK1], 'the', 'new')\n",
    "        - n=2\n",
    "            - ('and', [BLANK1])\n",
    "            - ([BLANK1], 'the')\n",
    "        - n=1\n",
    "            - ([BLANK1])\n",
    "    - blank 2\n",
    "        - n=4\n",
    "            - ('greater', 'succinctness', 'and', [BLANK2])\n",
    "            - ('succinctness', 'and', [BLANK2], <\\sent>)\n",
    "        - n=3\n",
    "            - ('succinctness', 'and', [BLANK2])\n",
    "            - ('and', [BLANK2], <\\sent>)\n",
    "        - n=2\n",
    "            - ('and', [BLANK2])\n",
    "            - ([BLANK2], <\\sent>)\n",
    "        - n=1\n",
    "            - ([BLANK2])\n",
    "3. For each of the possible solutions:\n",
    "    1. For each of the windows:\n",
    "        1. If the first or second word in the possible solution tuple is not in the vocab then discard it as a possible solution.\n",
    "        2. Replace [BLANK1] with the first item in the possible solution tuple\n",
    "        3. Replace [BLANK2] with the second item in the possible solution tuple\n",
    "        4. Lookup each of the window n-grams against the stored n-grams and determine how many times it occured.\n",
    "        5. What to do if the count for a n-gram is 0 for one word but not for another word?\n",
    "        6. Calculate the probability of each window n-gram\n",
    "        7. sum the logs of the probabilities of each window n-gram. \n",
    "    2. Determine which of the windows had the highest log likelihood and return the associated word tuple.\n",
    "\n",
    "### Scoring Experimentation:\n",
    "This section explains some of the experimentation with the various methods of scoring which prediction was the most accurate in the predict method. The baseline prediction worked by taking the log in base 10 of (the count of the ngram divided by the count of the one smaller ngram). The code for this is shown below. All scores reported in this section are f1-scores. The two datasets that were evaluated were the SAT question dataset (SAT) and the SC-Ques dataset (ESL). The basline performance was 0.155 for SAT and 0.209 for ESL. Out of the possible models it appears that the best prediction accuracy was attained when using lm.score with the Maximum Likelihood Estimator. The most optimal size for n is likely 3 or 4. Quadgrams have a slightly better score, but take longer to train. For testing purposes bigrams may be the best option.\n",
    "\n",
    "```\n",
    "ngram_size = len(new_window)\n",
    "raw_count = freq_dist[tuple(new_window)] # TODO: what if we try to use an ensemble of models with different maximum n-gram sizes?\n",
    "if raw_count > 0:\n",
    "    prior_count = 1\n",
    "    if ngram_size > 1:\n",
    "        prior = new_window[0: len(new_window) - 1]\n",
    "        prior_count = freq_dist[tuple(prior)]\n",
    "    raw_prob = raw_count / prior_count\n",
    "    log_prob = math.log10(raw_prob)\n",
    "    print(log_prob)\n",
    "    log_likelihood += log_prob\n",
    "```\n",
    "\n",
    "#### Bigrams:\n",
    "- MLE + lm.score\n",
    "    - SAT: 0.231\n",
    "    - ESL: 0.312\n",
    "- MLE + lm.logscore\n",
    "    - SAT: 0.122\n",
    "    - ESL: \n",
    "- Laplace + lm.score\n",
    "    - SAT: 0.185\n",
    "    - ESL: 0.268\n",
    "- Laplace + lm.logscore\n",
    "    - SAT: 0.191\n",
    "    - ESL: 0.288\n",
    "- KneserNeyInterpolated\n",
    "    - Of note is that this model is incredibly impractical. It is so slow that it feels like the computer froze most of the time. It also only got a 0.158 on lm.logscore for the SAT dataset.\n",
    "- AbsoluteDiscountingInterpolated\n",
    "    - Similar speed, including training times, to the MLE. Got a 0.184 for SAT on lm.logscore\n",
    "\n",
    "#### Trigrams:\n",
    "- MLE + lm.score\n",
    "    - SAT: 0.231\n",
    "    - ESL: 0.330\n",
    "    \n",
    "#### Quadgrams:\n",
    "- MLE + lm.score\n",
    "    - SAT: 238\n",
    "    - ESL: 335\n",
    "- MLE + lm.logscore\n",
    "    - SAT: 0.07\n",
    "    - ESL: \n",
    "- Laplace + lm.score\n",
    "    - SAT: 0.191\n",
    "    - ESL: 0.271\n",
    "- Laplace + lm.logscore\n",
    "    - SAT: 0.203\n",
    "    - ESL: 0.305\n",
    "\n",
    "#### 7-grams:\n",
    "- MLE + lm.score\n",
    "    - SAT: 0.238\n",
    "    - ESL: 0.336"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_2(row, lm,  mask=\"_____\"):\n",
    "    # tokenize the inbound question\n",
    "    tokens = word_tokenize(row['question'])\n",
    "    question = list(pad_both_ends(tokens, n=2))\n",
    "    # determine where the mask is without knowing how many masks there are.\n",
    "    # note that .index is O(n) so we may as well iterate through ourselves to be more verbose.\n",
    "    mask_indices = []\n",
    "    mask_count = 0\n",
    "    for idx in range(len(question)):\n",
    "        if question[idx] == mask:\n",
    "            # store the index of the mask\n",
    "            mask_indices.append(idx)\n",
    "            # convert the mask to one with a number for future reference\n",
    "            question[idx] = mask + str(mask_count)\n",
    "            mask_count += 1\n",
    "    # pull out the windows\n",
    "    grams = list(everygrams(question, min_len=1, max_len=n))\n",
    "    windows = []\n",
    "    for i in range(len(mask_indices)):\n",
    "        remask = mask + str(i)\n",
    "        for gram in grams:\n",
    "            if remask in gram:\n",
    "                windows.append(gram)\n",
    "    # extract the possible solutions\n",
    "    if mask_indices:\n",
    "        column_names = ['a)', 'b)', 'c)', 'd)', 'e)']\n",
    "        solution_likelihoods = []\n",
    "        for name in column_names:\n",
    "            if name in row and isinstance(row[name], str) and row[name] != \"\":\n",
    "                # tokenize the input\n",
    "                tokens = word_tokenize(row[name])\n",
    "                tokens = list(pad_both_ends(tokens, n=2))\n",
    "                # calculate the probability\n",
    "                log_likelihood = 0\n",
    "                custom_windows = []\n",
    "                for window in windows:\n",
    "                    # # fill in the blanks in the windows with the possible solutions.\n",
    "                    new_window = window\n",
    "                    for i in range(len(mask_indices)):\n",
    "                        remask = mask + str(i)\n",
    "                        new_window = list(map(lambda x: x.replace(remask, tokens[mask_indices[i]]), new_window))\n",
    "                    custom_windows.append(new_window)\n",
    "                    log_likelihood += lm.score(new_window[-1], new_window[0:-1])\n",
    "                solution_likelihoods.append((name, log_likelihood))\n",
    "        solution_likelihoods.sort(key = lambda x: x[1], reverse = True)\n",
    "        ans = solution_likelihoods[0][0]\n",
    "        return ans.replace(\")\", \"\")\n",
    "    \n",
    "    \n",
    "def score(y_true, y_pred):\n",
    "    print(\"%-15s: %.3f\" % (\"[ACCURACY]\", accuracy_score(y_true, y_pred)))\n",
    "    print(\"%-15s: %.3f\" % (\"[PRECISION]\", precision_score(y_true, y_pred, average='weighted')))\n",
    "    print(\"%-15s: %.3f\" % (\"[RECALL]\", recall_score(y_true, y_pred, average='weighted')))\n",
    "    print(\"%-15s: %.3f\" % (\"[F1-SCORE]\", f1_score(y_true, y_pred, average='weighted')))    \n",
    "    \n",
    "def record_model_metrics_as_json(y_true, y_pred):\n",
    "    metrics = {}\n",
    "    metrics['accuracy_score'] = accuracy_score(y_true, y_pred)\n",
    "    metrics['precision_score'] = precision_score(y_true, y_pred, average='weighted')\n",
    "    metrics['recall_score'] = recall_score(y_true, y_pred, average='weighted')\n",
    "    metrics['f1_score'] = f1_score(y_true, y_pred, average='weighted')\n",
    "    return metrics    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = sat.iloc[1]\n",
    "predict_2(test, lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions\n",
    "This section is used to actually compute the predictions for our test data and our given input frequency distribution. The results of the prediction are then evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7a0df0bca6543eaa075c6975cea7252",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/152 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy_score': 0.27631578947368424,\n",
       " 'precision_score': 0.2860102479757085,\n",
       " 'recall_score': 0.27631578947368424,\n",
       " 'f1_score': 0.27690052853930325}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = sat.progress_apply(lambda row: predict_2(row, lm), axis=1)\n",
    "true_values = sat['ans']\n",
    "sat_metrics = record_model_metrics_as_json(true_values, predictions)\n",
    "sat_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conf = confusion_matrix(true_values, predictions)\n",
    "# fig, ax = plt.subplots(figsize=(8,6), dpi=100)\n",
    "# display = ConfusionMatrixDisplay(conf, display_labels=['a', 'b', 'c', 'd', 'e'])\n",
    "# ax.set(title='Confusion Matrix for SAT sentence completion')\n",
    "# display.plot(ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of Results\n",
    "Our model does not perform that well currently. We hypothesize that this is due to both the complexity of the words in the SAT question dataset as well as the size of our input dataset. Our input dataset is only comprised of around 600,000 sentences. Additionally, our input dataset comes from spoken lecture transcripts. While these transcripts should be on fairly complex topics, they still contain mostly spoken language. The SAT question dataset prompts are more complex than the language spoken in our input dataset. This leads to a mismatch between our corpus and training data where most of our n-grams either have low counts or do not exist in our corpus. This could be remedied by expanding the size of our input dataset and thusly our corpus. The research paper that our test dataset came from compiled their corpus on over 1.1 billion words in newspaper articles. Even with this large increase in corpus size they still only achieved a prediction accuracy of just over 50% on the testing dataset. This implies that the test dataset is fairly difficult to get high accuracy scores on.\n",
    "\n",
    "Note that we did not perform cross validation on our system. The primary reason for this is due to cross validation not making sense in our case. We have created a corpus and are trying to compute the accuracy of our corpus using a test set. As we are not directly training a model, it does not make sense to perform cross validation only over our test set.\n",
    "\n",
    "## Additionl Dataset Note\n",
    "[Note] One additional area of improvement that we should also test is improving the size of our test set. I found another dataset that will work for this from the paper [SC-Ques: A Sentence Completion Question Dataset for English as a Second Language Learners](https://arxiv.org/abs/2206.12036). They provided a [link to their code and their data](https://github.com/ai4ed/SC-Ques) for research purposes. This, in turn, gave a link to a [dropbox containing their data](https://www.dropbox.com/s/lzznin2hxt6rmft/SC-Ques.tar.gz?dl=0). The data that we would be looking to use to expand our training set would be found in test.jsons and train.jsons. These two files have been preprocessed in the SC-Ques-Preprocessing.ipynb notebook and saved to the individual processed_data_idx.csv files. They were stored as multiple files due to space limitations with GitHub repositories.\n",
    "\n",
    "The below section is used to read in and join each of the processed_data_idx.csv files. There should be no need to run the SC-Ques-Preprocessing.ipynb script and is only included for posterity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>ans</th>\n",
       "      <th>blanks</th>\n",
       "      <th>a)</th>\n",
       "      <th>b)</th>\n",
       "      <th>c)</th>\n",
       "      <th>d)</th>\n",
       "      <th>e)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The plane is scheduled to arrive _____ because...</td>\n",
       "      <td>c</td>\n",
       "      <td>1</td>\n",
       "      <td>The plane is scheduled to arrive latest becaus...</td>\n",
       "      <td>The plane is scheduled to arrive later because...</td>\n",
       "      <td>The plane is scheduled to arrive late because ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>_____ he was preparing food for tomorrow's pa...</td>\n",
       "      <td>b</td>\n",
       "      <td>1</td>\n",
       "      <td>Because he was preparing food for tomorrow's p...</td>\n",
       "      <td>While he was preparing food for tomorrow's par...</td>\n",
       "      <td>If he was preparing food for tomorrow's party,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I don't like the people _____ may get angry ea...</td>\n",
       "      <td>d</td>\n",
       "      <td>1</td>\n",
       "      <td>I don't like the people who may get angry easily.</td>\n",
       "      <td>I don't like the people that may get angry eas...</td>\n",
       "      <td>I don't like the people which may get angry ea...</td>\n",
       "      <td>I don't like the people both may get angry eas...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stop making so much noise. It is _____ to the ...</td>\n",
       "      <td>c</td>\n",
       "      <td>1</td>\n",
       "      <td>Stop making so much noise. It is comfortable t...</td>\n",
       "      <td>Stop making so much noise. It is relaxed to th...</td>\n",
       "      <td>Stop making so much noise. It is harmful to th...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Charles Dickens _____ a lot of novels.</td>\n",
       "      <td>b</td>\n",
       "      <td>1</td>\n",
       "      <td>Charles Dickens write a lot of novels.</td>\n",
       "      <td>Charles Dickens wrote a lot of novels.</td>\n",
       "      <td>Charles Dickens writes a lot of novels.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14452</th>\n",
       "      <td>If you keep _____ this, your English handwriti...</td>\n",
       "      <td>b</td>\n",
       "      <td>1</td>\n",
       "      <td>If you keep do this, your English handwriting ...</td>\n",
       "      <td>If you keep doing this, your English handwriti...</td>\n",
       "      <td>If you keep to do this, your English handwriti...</td>\n",
       "      <td>If you keep does this, your English handwritin...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14453</th>\n",
       "      <td>Two days later, the fighting between the two c...</td>\n",
       "      <td>c</td>\n",
       "      <td>1</td>\n",
       "      <td>Two days later, the fighting between the two c...</td>\n",
       "      <td>Two days later, the fighting between the two c...</td>\n",
       "      <td>Two days later, the fighting between the two c...</td>\n",
       "      <td>Two days later, the fighting between the two c...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14454</th>\n",
       "      <td>What he had said about the incident and done w...</td>\n",
       "      <td>b</td>\n",
       "      <td>1</td>\n",
       "      <td>What he had said about the incident and done w...</td>\n",
       "      <td>What he had said about the incident and done w...</td>\n",
       "      <td>What he had said about the incident and done w...</td>\n",
       "      <td>What he had said about the incident and done w...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14455</th>\n",
       "      <td>Get up quickly, Lisa! You have _____ time to h...</td>\n",
       "      <td>c</td>\n",
       "      <td>1</td>\n",
       "      <td>Get up quickly, Lisa! You have a few time to h...</td>\n",
       "      <td>Get up quickly, Lisa! You have many time to ha...</td>\n",
       "      <td>Get up quickly, Lisa! You have no time to have...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14456</th>\n",
       "      <td>I'm sure more waste _____ because we're going ...</td>\n",
       "      <td>d</td>\n",
       "      <td>1</td>\n",
       "      <td>I'm sure more waste recycles because we're goi...</td>\n",
       "      <td>I'm sure more waste is recycling because we're...</td>\n",
       "      <td>I'm sure more waste is recycled because we're ...</td>\n",
       "      <td>I'm sure more waste will be recycled because w...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>289148 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                question ans  blanks  \\\n",
       "0      The plane is scheduled to arrive _____ because...   c       1   \n",
       "1       _____ he was preparing food for tomorrow's pa...   b       1   \n",
       "2      I don't like the people _____ may get angry ea...   d       1   \n",
       "3      Stop making so much noise. It is _____ to the ...   c       1   \n",
       "4                Charles Dickens _____ a lot of novels.    b       1   \n",
       "...                                                  ...  ..     ...   \n",
       "14452  If you keep _____ this, your English handwriti...   b       1   \n",
       "14453  Two days later, the fighting between the two c...   c       1   \n",
       "14454  What he had said about the incident and done w...   b       1   \n",
       "14455  Get up quickly, Lisa! You have _____ time to h...   c       1   \n",
       "14456  I'm sure more waste _____ because we're going ...   d       1   \n",
       "\n",
       "                                                      a)  \\\n",
       "0      The plane is scheduled to arrive latest becaus...   \n",
       "1      Because he was preparing food for tomorrow's p...   \n",
       "2      I don't like the people who may get angry easily.   \n",
       "3      Stop making so much noise. It is comfortable t...   \n",
       "4                 Charles Dickens write a lot of novels.   \n",
       "...                                                  ...   \n",
       "14452  If you keep do this, your English handwriting ...   \n",
       "14453  Two days later, the fighting between the two c...   \n",
       "14454  What he had said about the incident and done w...   \n",
       "14455  Get up quickly, Lisa! You have a few time to h...   \n",
       "14456  I'm sure more waste recycles because we're goi...   \n",
       "\n",
       "                                                      b)  \\\n",
       "0      The plane is scheduled to arrive later because...   \n",
       "1      While he was preparing food for tomorrow's par...   \n",
       "2      I don't like the people that may get angry eas...   \n",
       "3      Stop making so much noise. It is relaxed to th...   \n",
       "4                 Charles Dickens wrote a lot of novels.   \n",
       "...                                                  ...   \n",
       "14452  If you keep doing this, your English handwriti...   \n",
       "14453  Two days later, the fighting between the two c...   \n",
       "14454  What he had said about the incident and done w...   \n",
       "14455  Get up quickly, Lisa! You have many time to ha...   \n",
       "14456  I'm sure more waste is recycling because we're...   \n",
       "\n",
       "                                                      c)  \\\n",
       "0      The plane is scheduled to arrive late because ...   \n",
       "1      If he was preparing food for tomorrow's party,...   \n",
       "2      I don't like the people which may get angry ea...   \n",
       "3      Stop making so much noise. It is harmful to th...   \n",
       "4                Charles Dickens writes a lot of novels.   \n",
       "...                                                  ...   \n",
       "14452  If you keep to do this, your English handwriti...   \n",
       "14453  Two days later, the fighting between the two c...   \n",
       "14454  What he had said about the incident and done w...   \n",
       "14455  Get up quickly, Lisa! You have no time to have...   \n",
       "14456  I'm sure more waste is recycled because we're ...   \n",
       "\n",
       "                                                      d)  e)  \n",
       "0                                                    NaN NaN  \n",
       "1                                                    NaN NaN  \n",
       "2      I don't like the people both may get angry eas... NaN  \n",
       "3                                                    NaN NaN  \n",
       "4                                                    NaN NaN  \n",
       "...                                                  ...  ..  \n",
       "14452  If you keep does this, your English handwritin... NaN  \n",
       "14453  Two days later, the fighting between the two c... NaN  \n",
       "14454  What he had said about the incident and done w... NaN  \n",
       "14455                                                NaN NaN  \n",
       "14456  I'm sure more waste will be recycled because w... NaN  \n",
       "\n",
       "[289148 rows x 8 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "directory_path = Path(\"Datasets/SC-Ques\")\n",
    "sc_ques = pd.DataFrame()\n",
    "for data_path in directory_path.glob(\"**/processed_data_*.csv\"):\n",
    "    # print(data_path)\n",
    "    data = pd.read_csv(data_path)\n",
    "    sc_ques = pd.concat([sc_ques, data])\n",
    "sc_ques = sc_ques.drop(columns=['Unnamed: 0'])\n",
    "sc_ques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "—Do you often do some _____ in the morning? —Yes, I do morning _____ . \n",
      "—Do you often do some exercise in the morning? —Yes, I do morning exercise .\n",
      "—Do you often do some exercises in the morning? —Yes, I do morning exercises .\n",
      "—Do you often do some exercise in the morning? —Yes, I do morning exercises .\n",
      "—Do you often do some exercises in the morning? —Yes, I do morning exercise .\n"
     ]
    }
   ],
   "source": [
    "test = sc_ques.iloc[89546]\n",
    "print(test['question'])\n",
    "print(test['a)'])\n",
    "print(test['b)'])\n",
    "print(test['c)'])\n",
    "print(test['d)'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "977f16dd356d4ed08c02be2f32e7a0e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/289148 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy_score': 0.4089116991990261,\n",
       " 'precision_score': 0.4192645955929776,\n",
       " 'recall_score': 0.4089116991990261,\n",
       " 'f1_score': 0.4079403841784968}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# of note is that there is next to 0 f1-score difference for n=4 compared to n=3\n",
    "predictions = sc_ques.progress_apply(lambda row: predict_2(row, lm), axis=1)\n",
    "true_values = sc_ques['ans']\n",
    "sc_ques_metrics = record_model_metrics_as_json(true_values, predictions)\n",
    "sc_ques_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conf = confusion_matrix(true_values, predictions)\n",
    "# fig, ax = plt.subplots(figsize=(8,6), dpi=100)\n",
    "# display = ConfusionMatrixDisplay(conf, display_labels=['a', 'b', 'c', 'd'])\n",
    "# ax.set(title='Confusion Matrix for SAT sentence completion')\n",
    "# display.plot(ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store the most recent run of training metrics as a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved metrics.pkl\n"
     ]
    }
   ],
   "source": [
    "metrics = {\"sat\": sat_metrics, \"sc_ques\": sc_ques_metrics}\n",
    "to_pickle(filename=\"metrics.pkl\", model=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO:\n",
    "- write up the readme with dependencies\n",
    "- write up a how to run\n",
    "- write up how to use.\n",
    "- pickle the model that we need. This notebook is only for training the model basically\n",
    "- Need to write up a python script that allows us to load the model (from the git repo so nobody needs to download it) and will suppport sentence completion as well as sentence writing.\n",
    "- do not want to need to download the csv's either.\n",
    "- Needs to be by Wednesday.\n",
    "- Play with removing punctuation during cleaning.\n",
    "- number to word conversion (look at \"ozone contains _____ oxygen atoms\")\n",
    "\n",
    "## Final Presentation\n",
    "- aimed towards our peers\n",
    "- need functioning copy of final project\n",
    "- what it does and how we did it\n",
    "- how we chose data\n",
    "- what we did to the data\n",
    "- how we trained the model\n",
    "- give or take 10 min long including demonstration\n",
    "- 12 groups.\n",
    "- can switch to giving presentations during the last week of class.\n",
    "\n",
    "## Sentence Generation\n",
    "The below section takes a look at both sentence generation, as well as the optimization of some existing processes. Much of the work in this section is originally based on [a stack overflow post](https://stackoverflow.com/a/54979617) and [the Maximum Likelihood Estimator (MLE) documentation from NLTK](https://www.nltk.org/api/nltk.lm.html). The work in this section could be performed as one command if we were to use the padded_everygram_pipeline method. Unfortunatly, this method presents certain downsides when the n-gram size is larger than 2 (bigrams). This is because the sentences will be padded with more than one start of sentence or end of sentence tag. This leads to the model predicting the end of sentence tag following another end of sentence tag, thus causing an infinite loop.\n",
    "\n",
    "Instead, we will perform these steps in parts. The first part would be to get a flattened list of tokens comprised of each padded sentence in our corpus. This can be done using the flatten and pad_both_ends commands. As mentioned earlier, even if the n-gram size is larger than two, a value of 2 should be passed in to pad_both_ends. Following this, we can calculate our n-grams using everygrams, or ngrams.\n",
    "\n",
    "As per [this stackoverflow post](https://stackoverflow.com/a/60661188) the padded_everygram_pipeline creates the following iterators:\n",
    "- sentences padded and turned into sequences of `nltk.util.everygrams`\n",
    "- sentences padded as above and chained together for a flat stream of words\n",
    "\n",
    "This information should help with passing arguments to the fit method of the MLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<NgramCounter with 3 ngram orders and 5191901601 ngrams>\n",
      "27144525\n",
      "6221658\n",
      "0.014816437655688817\n",
      "0.06134493956885645\n",
      "-6.076657570353462\n",
      "-4.026911850939978\n"
     ]
    }
   ],
   "source": [
    "# print(lm.vocab.lookup(sample_lecture_sentences[0]))\n",
    "print(lm.counts)\n",
    "print(lm.counts['a'])\n",
    "print(lm.counts[['<s>']]['The'])\n",
    "print(lm.score(\"a\"))\n",
    "print(lm.score(\"The\", [\"<s>\"]))\n",
    "print(lm.logscore(\"a\"))\n",
    "print(lm.logscore(\"The\", [\"<s>\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Agenda The _demos_ must have at Paris for a few years ago had finally located a pointer to one side of a sermon in a bar stool and spoke to'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(lm.generate(30, random_seed=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Christian rulers ought to be alone : Beneath the glaring light that already , mostly eejit men . </s> Screw you , to kiss a boy her mother 's satin\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(lm.generate(30, text_seed=['<s>']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting output sequence: ['<s>']\n",
      "satisfaction for (\"      Will\" | ['<s>']) =: 0.00\n",
      "satisfaction for (\"      take\" | ['<s>', 'Will']) =: 0.00\n",
      "satisfaction for (\"       you\" | ['Will', 'take']) =: 0.08\n",
      "satisfaction for (\"      away\" | ['take', 'you']) =: 0.03\n",
      "satisfaction for (\"      from\" | ['you', 'away']) =: 0.38\n",
      "satisfaction for (\"       the\" | ['away', 'from']) =: 0.33\n",
      "satisfaction for (\"  casement\" | ['from', 'the']) =: 0.00\n",
      "satisfaction for (\"      open\" | ['the', 'casement']) =: 0.01\n",
      "satisfaction for (\"         ;\" | ['casement', 'open']) =: 0.17\n",
      "satisfaction for (\"        an\" | ['open', ';']) =: 0.01\n",
      "satisfaction for (\"         <\" | [';', 'an']) =: 0.00\n",
      "satisfaction for (\"         i\" | ['an', '<']) =: 0.51\n",
      "satisfaction for (\"         >\" | ['<', 'i']) =: 1.00\n",
      "satisfaction for (\"        a.\" | ['i', '>']) =: 0.06\n",
      "satisfaction for (\"         <\" | ['>', 'a.']) =: 1.00\n",
      "satisfaction for (\"        /i\" | ['a.', '<']) =: 1.00\n",
      "satisfaction for (\"         >\" | ['<', '/i']) =: 1.00\n",
      "satisfaction for (\"         <\" | ['/i', '>']) =: 0.52\n",
      "satisfaction for (\"       /sn\" | ['>', '<']) =: 0.04\n",
      "satisfaction for (\"         >\" | ['<', '/sn']) =: 1.00\n",
      "satisfaction for (\"         <\" | ['/sn', '>']) =: 0.99\n",
      "satisfaction for (\"        /p\" | ['>', '<']) =: 0.14\n",
      "satisfaction for (\"         >\" | ['<', '/p']) =: 1.00\n",
      "satisfaction for (\"         <\" | ['/p', '>']) =: 1.00\n",
      "satisfaction for (\"        hw\" | ['>', '<']) =: 0.09\n",
      "satisfaction for (\"         >\" | ['<', 'hw']) =: 1.00\n",
      "satisfaction for (\"        Ni\" | ['hw', '>']) =: 0.01\n",
      "satisfaction for (\"        ''\" | ['>', 'Ni']) =: 0.45\n",
      "satisfaction for (\"       sey\" | ['Ni', \"''\"]) =: 0.02\n",
      "satisfaction for (\"         <\" | [\"''\", 'sey']) =: 1.00\n",
      "satisfaction for (\"       /hw\" | ['sey', '<']) =: 1.00\n",
      "satisfaction for (\"         >\" | ['<', '/hw']) =: 1.00\n",
      "satisfaction for (\"         (\" | ['/hw', '>']) =: 0.85\n",
      "satisfaction for (\"         ?\" | ['>', '(']) =: 0.59\n",
      "satisfaction for (\"      </s>\" | ['(', '?']) =: 0.94\n",
      "satisfaction for (\"        he\" | ['</s>', '<s>']) =: 0.00\n",
      "output: <s> Will take you away from the casement open ; an < i > a. < /i > < /sn > < /p > < hw > Ni '' sey < /hw > ( ? </s> <s>\n"
     ]
    }
   ],
   "source": [
    "# look at generating text until it is satisfied\n",
    "start_token = '<s>'\n",
    "end_token = '</s>'\n",
    "output = [start_token]\n",
    "print(\"starting output sequence: %s\" % str(output))\n",
    "satisfaction_threshold_log = -15\n",
    "satisfaction_threshold = 0\n",
    "satisfaction = 0\n",
    "count = 0\n",
    "while satisfaction > satisfaction_threshold_log and count < 50:\n",
    "    count += 1\n",
    "    generated = lm.generate(1, text_seed=output) # , random_seed=3\n",
    "    if isinstance(generated, str):\n",
    "        generated = [generated]\n",
    "    # print(\"generated sequence: %s\" % str(generated))\n",
    "    temp_output = output + generated\n",
    "    # print(\"new output sequence: %s\" % str(temp_output))\n",
    "    prior_token = generated[-1]\n",
    "    # print(\"new token: %s\" % prior_token)\n",
    "    # print(\"last token: %s\" % temp_output[-n:-1])\n",
    "    # # Note that Logscore only can evaluate the score for a single word, not a list as per https://www.nltk.org/_modules/nltk/lm/api.html\n",
    "    # # could change this to use P = log_10(count_ngram / count_n-1gram)\n",
    "    satisfaction = lm.score(prior_token, temp_output[-n:-1])\n",
    "    print(\"satisfaction for (\\\"%10s\\\" | %s) =: %.2f\" % (prior_token, \n",
    "                                                  temp_output[-n:-1], \n",
    "                                                  satisfaction))\n",
    "    # if satisfaction > satisfaction_threshold_log:\n",
    "    if satisfaction > satisfaction_threshold:\n",
    "        output += generated\n",
    "        if generated == [end_token]:\n",
    "            output += [start_token]\n",
    "    else:\n",
    "        break\n",
    "    \n",
    "print(\"output: %s\" % ' '.join(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes for final project:\n",
    "- make system work better. Go grab a bunch of data and add it to the model.\n",
    "- Tune the model if you have time, but throwing more data at is is probably less effort per improvement.\n",
    "- improve the output through more data.\n",
    "- Allow input to be more variable and less brittle.\n",
    "- five page writeup containing everything we did and what data was used, etc.\n",
    "- Presentation as a brief version of the paper + demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
