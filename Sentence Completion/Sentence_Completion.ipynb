{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Completion\n",
    "This notebook is used for a final project as a part of the CS 4980 Natural Language Processing course at the Milwaukee School of Engineering during the spring term of 2023. This notebook was created by [Grant Fass](grantfass@gmail.com) and [Nicholas Kaja](kajan@msoe.edu). The following notebook will explore the problem of natural language sentence completion. This notebook can also be reached directly through the following GitHub repository: [https://github.com/GrantFass/NLP/tree/main/Sentence%20Completion](https://github.com/GrantFass/NLP/tree/main/Sentence%20Completion)\n",
    "\n",
    "Natural language sentence completion involves determining what word best fits in a blank present in a sentence. This type of question is typically found on the Scholastic Aptitude Test (SAT). It is useful because it can measure the performance of a language model (LM) on questions that educational experts deem important. LMs that perform well on this type of problem will likely perform better on broader tasks.\n",
    "\n",
    "The primary method for forming a sentence completion model is to compute the probability of each possible sentence tehn choose the most probable option. This probability computation can be done using n-grams, Latent Semantic Analysis (LSA), and Syntactic Dependency Trees. Some research has also been done into combining these with methods of preserving long-range dependencies in the sentences. Of these options, N-grams is one of the easiest starting points due to its ease of implementation and understanding. N-grams also allow for sufficient variations over the base model such as different N-gram algorithms, different values of N, and different methods of tokenization.\n",
    "\n",
    "We will be constructing our models from a dataset of Khan Academy lecture transcripts. This dataset was scraped using BeautifulSoup during January of 2023 by Nicholas Kaja as a part of a senior design project. The performance of our model will be evaluated on a [SAT Question Dataset](https://github.com/ctr4si/sentence-completion/tree/master/data/completion). After evaluation, we plan to take our best performing model and apply it to sentence generation. This will allow us to get a better feel for how well it performs. Once our model is working, we also plan to implement some additional features such as Named Entity Recognition (NER). If there is enough time we also plan to investigate using the [OpenAI tokenizer](https://platform.openai.com/tokenizer) instead of the [NLTK Word Tokenizer](https://www.nltk.org/api/nltk.tokenize.html) or the [SpaCy Tokenizer](https://spacy.io/api/tokenizer). The OpenAI tokenizer has a [python package found on github](https://github.com/openai/tiktoken).\n",
    "\n",
    "For more information please see the Data Collection And Processing document as well as the Project Background document. Both of these documents can be found in the repository under the Sentence Completion directory.\n",
    "\n",
    "## Project Demo:\n",
    "Note that this file is mainly used for training and testing the model we build. It also includes the explanations of processes. To view an interactive demo please visit the website located at [grantfass.org](http://grantfass.org).\n",
    "\n",
    "## Pip Installations\n",
    "The below magic command should install all of the required python packages to run this project. If it does not run successfully please try to run the command through an admin shell instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt --user"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "This cell contains the imports and some setup functions. This is mostly generalized which means there are extra imports as of now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict\n",
    "import pandas as pd\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "import nltk.data\n",
    "import re\n",
    "import contractions\n",
    "from bs4 import BeautifulSoup\n",
    "import unidecode\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score\n",
    "from sklearn.metrics import f1_score, recall_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import math\n",
    "import numpy as np\n",
    "import copy\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import pickle\n",
    "from nltk.lm.preprocessing import flatten\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "from nltk.util import pad_sequence\n",
    "from nltk.util import everygrams\n",
    "from nltk.lm import MLE, KneserNeyInterpolated, Laplace, AbsoluteDiscountingInterpolated\n",
    "from functools import partial\n",
    "\n",
    "nltk.download([\n",
    "\"names\",\n",
    "\"stopwords\",\n",
    "\"state_union\",\n",
    "\"twitter_samples\",\n",
    "\"movie_reviews\",\n",
    "\"averaged_perceptron_tagger\",\n",
    "\"vader_lexicon\",\n",
    "\"punkt\",\n",
    "])\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "encoder = LabelEncoder()\n",
    "sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "tqdm.pandas()\n",
    "n = 3\n",
    "new_dir = \"Datasets/Input\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading  \n",
    "This section is used to load in all of the training data for the vocabulary. This comes from the 5 csv files of Khan Academy lectures. These files are all combined into a single dataframe for easier use and analysis. In the future we may need to include other sources of data such as ted talks or even ebooks in order to widen our vocabulary.\n",
    "\n",
    "We also load in the SAT dataset here. This is a short dataset that we are primarily using as our testing set. The goal here is to guess which response option, out of five, is the correct response for a fill-in-the-blank SAT question. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing_df = pd.read_csv(Path(\"Datasets/KhanAcademy/Computing.csv\"))\n",
    "# computing_df = computing_df.dropna()\n",
    "\n",
    "# economics_df = pd.read_csv(Path(\"Datasets/KhanAcademy/Economics.csv\"))\n",
    "# economics_df = economics_df.dropna()\n",
    "\n",
    "# humanities_df = pd.read_csv(Path(\"Datasets/KhanAcademy/Humanities.csv\"))\n",
    "# humanities_df = humanities_df.dropna()\n",
    "\n",
    "# math_df = pd.read_csv(Path(\"Datasets/KhanAcademy/Math.csv\"))\n",
    "# math_df = math_df.dropna()\n",
    "\n",
    "# science_df = pd.read_csv(Path(\"Datasets/KhanAcademy/Science.csv\"))\n",
    "# science_df = science_df.dropna()\n",
    "\n",
    "# khan_dfs = [computing_df, economics_df, humanities_df, math_df, science_df]\n",
    "# khan = pd.concat(khan_dfs, axis=0)\n",
    "# khan.info(verbose=True, show_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sat = pd.read_csv(Path(\"Datasets/SAT_Question_Dataset.csv\"))\n",
    "sat.info(verbose=True, show_counts=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning\n",
    "This section is dedicated to cleaning up some of our vocabulary dataset before it is used to create the vocabulary. We have decided, for simplicity, to remove the first sentence of every transcript in the dataset. This will allow us to easily remove all of the tagging information such as \"[Instructor]\" and \"- Speaker 1:\". Most of the first sentences are greetings and introductions. As such they are not necessarily as important to answering the SAT question dataset. We have also elected to expand contractions, remove html tags, remove quotes, and replace accented characters.\n",
    "\n",
    "The same general cleaning method is run over our testing data as well. The first sentence removal is not performed on the testing data.\n",
    "\n",
    "[Note] Before this section, as a part of the cleaning, we will want to perform SpaCy NER. Use SpaCy NER to identify the multi-word-expressions then use the [nltk.tokenize.mwe module](https://www.nltk.org/api/nltk.tokenize.mwe.html) to combine the tokens - Grant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_base(x: str):\n",
    "        # remove any html tags\n",
    "        x = BeautifulSoup(x, \"html.parser\").get_text(separator=\" \")\n",
    "        # # set all to lower\n",
    "        # x = x.lower()\n",
    "        # clean up the contractions\n",
    "        x = contractions.fix(x)\n",
    "        # remove accended characters\n",
    "        x = unidecode.unidecode(x)\n",
    "        # # remove stopwords: https://stackoverflow.com/questions/19560498/faster-way-to-remove-stop-words-in-python\n",
    "        # x = ' '.join([word for word in x.split() if word not in cachedStopWords]) # slower to use word tokenize\n",
    "        # # fix punctuation spacing\n",
    "        # x = re.sub(r'(?<=[\\.\\,\\?])(?=[^\\s])', r' ', x)\n",
    "        # # strip punctuation\n",
    "        # x = re.sub(r'[\\.\\,\\?\\\\\\/\\<\\>\\;\\:\\[\\]\\{\\}]', r'', x)\n",
    "        # strip quotes\n",
    "        # x = x.replace('\\'', '').replace('\\\"', '')\n",
    "        x = x.replace('\\\"', '')\n",
    "        # # remove some actions\n",
    "        # remove_list = ['(Laughter)', '(laughter)', '(Music)', '(music)', '(Music ends)', '(Audience cheers)', '(Applause)', '(Applause ends)', '(Applause continues)', '(Bells)', '(Trumpet)', '(Clears throat)']\n",
    "        # x = ' '.join([word for word in x.split() if word not in remove_list])\n",
    "        # remove extraneous items\n",
    "        x = x.replace(' -- ', '').replace(' .. ', ' ').replace(' ... ', ' ')\n",
    "        # remove extra whitespace\n",
    "        x = ' '.join(x.strip().split())\n",
    "        # # may want to add lematization\n",
    "        # x = ' '.join([lemmatizer.lemmatize(word) for word in x.split()])\n",
    "        # remove some of the extra bracket tags\n",
    "        x = re.sub(r\"\\s{2,}\", \" \", re.sub(r\"[\\(\\[\\{][^\\)\\]\\}]*[\\)\\]\\}]\", \"\", x))\n",
    "        return x\n",
    "\n",
    "def remove_first_sentence(doc):\n",
    "    \"\"\"\n",
    "    This removes the first sentence of a document. We use this to remove all narrator / speaker tags, and\n",
    "    to remove unnecessary introductory sentences that most transcripts have\n",
    "    \"\"\"\n",
    "    return ' '.join(nltk.sent_tokenize(doc)[1:])\n",
    "\n",
    "# transcripts = khan['transcript']\n",
    "# # transcripts = transcripts.progress_apply(remove_first_sentence)\n",
    "# transcripts = transcripts.progress_apply(clean_base)\n",
    "# khan['clean_transcript'] = transcripts\n",
    "# khan.head()\n",
    "\n",
    "directory_path = Path(new_dir)\n",
    "documents = []\n",
    "for data_path in tqdm(list(directory_path.glob(\"**/*.txt\"))):\n",
    "    text = \"\"\n",
    "    with open(Path(data_path), \"r\", encoding=\"utf-8\") as f:\n",
    "        text = clean_base(f.read())\n",
    "        documents.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column_name in ['question', 'a)', 'b)', 'c)', 'd)', 'e)']:\n",
    "    sat[column_name] = sat[column_name].progress_apply(clean_base)\n",
    "sat.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-Grams\n",
    "This section is used for the tokenization of our text into N-Grams. Various tokenization approaches may yield different results. As such, it is important to test different approaches here. We begin by dividing each of the transcripts into its constituent sentences. These sentences are then each tokenized using the word tokenizer provided by nltk. A start and end of sentence tag is then added to each list of tokens. Next, these tokens are used to generate all possible n-grams below a certain size threshold. Lastly, we create a frequency distribution dictionary for the n-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_document(document):\n",
    "    return [word_tokenize(t) for t in sent_detector.tokenize(document)]\n",
    "\n",
    "def tokenize_documents(documents):\n",
    "    sents = []\n",
    "    for doc in tqdm(documents):\n",
    "        for sent in tokenize_document(doc):\n",
    "            sents.append(sent)\n",
    "    return sents\n",
    "\n",
    "def pad_sentence(tokens):\n",
    "    return pad_both_ends(tokens, n=2)\n",
    "\n",
    "def pad_sentences(sents):\n",
    "    return [pad_sentence(sent) for sent in sents]\n",
    "\n",
    "def custom_padded_everygam_pipeline(n, tokenized_sents):\n",
    "    # these few lines come directly from the padded_everygram_pipeline source code\n",
    "    padding_fn = partial(pad_both_ends, n=2)\n",
    "    return (\n",
    "        (everygrams(list(padding_fn(sent)), max_len=n) for sent in tokenized_sents),\n",
    "        flatten(map(padding_fn, tokenized_sents)),\n",
    "    )\n",
    "\n",
    "print(\"Tokenizing Documents...\")\n",
    "# sents = tokenize_documents(khan['clean_transcript'])\n",
    "sents = tokenize_documents(documents)\n",
    "print(\"Building Custom Everygram Pipeline...\")\n",
    "train, vocab = custom_padded_everygam_pipeline(n, sents)\n",
    "lm = MLE(n)\n",
    "# lm = Laplace(n)\n",
    "print(\"Fitting Language Model...\")\n",
    "lm.fit(train, vocab)\n",
    "print(lm.vocab)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store Model\n",
    "The next step is to store the freq_dist model for use in future prediction methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_pickle(filename, model):\n",
    "    with open(filename, 'wb') as fp:\n",
    "        pickle.dump(model, fp)\n",
    "        print(\"Saved %s\" % (filename))\n",
    "        \n",
    "def from_pickle(filename):\n",
    "    model = []\n",
    "    with open(filename, 'rb') as fp:\n",
    "        model = pickle.load(fp)\n",
    "    return model\n",
    "\n",
    "        \n",
    "to_pickle(filename='lm.pkl', model=lm)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAT Dataset\n",
    "This section focuses on the processing of individual SAT questions as well as defining a prediction method for the dataset. Below is a sample approach for the prediction method.\n",
    "\n",
    "### SAT Question Prediction Approach\n",
    "Note that this approach is slightly different from what was implemented. One of the main differences is that our tokenization currently includes punctuation as its own token whereas the below example removes punctuation.\n",
    "\n",
    "\"\"\"  \n",
    "Question:  \n",
    "Responding to criticism that the script was rambling and _____, the new screenwriter revised the dialogue for greater succinctness and _____.\n",
    "\n",
    "Possible Solutions:  \n",
    "[('engaging', 'simplicity'), ('subjective', 'abiguity'), ('muddled', 'clarity'), ('terse', 'emptiness'), ('difficult', 'abstraction')]  \n",
    "\"\"\"\n",
    "\n",
    "N-gram size of 4 max\n",
    "1. Change the blanks out to something we can use to mask the individual ones later. For example, [BLANK1] and [BLANK2].\n",
    "    - esponding to criticism that the script was rambling and [BLANK1], the new screenwriter revised the dialogue for greater succinctness and [BLANK2].<\\sent>\n",
    "2. Pull out the possible sliding windows centered around each blank.\n",
    "    - blank 1\n",
    "        - n=4\n",
    "            - ('was', 'rambling', 'and', [BLANK1])\n",
    "            - ('rambling', 'and', [BLANK1], 'the')\n",
    "            - ('and', [BLANK1], 'the', 'new')\n",
    "            - ([BLANK1], 'the', 'new', 'screenwriter')\n",
    "        - n=3\n",
    "            - ('rambling', 'and', [BLANK1])\n",
    "            - ('and', [BLANK1], 'the')\n",
    "            - ([BLANK1], 'the', 'new')\n",
    "        - n=2\n",
    "            - ('and', [BLANK1])\n",
    "            - ([BLANK1], 'the')\n",
    "        - n=1\n",
    "            - ([BLANK1])\n",
    "    - blank 2\n",
    "        - n=4\n",
    "            - ('greater', 'succinctness', 'and', [BLANK2])\n",
    "            - ('succinctness', 'and', [BLANK2], <\\sent>)\n",
    "        - n=3\n",
    "            - ('succinctness', 'and', [BLANK2])\n",
    "            - ('and', [BLANK2], <\\sent>)\n",
    "        - n=2\n",
    "            - ('and', [BLANK2])\n",
    "            - ([BLANK2], <\\sent>)\n",
    "        - n=1\n",
    "            - ([BLANK2])\n",
    "3. For each of the possible solutions:\n",
    "    1. For each of the windows:\n",
    "        1. If the first or second word in the possible solution tuple is not in the vocab then discard it as a possible solution.\n",
    "        2. Replace [BLANK1] with the first item in the possible solution tuple\n",
    "        3. Replace [BLANK2] with the second item in the possible solution tuple\n",
    "        4. Lookup each of the window n-grams against the stored n-grams and determine how many times it occured.\n",
    "        5. What to do if the count for a n-gram is 0 for one word but not for another word?\n",
    "        6. Calculate the probability of each window n-gram\n",
    "        7. sum the logs of the probabilities of each window n-gram. \n",
    "    2. Determine which of the windows had the highest log likelihood and return the associated word tuple.\n",
    "\n",
    "### Scoring Experimentation:\n",
    "This section explains some of the experimentation with the various methods of scoring which prediction was the most accurate in the predict method. The baseline prediction worked by taking the log in base 10 of (the count of the ngram divided by the count of the one smaller ngram). The code for this is shown below. All scores reported in this section are f1-scores. The two datasets that were evaluated were the SAT question dataset (SAT) and the SC-Ques dataset (ESL). The basline performance was 0.155 for SAT and 0.209 for ESL. Out of the possible models it appears that the best prediction accuracy was attained when using lm.score with the Maximum Likelihood Estimator. The most optimal size for n is likely 3 or 4. Quadgrams have a slightly better score, but take longer to train. For testing purposes bigrams may be the best option.\n",
    "\n",
    "```\n",
    "ngram_size = len(new_window)\n",
    "raw_count = freq_dist[tuple(new_window)] # TODO: what if we try to use an ensemble of models with different maximum n-gram sizes?\n",
    "if raw_count > 0:\n",
    "    prior_count = 1\n",
    "    if ngram_size > 1:\n",
    "        prior = new_window[0: len(new_window) - 1]\n",
    "        prior_count = freq_dist[tuple(prior)]\n",
    "    raw_prob = raw_count / prior_count\n",
    "    log_prob = math.log10(raw_prob)\n",
    "    print(log_prob)\n",
    "    log_likelihood += log_prob\n",
    "```\n",
    "\n",
    "#### Bigrams:\n",
    "- MLE + lm.score\n",
    "    - SAT: 0.231\n",
    "    - ESL: 0.312\n",
    "- MLE + lm.logscore\n",
    "    - SAT: 0.122\n",
    "    - ESL: \n",
    "- Laplace + lm.score\n",
    "    - SAT: 0.185\n",
    "    - ESL: 0.268\n",
    "- Laplace + lm.logscore\n",
    "    - SAT: 0.191\n",
    "    - ESL: 0.288\n",
    "- KneserNeyInterpolated\n",
    "    - Of note is that this model is incredibly impractical. It is so slow that it feels like the computer froze most of the time. It also only got a 0.158 on lm.logscore for the SAT dataset.\n",
    "- AbsoluteDiscountingInterpolated\n",
    "    - Similar speed, including training times, to the MLE. Got a 0.184 for SAT on lm.logscore\n",
    "\n",
    "#### Trigrams:\n",
    "- MLE + lm.score\n",
    "    - SAT: 0.231\n",
    "    - ESL: 0.330\n",
    "    \n",
    "#### Quadgrams:\n",
    "- MLE + lm.score\n",
    "    - SAT: 238\n",
    "    - ESL: 335\n",
    "- MLE + lm.logscore\n",
    "    - SAT: 0.07\n",
    "    - ESL: \n",
    "- Laplace + lm.score\n",
    "    - SAT: 0.191\n",
    "    - ESL: 0.271\n",
    "- Laplace + lm.logscore\n",
    "    - SAT: 0.203\n",
    "    - ESL: 0.305\n",
    "\n",
    "#### 7-grams:\n",
    "- MLE + lm.score\n",
    "    - SAT: 0.238\n",
    "    - ESL: 0.336"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_2(row, lm,  mask=\"_____\"):\n",
    "    # tokenize the inbound question\n",
    "    tokens = word_tokenize(row['question'])\n",
    "    question = list(pad_both_ends(tokens, n=2))\n",
    "    # determine where the mask is without knowing how many masks there are.\n",
    "    # note that .index is O(n) so we may as well iterate through ourselves to be more verbose.\n",
    "    mask_indices = []\n",
    "    mask_count = 0\n",
    "    for idx in range(len(question)):\n",
    "        if question[idx] == mask:\n",
    "            # store the index of the mask\n",
    "            mask_indices.append(idx)\n",
    "            # convert the mask to one with a number for future reference\n",
    "            question[idx] = mask + str(mask_count)\n",
    "            mask_count += 1\n",
    "    # pull out the windows\n",
    "    grams = list(everygrams(question, min_len=1, max_len=n))\n",
    "    windows = []\n",
    "    for i in range(len(mask_indices)):\n",
    "        remask = mask + str(i)\n",
    "        for gram in grams:\n",
    "            if remask in gram:\n",
    "                windows.append(gram)\n",
    "    # extract the possible solutions\n",
    "    if mask_indices:\n",
    "        column_names = ['a)', 'b)', 'c)', 'd)', 'e)']\n",
    "        solution_likelihoods = []\n",
    "        for name in column_names:\n",
    "            if name in row and isinstance(row[name], str) and row[name] != \"\":\n",
    "                # tokenize the input\n",
    "                tokens = word_tokenize(row[name])\n",
    "                tokens = list(pad_both_ends(tokens, n=2))\n",
    "                # calculate the probability\n",
    "                log_likelihood = 0\n",
    "                custom_windows = []\n",
    "                for window in windows:\n",
    "                    # # fill in the blanks in the windows with the possible solutions.\n",
    "                    new_window = window\n",
    "                    for i in range(len(mask_indices)):\n",
    "                        remask = mask + str(i)\n",
    "                        new_window = list(map(lambda x: x.replace(remask, tokens[mask_indices[i]]), new_window))\n",
    "                    custom_windows.append(new_window)\n",
    "                    log_likelihood += lm.score(new_window[-1], new_window[0:-1])\n",
    "                solution_likelihoods.append((name, log_likelihood))\n",
    "        solution_likelihoods.sort(key = lambda x: x[1], reverse = True)\n",
    "        ans = solution_likelihoods[0][0]\n",
    "        return ans.replace(\")\", \"\")\n",
    "    \n",
    "    \n",
    "def score(y_true, y_pred):\n",
    "    print(\"%-15s: %.3f\" % (\"[ACCURACY]\", accuracy_score(y_true, y_pred)))\n",
    "    print(\"%-15s: %.3f\" % (\"[PRECISION]\", precision_score(y_true, y_pred, average='weighted')))\n",
    "    print(\"%-15s: %.3f\" % (\"[RECALL]\", recall_score(y_true, y_pred, average='weighted')))\n",
    "    print(\"%-15s: %.3f\" % (\"[F1-SCORE]\", f1_score(y_true, y_pred, average='weighted')))    \n",
    "    \n",
    "def record_model_metrics_as_json(y_true, y_pred):\n",
    "    metrics = {}\n",
    "    metrics['accuracy_score'] = accuracy_score(y_true, y_pred)\n",
    "    metrics['precision_score'] = precision_score(y_true, y_pred, average='weighted')\n",
    "    metrics['recall_score'] = recall_score(y_true, y_pred, average='weighted')\n",
    "    metrics['f1_score'] = f1_score(y_true, y_pred, average='weighted')\n",
    "    return metrics    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = sat.iloc[1]\n",
    "predict_2(test, lm)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions\n",
    "This section is used to actually compute the predictions for our test data and our given input frequency distribution. The results of the prediction are then evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = sat.progress_apply(lambda row: predict_2(row, lm), axis=1)\n",
    "true_values = sat['ans']\n",
    "sat_metrics = record_model_metrics_as_json(true_values, predictions)\n",
    "sat_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conf = confusion_matrix(true_values, predictions)\n",
    "# fig, ax = plt.subplots(figsize=(8,6), dpi=100)\n",
    "# display = ConfusionMatrixDisplay(conf, display_labels=['a', 'b', 'c', 'd', 'e'])\n",
    "# ax.set(title='Confusion Matrix for SAT sentence completion')\n",
    "# display.plot(ax=ax)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of Results\n",
    "Our model does not perform that well currently. We hypothesize that this is due to both the complexity of the words in the SAT question dataset as well as the size of our input dataset. Our input dataset is only comprised of around 600,000 sentences. Additionally, our input dataset comes from spoken lecture transcripts. While these transcripts should be on fairly complex topics, they still contain mostly spoken language. The SAT question dataset prompts are more complex than the language spoken in our input dataset. This leads to a mismatch between our corpus and training data where most of our n-grams either have low counts or do not exist in our corpus. This could be remedied by expanding the size of our input dataset and thusly our corpus. The research paper that our test dataset came from compiled their corpus on over 1.1 billion words in newspaper articles. Even with this large increase in corpus size they still only achieved a prediction accuracy of just over 50% on the testing dataset. This implies that the test dataset is fairly difficult to get high accuracy scores on.\n",
    "\n",
    "Note that we did not perform cross validation on our system. The primary reason for this is due to cross validation not making sense in our case. We have created a corpus and are trying to compute the accuracy of our corpus using a test set. As we are not directly training a model, it does not make sense to perform cross validation only over our test set.\n",
    "\n",
    "## Additionl Dataset Note\n",
    "[Note] One additional area of improvement that we should also test is improving the size of our test set. I found another dataset that will work for this from the paper [SC-Ques: A Sentence Completion Question Dataset for English as a Second Language Learners](https://arxiv.org/abs/2206.12036). They provided a [link to their code and their data](https://github.com/ai4ed/SC-Ques) for research purposes. This, in turn, gave a link to a [dropbox containing their data](https://www.dropbox.com/s/lzznin2hxt6rmft/SC-Ques.tar.gz?dl=0). The data that we would be looking to use to expand our training set would be found in test.jsons and train.jsons. These two files have been preprocessed in the SC-Ques-Preprocessing.ipynb notebook and saved to the individual processed_data_idx.csv files. They were stored as multiple files due to space limitations with GitHub repositories.\n",
    "\n",
    "The below section is used to read in and join each of the processed_data_idx.csv files. There should be no need to run the SC-Ques-Preprocessing.ipynb script and is only included for posterity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = Path(\"Datasets/SC-Ques\")\n",
    "sc_ques = pd.DataFrame()\n",
    "for data_path in directory_path.glob(\"**/processed_data_*.csv\"):\n",
    "    # print(data_path)\n",
    "    data = pd.read_csv(data_path)\n",
    "    sc_ques = pd.concat([sc_ques, data])\n",
    "sc_ques = sc_ques.drop(columns=['Unnamed: 0'])\n",
    "sc_ques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = sc_ques.iloc[89546]\n",
    "print(test['question'])\n",
    "print(test['a)'])\n",
    "print(test['b)'])\n",
    "print(test['c)'])\n",
    "print(test['d)'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# of note is that there is next to 0 f1-score difference for n=4 compared to n=3\n",
    "predictions = sc_ques.progress_apply(lambda row: predict_2(row, lm), axis=1)\n",
    "true_values = sc_ques['ans']\n",
    "sc_ques_metrics = record_model_metrics_as_json(true_values, predictions)\n",
    "sc_ques_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conf = confusion_matrix(true_values, predictions)\n",
    "# fig, ax = plt.subplots(figsize=(8,6), dpi=100)\n",
    "# display = ConfusionMatrixDisplay(conf, display_labels=['a', 'b', 'c', 'd'])\n",
    "# ax.set(title='Confusion Matrix for SAT sentence completion')\n",
    "# display.plot(ax=ax)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store the most recent run of training metrics as a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {\"sat\": sat_metrics, \"sc_ques\": sc_ques_metrics}\n",
    "to_pickle(filename=\"metrics.pkl\", model=metrics)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO:\n",
    "- write up the readme with dependencies\n",
    "- write up a how to run\n",
    "- write up how to use.\n",
    "- pickle the model that we need. This notebook is only for training the model basically\n",
    "- Need to write up a python script that allows us to load the model (from the git repo so nobody needs to download it) and will suppport sentence completion as well as sentence writing.\n",
    "- do not want to need to download the csv's either.\n",
    "- Needs to be by Wednesday.\n",
    "- Play with removing punctuation during cleaning.\n",
    "- number to word conversion (look at \"ozone contains _____ oxygen atoms\")\n",
    "\n",
    "## Final Presentation\n",
    "- aimed towards our peers\n",
    "- need functioning copy of final project\n",
    "- what it does and how we did it\n",
    "- how we chose data\n",
    "- what we did to the data\n",
    "- how we trained the model\n",
    "- give or take 10 min long including demonstration\n",
    "- 12 groups.\n",
    "- can switch to giving presentations during the last week of class.\n",
    "\n",
    "## Sentence Generation\n",
    "The below section takes a look at both sentence generation, as well as the optimization of some existing processes. Much of the work in this section is originally based on [a stack overflow post](https://stackoverflow.com/a/54979617) and [the Maximum Likelihood Estimator (MLE) documentation from NLTK](https://www.nltk.org/api/nltk.lm.html). The work in this section could be performed as one command if we were to use the padded_everygram_pipeline method. Unfortunatly, this method presents certain downsides when the n-gram size is larger than 2 (bigrams). This is because the sentences will be padded with more than one start of sentence or end of sentence tag. This leads to the model predicting the end of sentence tag following another end of sentence tag, thus causing an infinite loop.\n",
    "\n",
    "Instead, we will perform these steps in parts. The first part would be to get a flattened list of tokens comprised of each padded sentence in our corpus. This can be done using the flatten and pad_both_ends commands. As mentioned earlier, even if the n-gram size is larger than two, a value of 2 should be passed in to pad_both_ends. Following this, we can calculate our n-grams using everygrams, or ngrams.\n",
    "\n",
    "As per [this stackoverflow post](https://stackoverflow.com/a/60661188) the padded_everygram_pipeline creates the following iterators:\n",
    "- sentences padded and turned into sequences of `nltk.util.everygrams`\n",
    "- sentences padded as above and chained together for a flat stream of words\n",
    "\n",
    "This information should help with passing arguments to the fit method of the MLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(lm.vocab.lookup(sample_lecture_sentences[0]))\n",
    "print(lm.counts)\n",
    "print(lm.counts['a'])\n",
    "print(lm.counts[['<s>']]['The'])\n",
    "print(lm.score(\"a\"))\n",
    "print(lm.score(\"The\", [\"<s>\"]))\n",
    "print(lm.logscore(\"a\"))\n",
    "print(lm.logscore(\"The\", [\"<s>\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "' '.join(lm.generate(30, random_seed=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "' '.join(lm.generate(30, text_seed=['a'], random_seed=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at generating text until it is satisfied\n",
    "start_token = '<s>'\n",
    "end_token = '</s>'\n",
    "output = [start_token]\n",
    "print(\"starting output sequence: %s\" % str(output))\n",
    "satisfaction_threshold_log = -15\n",
    "satisfaction_threshold = 0\n",
    "satisfaction = 0\n",
    "count = 0\n",
    "while satisfaction > satisfaction_threshold_log and count < 50:\n",
    "    count += 1\n",
    "    generated = lm.generate(1, text_seed=output) # , random_seed=3\n",
    "    if isinstance(generated, str):\n",
    "        generated = [generated]\n",
    "    # print(\"generated sequence: %s\" % str(generated))\n",
    "    temp_output = output + generated\n",
    "    # print(\"new output sequence: %s\" % str(temp_output))\n",
    "    prior_token = generated[-1]\n",
    "    # print(\"new token: %s\" % prior_token)\n",
    "    # print(\"last token: %s\" % temp_output[-n:-1])\n",
    "    # # Note that Logscore only can evaluate the score for a single word, not a list as per https://www.nltk.org/_modules/nltk/lm/api.html\n",
    "    # # could change this to use P = log_10(count_ngram / count_n-1gram)\n",
    "    satisfaction = lm.score(prior_token, temp_output[-n:-1])\n",
    "    print(\"satisfaction for (\\\"%10s\\\" | %s) =: %.2f\" % (prior_token, \n",
    "                                                  temp_output[-n:-1], \n",
    "                                                  satisfaction))\n",
    "    # if satisfaction > satisfaction_threshold_log:\n",
    "    if satisfaction > satisfaction_threshold:\n",
    "        output += generated\n",
    "        if generated == [end_token]:\n",
    "            output += [start_token]\n",
    "    else:\n",
    "        break\n",
    "    \n",
    "print(\"output: %s\" % ' '.join(output))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes for final project:\n",
    "- make system work better. Go grab a bunch of data and add it to the model.\n",
    "- Tune the model if you have time, but throwing more data at is is probably less effort per improvement.\n",
    "- improve the output through more data.\n",
    "- Allow input to be more variable and less brittle.\n",
    "- five page writeup containing everything we did and what data was used, etc.\n",
    "- Presentation as a brief version of the paper + demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
