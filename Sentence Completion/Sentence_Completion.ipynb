{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Completion\n",
    "This notebook is used for a final project as a part of the CS 4980 Natural Language Processing course at the Milwaukee School of Engineering during the spring term of 2023. This notebook was created by [Grant Fass](grantfass@gmail.com) and [Nicholas Kaja](kajan@msoe.edu). The following notebook will explore the problem of natural language sentence completion. This notebook can also be reached directly through the following GitHub repository: [https://github.com/GrantFass/NLP/tree/main/Sentence%20Completion](https://github.com/GrantFass/NLP/tree/main/Sentence%20Completion)\n",
    "\n",
    "Natural language sentence completion involves determining what word best fits in a blank present in a sentence. This type of question is typically found on the Scholastic Aptitude Test (SAT). It is useful because it can measure the performance of a language model (LM) on questions that educational experts deem important. LMs that perform well on this type of problem will likely perform better on broader tasks.\n",
    "\n",
    "The primary method for forming a sentence completion model is to compute the probability of each possible sentence tehn choose the most probable option. This probability computation can be done using n-grams, Latent Semantic Analysis (LSA), and Syntactic Dependency Trees. Some research has also been done into combining these with methods of preserving long-range dependencies in the sentences. Of these options, N-grams is one of the easiest starting points due to its ease of implementation and understanding. N-grams also allow for sufficient variations over the base model such as different N-gram algorithms, different values of N, and different methods of tokenization.\n",
    "\n",
    "We will be constructing our models from a dataset of Khan Academy lecture transcripts. This dataset was scraped using BeautifulSoup during January of 2023 by Nicholas Kaja as a part of a senior design project. The performance of our model will be evaluated on a [SAT Question Dataset](https://github.com/ctr4si/sentence-completion/tree/master/data/completion). After evaluation, we plan to take our best performing model and apply it to sentence generation. This will allow us to get a better feel for how well it performs. Once our model is working, we also plan to implement some additional features such as Named Entity Recognition (NER). If there is enough time we also plan to investigate using the [OpenAI tokenizer](https://platform.openai.com/tokenizer) instead of the [NLTK Word Tokenizer](https://www.nltk.org/api/nltk.tokenize.html) or the [SpaCy Tokenizer](https://spacy.io/api/tokenizer). The OpenAI tokenizer has a [python package found on github](https://github.com/openai/tiktoken).\n",
    "\n",
    "For more information please see the Data Collection And Processing document as well as the Project Background document. Both of these documents can be found in the repository under the Sentence Completion directory.\n",
    "\n",
    "## Pip Installations\n",
    "The below magic command should install all of the required python packages to run this project. If it does not run successfully please try to run the command through an admin shell instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in /usr/local/anaconda3/lib/python3.7/site-packages (from -r requirements.txt (line 1)) (4.11.1)\n",
      "Requirement already satisfied: contractions in /home/fassg/.local/lib/python3.7/site-packages (from -r requirements.txt (line 2)) (0.1.73)\n",
      "Requirement already satisfied: matplotlib in /usr/local/anaconda3/lib/python3.7/site-packages (from -r requirements.txt (line 3)) (3.5.1)\n",
      "Requirement already satisfied: nltk in /usr/local/anaconda3/lib/python3.7/site-packages (from -r requirements.txt (line 4)) (3.7)\n",
      "Requirement already satisfied: numpy in /home/fassg/.local/lib/python3.7/site-packages (from -r requirements.txt (line 5)) (1.21.5)\n",
      "Requirement already satisfied: pandas in /home/fassg/.local/lib/python3.7/site-packages (from -r requirements.txt (line 6)) (1.3.5)\n",
      "Requirement already satisfied: scikit_learn in /home/fassg/.local/lib/python3.7/site-packages (from -r requirements.txt (line 7)) (1.0.2)\n",
      "Requirement already satisfied: seaborn in /home/fassg/.local/lib/python3.7/site-packages (from -r requirements.txt (line 8)) (0.11.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/anaconda3/lib/python3.7/site-packages (from -r requirements.txt (line 9)) (4.64.0)\n",
      "Requirement already satisfied: Unidecode in /usr/local/anaconda3/lib/python3.7/site-packages (from -r requirements.txt (line 10)) (1.2.0)\n",
      "Requirement already satisfied: ipywidgets in /usr/local/anaconda3/lib/python3.7/site-packages (from -r requirements.txt (line 11)) (7.6.5)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/anaconda3/lib/python3.7/site-packages (from beautifulsoup4->-r requirements.txt (line 1)) (2.3.1)\n",
      "Requirement already satisfied: textsearch>=0.0.21 in /home/fassg/.local/lib/python3.7/site-packages (from contractions->-r requirements.txt (line 2)) (0.0.24)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/fassg/.local/lib/python3.7/site-packages (from matplotlib->-r requirements.txt (line 3)) (9.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/anaconda3/lib/python3.7/site-packages (from matplotlib->-r requirements.txt (line 3)) (2.8.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/anaconda3/lib/python3.7/site-packages (from matplotlib->-r requirements.txt (line 3)) (21.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/anaconda3/lib/python3.7/site-packages (from matplotlib->-r requirements.txt (line 3)) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/anaconda3/lib/python3.7/site-packages (from matplotlib->-r requirements.txt (line 3)) (1.4.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/anaconda3/lib/python3.7/site-packages (from matplotlib->-r requirements.txt (line 3)) (4.25.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/anaconda3/lib/python3.7/site-packages (from matplotlib->-r requirements.txt (line 3)) (3.0.4)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/anaconda3/lib/python3.7/site-packages (from nltk->-r requirements.txt (line 4)) (2022.7.9)\n",
      "Requirement already satisfied: joblib in /usr/local/anaconda3/lib/python3.7/site-packages (from nltk->-r requirements.txt (line 4)) (1.1.0)\n",
      "Requirement already satisfied: click in /usr/local/anaconda3/lib/python3.7/site-packages (from nltk->-r requirements.txt (line 4)) (8.0.4)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/anaconda3/lib/python3.7/site-packages (from pandas->-r requirements.txt (line 6)) (2022.1)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /usr/local/anaconda3/lib/python3.7/site-packages (from scikit_learn->-r requirements.txt (line 7)) (1.7.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/fassg/.local/lib/python3.7/site-packages (from scikit_learn->-r requirements.txt (line 7)) (3.1.0)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/anaconda3/lib/python3.7/site-packages (from ipywidgets->-r requirements.txt (line 11)) (6.9.1)\n",
      "Requirement already satisfied: ipython>=4.0.0 in /usr/local/anaconda3/lib/python3.7/site-packages (from ipywidgets->-r requirements.txt (line 11)) (7.31.1)\n",
      "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/anaconda3/lib/python3.7/site-packages (from ipywidgets->-r requirements.txt (line 11)) (1.0.0)\n",
      "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/anaconda3/lib/python3.7/site-packages (from ipywidgets->-r requirements.txt (line 11)) (5.3.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/anaconda3/lib/python3.7/site-packages (from ipywidgets->-r requirements.txt (line 11)) (5.1.1)\n",
      "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/anaconda3/lib/python3.7/site-packages (from ipywidgets->-r requirements.txt (line 11)) (0.2.0)\n",
      "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/anaconda3/lib/python3.7/site-packages (from ipywidgets->-r requirements.txt (line 11)) (3.5.2)\n",
      "Requirement already satisfied: debugpy<2.0,>=1.0.0 in /usr/local/anaconda3/lib/python3.7/site-packages (from ipykernel>=4.5.1->ipywidgets->-r requirements.txt (line 11)) (1.5.1)\n",
      "Requirement already satisfied: nest-asyncio in /usr/local/anaconda3/lib/python3.7/site-packages (from ipykernel>=4.5.1->ipywidgets->-r requirements.txt (line 11)) (1.5.5)\n",
      "Requirement already satisfied: tornado<7.0,>=4.2 in /usr/local/anaconda3/lib/python3.7/site-packages (from ipykernel>=4.5.1->ipywidgets->-r requirements.txt (line 11)) (6.1)\n",
      "Requirement already satisfied: matplotlib-inline<0.2.0,>=0.1.0 in /usr/local/anaconda3/lib/python3.7/site-packages (from ipykernel>=4.5.1->ipywidgets->-r requirements.txt (line 11)) (0.1.2)\n",
      "Requirement already satisfied: jupyter-client<8.0 in /usr/local/anaconda3/lib/python3.7/site-packages (from ipykernel>=4.5.1->ipywidgets->-r requirements.txt (line 11)) (7.2.2)\n",
      "Requirement already satisfied: jedi>=0.16 in /usr/local/anaconda3/lib/python3.7/site-packages (from ipython>=4.0.0->ipywidgets->-r requirements.txt (line 11)) (0.18.1)\n",
      "Requirement already satisfied: decorator in /usr/local/anaconda3/lib/python3.7/site-packages (from ipython>=4.0.0->ipywidgets->-r requirements.txt (line 11)) (5.1.1)\n",
      "Requirement already satisfied: pickleshare in /usr/local/anaconda3/lib/python3.7/site-packages (from ipython>=4.0.0->ipywidgets->-r requirements.txt (line 11)) (0.7.5)\n",
      "Requirement already satisfied: pygments in /usr/local/anaconda3/lib/python3.7/site-packages (from ipython>=4.0.0->ipywidgets->-r requirements.txt (line 11)) (2.11.2)\n",
      "Requirement already satisfied: setuptools>=18.5 in /usr/local/anaconda3/lib/python3.7/site-packages (from ipython>=4.0.0->ipywidgets->-r requirements.txt (line 11)) (63.4.1)\n",
      "Requirement already satisfied: backcall in /usr/local/anaconda3/lib/python3.7/site-packages (from ipython>=4.0.0->ipywidgets->-r requirements.txt (line 11)) (0.2.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/local/anaconda3/lib/python3.7/site-packages (from ipython>=4.0.0->ipywidgets->-r requirements.txt (line 11)) (4.8.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/anaconda3/lib/python3.7/site-packages (from ipython>=4.0.0->ipywidgets->-r requirements.txt (line 11)) (3.0.20)\n",
      "Requirement already satisfied: typing-extensions in /home/fassg/.local/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib->-r requirements.txt (line 3)) (4.0.1)\n",
      "Requirement already satisfied: jsonschema>=2.6 in /usr/local/anaconda3/lib/python3.7/site-packages (from nbformat>=4.2.0->ipywidgets->-r requirements.txt (line 11)) (4.4.0)\n",
      "Requirement already satisfied: jupyter-core in /usr/local/anaconda3/lib/python3.7/site-packages (from nbformat>=4.2.0->ipywidgets->-r requirements.txt (line 11)) (4.10.0)\n",
      "Requirement already satisfied: fastjsonschema in /usr/local/anaconda3/lib/python3.7/site-packages (from nbformat>=4.2.0->ipywidgets->-r requirements.txt (line 11)) (2.15.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/anaconda3/lib/python3.7/site-packages (from python-dateutil>=2.7->matplotlib->-r requirements.txt (line 3)) (1.16.0)\n",
      "Requirement already satisfied: pyahocorasick in /home/fassg/.local/lib/python3.7/site-packages (from textsearch>=0.0.21->contractions->-r requirements.txt (line 2)) (2.0.0)\n",
      "Requirement already satisfied: anyascii in /home/fassg/.local/lib/python3.7/site-packages (from textsearch>=0.0.21->contractions->-r requirements.txt (line 2)) (0.3.2)\n",
      "Requirement already satisfied: notebook>=4.4.1 in /usr/local/anaconda3/lib/python3.7/site-packages (from widgetsnbextension~=3.5.0->ipywidgets->-r requirements.txt (line 11)) (6.4.12)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/anaconda3/lib/python3.7/site-packages (from click->nltk->-r requirements.txt (line 4)) (4.11.3)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/anaconda3/lib/python3.7/site-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets->-r requirements.txt (line 11)) (0.8.3)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/anaconda3/lib/python3.7/site-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets->-r requirements.txt (line 11)) (0.18.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /usr/local/anaconda3/lib/python3.7/site-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets->-r requirements.txt (line 11)) (21.4.0)\n",
      "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/anaconda3/lib/python3.7/site-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets->-r requirements.txt (line 11)) (5.2.0)\n",
      "Requirement already satisfied: pyzmq>=22.3 in /usr/local/anaconda3/lib/python3.7/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets->-r requirements.txt (line 11)) (23.2.0)\n",
      "Requirement already satisfied: entrypoints in /usr/local/anaconda3/lib/python3.7/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets->-r requirements.txt (line 11)) (0.4)\n",
      "Requirement already satisfied: argon2-cffi in /usr/local/anaconda3/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->-r requirements.txt (line 11)) (21.3.0)\n",
      "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/anaconda3/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->-r requirements.txt (line 11)) (1.8.0)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /usr/local/anaconda3/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->-r requirements.txt (line 11)) (0.13.1)\n",
      "Requirement already satisfied: prometheus-client in /usr/local/anaconda3/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->-r requirements.txt (line 11)) (0.14.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/anaconda3/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->-r requirements.txt (line 11)) (2.10.1)\n",
      "Requirement already satisfied: nbconvert>=5 in /usr/local/anaconda3/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->-r requirements.txt (line 11)) (6.4.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/anaconda3/lib/python3.7/site-packages (from pexpect>4.3->ipython>=4.0.0->ipywidgets->-r requirements.txt (line 11)) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/anaconda3/lib/python3.7/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets->-r requirements.txt (line 11)) (0.2.5)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/anaconda3/lib/python3.7/site-packages (from importlib-metadata->click->nltk->-r requirements.txt (line 4)) (3.8.0)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/anaconda3/lib/python3.7/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->-r requirements.txt (line 11)) (1.5.0)\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in /usr/local/anaconda3/lib/python3.7/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->-r requirements.txt (line 11)) (0.5.13)\n",
      "Requirement already satisfied: jupyterlab-pygments in /usr/local/anaconda3/lib/python3.7/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->-r requirements.txt (line 11)) (0.1.2)\n",
      "Requirement already satisfied: bleach in /usr/local/anaconda3/lib/python3.7/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->-r requirements.txt (line 11)) (4.1.0)\n",
      "Requirement already satisfied: testpath in /usr/local/anaconda3/lib/python3.7/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->-r requirements.txt (line 11)) (0.6.0)\n",
      "Requirement already satisfied: defusedxml in /usr/local/anaconda3/lib/python3.7/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->-r requirements.txt (line 11)) (0.7.1)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/anaconda3/lib/python3.7/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->-r requirements.txt (line 11)) (0.8.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/anaconda3/lib/python3.7/site-packages (from jinja2->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->-r requirements.txt (line 11)) (2.0.1)\n",
      "Requirement already satisfied: argon2-cffi-bindings in /usr/local/anaconda3/lib/python3.7/site-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->-r requirements.txt (line 11)) (21.2.0)\n",
      "Requirement already satisfied: cffi>=1.0.1 in /usr/local/anaconda3/lib/python3.7/site-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->-r requirements.txt (line 11)) (1.15.1)\n",
      "Requirement already satisfied: webencodings in /usr/local/anaconda3/lib/python3.7/site-packages (from bleach->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->-r requirements.txt (line 11)) (0.5.1)\n",
      "Requirement already satisfied: pycparser in /usr/local/anaconda3/lib/python3.7/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->-r requirements.txt (line 11)) (2.21)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "This cell contains the imports and some setup functions. This is mostly generalized which means there are extra imports as of now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package names to /home/fassg/nltk_data...\n",
      "[nltk_data]   Package names is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/fassg/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package state_union to\n",
      "[nltk_data]     /home/fassg/nltk_data...\n",
      "[nltk_data]   Package state_union is already up-to-date!\n",
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     /home/fassg/nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n",
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     /home/fassg/nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/fassg/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/fassg/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/fassg/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict\n",
    "import pandas as pd\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from nltk.util import everygrams\n",
    "import nltk.data\n",
    "import re\n",
    "import contractions\n",
    "from bs4 import BeautifulSoup\n",
    "import unidecode\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score\n",
    "from sklearn.metrics import f1_score, recall_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import math\n",
    "import numpy as np\n",
    "import copy\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "nltk.download([\n",
    "\"names\",\n",
    "\"stopwords\",\n",
    "\"state_union\",\n",
    "\"twitter_samples\",\n",
    "\"movie_reviews\",\n",
    "\"averaged_perceptron_tagger\",\n",
    "\"vader_lexicon\",\n",
    "\"punkt\",\n",
    "])\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "encoder = LabelEncoder()\n",
    "sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "tqdm.pandas()\n",
    "n = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading  \n",
    "This section is used to load in all of the training data for the vocabulary. This comes from the 5 csv files of Khan Academy lectures. These files are all combined into a single dataframe for easier use and analysis. In the future we may need to include other sources of data such as ted talks or even ebooks in order to widen our vocabulary.\n",
    "\n",
    "We also load in the SAT dataset here. This is a short dataset that we are primarily using as our testing set. The goal here is to guess which response option, out of five, is the correct response for a fill-in-the-blank SAT question. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 8261 entries, 0 to 2789\n",
      "Data columns (total 6 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   course       8261 non-null   object\n",
      " 1   unit         8261 non-null   object\n",
      " 2   lesson       8261 non-null   object\n",
      " 3   video_title  8261 non-null   object\n",
      " 4   about        8261 non-null   object\n",
      " 5   transcript   8261 non-null   object\n",
      "dtypes: object(6)\n",
      "memory usage: 451.8+ KB\n"
     ]
    }
   ],
   "source": [
    "computing_df = pd.read_csv(Path(\"Datasets/KhanAcademy/Computing.csv\"))\n",
    "computing_df = computing_df.dropna()\n",
    "\n",
    "economics_df = pd.read_csv(Path(\"Datasets/KhanAcademy/Economics.csv\"))\n",
    "economics_df = economics_df.dropna()\n",
    "\n",
    "humanities_df = pd.read_csv(Path(\"Datasets/KhanAcademy/Humanities.csv\"))\n",
    "humanities_df = humanities_df.dropna()\n",
    "\n",
    "math_df = pd.read_csv(Path(\"Datasets/KhanAcademy/Math.csv\"))\n",
    "math_df = math_df.dropna()\n",
    "\n",
    "science_df = pd.read_csv(Path(\"Datasets/KhanAcademy/Science.csv\"))\n",
    "science_df = science_df.dropna()\n",
    "\n",
    "khan_dfs = [computing_df, economics_df, humanities_df, math_df, science_df]\n",
    "khan = pd.concat(khan_dfs, axis=0)\n",
    "khan.info(verbose=True, show_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Datasets\\\\SAT_Question_Dataset.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_324993/2687519656.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Datasets\\\\SAT_Question_Dataset.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_counts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoding_errors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         )\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m                 \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m             )\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Datasets\\\\SAT_Question_Dataset.csv'"
     ]
    }
   ],
   "source": [
    "sat = pd.read_csv(Path(\"Datasets/SAT_Question_Dataset.csv\"))\n",
    "sat.info(verbose=True, show_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning\n",
    "This section is dedicated to cleaning up some of our vocabulary dataset before it is used to create the vocabulary. We have decided, for simplicity, to remove the first sentence of every transcript in the dataset. This will allow us to easily remove all of the tagging information such as \"[Instructor]\" and \"- Speaker 1:\". Most of the first sentences are greetings and introductions. As such they are not necessarily as important to answering the SAT question dataset. We have also elected to expand contractions, remove html tags, remove quotes, and replace accented characters.\n",
    "\n",
    "The same general cleaning method is run over our testing data as well. The first sentence removal is not performed on the testing data.\n",
    "\n",
    "[Note] Before this section, as a part of the cleaning, we will want to perform SpaCy NER. Use SpaCy NER to identify the multi-word-expressions then use the [nltk.tokenize.mwe module](https://www.nltk.org/api/nltk.tokenize.mwe.html) to combine the tokens - Grant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_base(x: str):\n",
    "        # remove any html tags\n",
    "        x = BeautifulSoup(x, \"html.parser\").get_text(separator=\" \")\n",
    "        # # set all to lower\n",
    "        # x = x.lower()\n",
    "        # clean up the contractions\n",
    "        x = contractions.fix(x)\n",
    "        # remove accended characters\n",
    "        x = unidecode.unidecode(x)\n",
    "        # # remove stopwords: https://stackoverflow.com/questions/19560498/faster-way-to-remove-stop-words-in-python\n",
    "        # x = ' '.join([word for word in x.split() if word not in cachedStopWords]) # slower to use word tokenize\n",
    "        # # fix punctuation spacing\n",
    "        # x = re.sub(r'(?<=[\\.\\,\\?])(?=[^\\s])', r' ', x)\n",
    "        # # strip punctuation\n",
    "        # x = re.sub(r'[\\.\\,\\?\\\\\\/\\<\\>\\;\\:\\[\\]\\{\\}]', r'', x)\n",
    "        # strip quotes\n",
    "        x = x.replace('\\'', '').replace('\\\"', '')\n",
    "        # remove some actions\n",
    "        remove_list = ['(Laughter)', '(laughter)', '(Music)', '(music)', '(Music ends)', '(Audience cheers)', '(Applause)', '(Applause ends)', '(Applause continues)', '(Bells)', '(Trumpet)', '(Clears throat)']\n",
    "        x = ' '.join([word for word in x.split() if word not in remove_list])\n",
    "        # remove extraneous items\n",
    "        x = x.replace(' -- ', '').replace(' .. ', ' ').replace(' ... ', ' ')\n",
    "        # remove extra whitespace\n",
    "        x = ' '.join(x.strip().split())\n",
    "        # # may want to add lematization\n",
    "        # x = ' '.join([lemmatizer.lemmatize(word) for word in x.split()])\n",
    "        # remove some of the extra bracket tags\n",
    "        x = re.sub(r\"\\s{2,}\", \" \", re.sub(r\"[\\(\\[\\{][^\\)\\]\\}]*[\\)\\]\\}]\", \"\", x))\n",
    "        return x\n",
    "\n",
    "def remove_first_sentence(doc):\n",
    "    \"\"\"\n",
    "    This removes the first sentence of a document. We use this to remove all narrator / speaker tags, and\n",
    "    to remove unnecessary introductory sentences that most transcripts have\n",
    "    \"\"\"\n",
    "    return ' '.join(nltk.sent_tokenize(doc)[1:])\n",
    "\n",
    "transcripts = khan['transcript']\n",
    "transcripts = transcripts.progress_apply(remove_first_sentence)\n",
    "transcripts = transcripts.progress_apply(clean_base)\n",
    "khan['clean_transcript'] = transcripts\n",
    "khan.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column_name in ['question', 'a)', 'b)', 'c)', 'd)', 'e)']:\n",
    "    sat[column_name] = sat[column_name].progress_apply(clean_base)\n",
    "sat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-Grams\n",
    "This section is used for the tokenization of our text into N-Grams. Various tokenization approaches may yield different results. As such, it is important to test different approaches here. We begin by dividing each of the transcripts into its constituent sentences. These sentences are then each tokenized using the word tokenizer provided by nltk. A start and end of sentence tag is then added to each list of tokens. Next, these tokens are used to generate all possible n-grams below a certain size threshold. Lastly, we create a frequency distribution dictionary for the n-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of all of the sentences with each sentence tokenized.\n",
    "def tokenize_transcript(transcript):\n",
    "    token_document = [word_tokenize(t) for t in sent_detector.tokenize(transcript)]\n",
    "    sents = []\n",
    "    start_of_sentence = \"<sent>\"\n",
    "    end_of_sentence = \"<\\\\sent>\"\n",
    "    for sent in token_document:\n",
    "        sent.insert(0, start_of_sentence)\n",
    "        sent.append(end_of_sentence)\n",
    "        sents.append(sent)\n",
    "    return sents\n",
    "\n",
    "sents = []\n",
    "for transcript in tqdm(khan['clean_transcript']):\n",
    "    sents += tokenize_transcript(transcript)\n",
    "\n",
    "print(len(sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 4\n",
    "print(\"Computing all N-grams where N is %d or less\" % n)\n",
    "grams = []\n",
    "for sent in tqdm(sents):\n",
    "    grams += list(everygrams(sent, min_len=2, max_len=n))\n",
    "print(\"Computing frequency distribution\")\n",
    "freq_dist = nltk.FreqDist(grams)\n",
    "freq_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n = 3\n",
    "# print(\"Computing all N-grams where N is %d or less\" % n)\n",
    "# grams = []\n",
    "# for sent in tqdm(sents):\n",
    "#     for i in range(0, 3):\n",
    "#         # TODO: change to everygrams.\n",
    "#         grams += list(ngrams(sent, i + 1))\n",
    "    \n",
    "# print(\"Computing frequency distribution\")\n",
    "# freq_dist = nltk.FreqDist(grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_dist[(',',)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_dist[('among',)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store Model\n",
    "The next step is to store the freq_dist model for use in future prediction methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_pickle(filename, model):\n",
    "    with open(filename, 'wb') as fp:\n",
    "        pickle.dump(model, fp)\n",
    "        print(\"Saved %s to file %s\" % ('freq_dist', filename))\n",
    "        \n",
    "to_pickle(filename='freq_dist.pkl', model=freq_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model\n",
    "load the stored model back in as practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_pickle(filename):\n",
    "    model = []\n",
    "    with open(filename, 'rb') as fp:\n",
    "        model = pickle.load(fp)\n",
    "    return model\n",
    "\n",
    "freq_dist = from_pickle(filename='freq_dist.pkl')\n",
    "freq_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAT Dataset\n",
    "This section focuses on the processing of individual SAT questions as well as defining a prediction method for the dataset. Below is a sample approach for the prediction method.\n",
    "\n",
    "### SAT Question Prediction Approach\n",
    "Note that this approach is slightly different from what was implemented. One of the main differences is that our tokenization currently includes punctuation as its own token whereas the below example removes punctuation.\n",
    "\n",
    "\"\"\"  \n",
    "Question:  \n",
    "Responding to criticism that the script was rambling and _____, the new screenwriter revised the dialogue for greater succinctness and _____.\n",
    "\n",
    "Possible Solutions:  \n",
    "[('engaging', 'simplicity'), ('subjective', 'abiguity'), ('muddled', 'clarity'), ('terse', 'emptiness'), ('difficult', 'abstraction')]  \n",
    "\"\"\"\n",
    "\n",
    "N-gram size of 4 max\n",
    "1. Change the blanks out to something we can use to mask the individual ones later. For example, [BLANK1] and [BLANK2].\n",
    "    - esponding to criticism that the script was rambling and [BLANK1], the new screenwriter revised the dialogue for greater succinctness and [BLANK2].<\\sent>\n",
    "2. Pull out the possible sliding windows centered around each blank.\n",
    "    - blank 1\n",
    "        - n=4\n",
    "            - ('was', 'rambling', 'and', [BLANK1])\n",
    "            - ('rambling', 'and', [BLANK1], 'the')\n",
    "            - ('and', [BLANK1], 'the', 'new')\n",
    "            - ([BLANK1], 'the', 'new', 'screenwriter')\n",
    "        - n=3\n",
    "            - ('rambling', 'and', [BLANK1])\n",
    "            - ('and', [BLANK1], 'the')\n",
    "            - ([BLANK1], 'the', 'new')\n",
    "        - n=2\n",
    "            - ('and', [BLANK1])\n",
    "            - ([BLANK1], 'the')\n",
    "        - n=1\n",
    "            - ([BLANK1])\n",
    "    - blank 2\n",
    "        - n=4\n",
    "            - ('greater', 'succinctness', 'and', [BLANK2])\n",
    "            - ('succinctness', 'and', [BLANK2], <\\sent>)\n",
    "        - n=3\n",
    "            - ('succinctness', 'and', [BLANK2])\n",
    "            - ('and', [BLANK2], <\\sent>)\n",
    "        - n=2\n",
    "            - ('and', [BLANK2])\n",
    "            - ([BLANK2], <\\sent>)\n",
    "        - n=1\n",
    "            - ([BLANK2])\n",
    "3. For each of the possible solutions:\n",
    "    1. For each of the windows:\n",
    "        1. If the first or second word in the possible solution tuple is not in the vocab then discard it as a possible solution.\n",
    "        2. Replace [BLANK1] with the first item in the possible solution tuple\n",
    "        3. Replace [BLANK2] with the second item in the possible solution tuple\n",
    "        4. Lookup each of the window n-grams against the stored n-grams and determine how many times it occured.\n",
    "        5. What to do if the count for a n-gram is 0 for one word but not for another word?\n",
    "        6. Calculate the probability of each window n-gram\n",
    "        7. sum the logs of the probabilities of each window n-gram. \n",
    "    2. Determine which of the windows had the highest log likelihood and return the associated word tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_question(question):\n",
    "    if not isinstance(question, str) or question is None:\n",
    "        return []\n",
    "    sent = word_tokenize(question)\n",
    "    start_of_sentence = \"<sent>\"\n",
    "    end_of_sentence = \"<\\\\sent>\"\n",
    "    sent.insert(0, start_of_sentence)\n",
    "    sent.append(end_of_sentence)\n",
    "    return sent\n",
    "\n",
    "def get_mask_indices(row, mask='_____'):\n",
    "    \"\"\"Returns a list containing the integer indices of where the mask occurs for a given row of the SAT dataset.\n",
    "\n",
    "    Args:\n",
    "        row : a given row of the SAT dataset\n",
    "        mask (str, optional): the mask to look for the indicies of occurance of. Defaults to '_____'.\n",
    "\n",
    "    Returns:\n",
    "        list: contains integer indices of where the mask occurs for a given row of the SAT dataset\n",
    "    \"\"\"\n",
    "    mask_indices = []\n",
    "    previous_idx = 0\n",
    "    for blank_id in range(row['blanks']):\n",
    "        idx = tokenize_question(row['question']).index(mask, previous_idx)\n",
    "        mask_indices.append(idx)\n",
    "        previous_idx = idx + 1\n",
    "    return mask_indices\n",
    "\n",
    "def extract_possible_solutions(row, mask_indices):\n",
    "    \"\"\"computes the possible solutions that can fill in the blanks for a given question in a row of the SAT dataset.\n",
    "    The possible solutions come from the a-e columns of the row while the question itself comes from the question column.\n",
    "\n",
    "    Args:\n",
    "        row (_type_): a given row of the SAT dataset.\n",
    "        mask_indices (list): list containing integer indices of where the mask occurs for a given row of the SAT dataset.\n",
    "\n",
    "    Returns:\n",
    "        list: a list of tuples. Each tuple contains the possible solutions in their respective order to the question.\n",
    "    \"\"\"\n",
    "    possible_solutions = []\n",
    "    for i in ['a)', 'b)', 'c)', 'd)', 'e)']:\n",
    "        if i in row:\n",
    "            # print(test[i])\n",
    "            tokens = tokenize_question(row[i])\n",
    "            if tokens:\n",
    "                possible_solution = []\n",
    "                for mask_idx in mask_indices:\n",
    "                    possible_solution.append(tokens[mask_idx])\n",
    "                possible_solutions.append(tuple(possible_solution))\n",
    "    return possible_solutions\n",
    "\n",
    "def get_ngrams_with_mask(mask_indices, tokenized_question, n, mask='_____'):\n",
    "    \"\"\"This method is used to compute the sliding window for the original n-gram size as well\n",
    "    as each of the subsequently smaller sizes of n. This method requires the variable 'n' to\n",
    "    be defined globally for the maximum n-gram size to look for.\n",
    "\n",
    "    Args:\n",
    "        mask_indices (list): list containing integer indices of where the mask occurs for a given row of the SAT dataset.\n",
    "        tokenized_question (list): list comprised of the individual tokens that were parsed from the input question\n",
    "        n (int): the maximum n-gram size to use.\n",
    "        mask (str, optional): the mask to look for the indicies of occurance of. Defaults to '_____'.\n",
    "        \n",
    "\n",
    "    Returns:\n",
    "        list: A list containing tuples of each n-gram is returned. If more than one \n",
    "    \"\"\"\n",
    "    ranges = []\n",
    "    count = 0\n",
    "    for mask_idx in mask_indices:\n",
    "        remask = mask + str(count)\n",
    "        tokenized_question[mask_idx] = remask\n",
    "        count += 1\n",
    "        # print(\"\\nmask_idx = %d\" % mask_idx)\n",
    "        for i in range(n, 0, -1):  # for each successively smaller n-gram size\n",
    "            # print(\"n = %d\" % i)\n",
    "            upper_bound = mask_idx + 1\n",
    "            lower_bound = mask_idx - i + 1\n",
    "            if lower_bound >= 0 and upper_bound < len(tokenized_question):\n",
    "                ranges.append((lower_bound, upper_bound))\n",
    "                # print(\"range(%d, %d)\" % (lower_bound, upper_bound))\n",
    "            for j in range(1, i):  # already processed first n-gram for size i. Now process the rest where j is the offset.\n",
    "                upper_bound += 1\n",
    "                lower_bound += 1\n",
    "                if lower_bound >= 0 and upper_bound < len(tokenized_question):\n",
    "                    ranges.append((lower_bound, upper_bound))\n",
    "                    # print(\"range(%d, %d)\" % (lower_bound, upper_bound))\n",
    "    n_grams = []\n",
    "    for indices in ranges:\n",
    "        n_gram = []\n",
    "        for i in range(indices[0], indices[1]):\n",
    "            n_gram.append(tokenized_question[i])\n",
    "        n_grams.append(n_gram)\n",
    "        \n",
    "    return n_grams\n",
    "    \n",
    "    \n",
    "print(\"compute the windows\")\n",
    "test = sat.iloc[1]\n",
    "mask = \"_____\"\n",
    "mask_indices = get_mask_indices(test)\n",
    "tokenized_question = tokenize_question(test['question'])\n",
    "possible_solutions = extract_possible_solutions(test, mask_indices)\n",
    "windows = get_ngrams_with_mask(mask_indices, tokenized_question, n=3)\n",
    "print(\"mask indicies = %s\" % str(mask_indices))\n",
    "print(possible_solutions)\n",
    "print(windows)\n",
    "print(tokenized_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_answer(possible_solutions, windows, freq_dist, mask='_____'):\n",
    "    \"\"\"method used to determine which one of the possible solutions is the most likely answer.\n",
    "\n",
    "    Args:\n",
    "        possible_solutions (list): list of tuples where each tuple is one of the possible solutions to fill in the blanks in the sentence.\n",
    "        windows (list): the windows of n-grams. These windows are centered around the locations of the blanks in the sentence.\n",
    "        freq_dist (dict): a dictionary where the key is a n-gram as a tuple and the value is the frequency count of the n-gram tuple.\n",
    "        mask (str, optional): the mask to look for the indicies of occurance of. Defaults to '_____'.\n",
    "\n",
    "    Returns:\n",
    "        (tuple): the tuple containing the words from the best possible solution predicted.\n",
    "    \"\"\"\n",
    "    log_likelihood_given_solution = []\n",
    "    for possible_solution in possible_solutions:  # compute the log likelihood of each possible solution.\n",
    "        # print(\"\\nGenerating Solutions For: %s\" % (possible_solution,))\n",
    "        log_likelihood = 0\n",
    "        for window in windows:  # go through each of the generated windows and find its probability.\n",
    "            for i in range(len(possible_solution)):  # replace the masks with each of the possible solution words\n",
    "                window = list(map(lambda x: x.replace(mask + str(i), possible_solution[i]), window))\n",
    "            # print(window)\n",
    "            ngram_size = len(window)\n",
    "            raw_ngram_count = freq_dist[tuple(window)]\n",
    "            if raw_ngram_count > 0:  # the ngram occurs in our corpus\n",
    "                ngram_count_of_prior = 1  # default to 1 for unigrams\n",
    "                if ngram_size > 1:  # not a unigram so we need to find the count of the 'given' ngram\n",
    "                    prior = window[0: len(window)-1]\n",
    "                    ngram_count_of_prior = freq_dist[tuple(prior)]\n",
    "                vocab = 0\n",
    "                raw_probability = raw_ngram_count / (ngram_count_of_prior + vocab)\n",
    "                log_probability = math.log10(raw_probability)\n",
    "                log_likelihood += log_probability\n",
    "        # print(\"Log Likelihood = %.2f\" % log_likelihood)\n",
    "        log_likelihood_given_solution.append((possible_solution, log_likelihood))\n",
    "    log_likelihood_given_solution.sort(key = lambda x: x[1], reverse = True)\n",
    "    answer = log_likelihood_given_solution[0][0]\n",
    "    # print(\"Best Answer: %s\" % (answer, ))\n",
    "    # print(\"sorted likelihoods per possible solution:\\n%s\" % str(log_likelihood_given_solution))\n",
    "    return answer\n",
    "\n",
    "best_answer = find_best_answer(possible_solutions, windows, freq_dist)\n",
    "print(\"Best Answer: %s\" % (best_answer, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_prediction_based_on_best_answer(best_answer, row, mask_indices):\n",
    "    \"\"\"method to determine the column name of the best answer\n",
    "\n",
    "    Args:\n",
    "        best_answer (tuple): the tuple containing the words from the best possible solution predicted.\n",
    "        row : a row in the pandas data frame for the SAT dataset.\n",
    "        mask_indices (list): list containing the indices of where the mask (blanks) occur.\n",
    "\n",
    "    Returns:\n",
    "        str: the column name corresponding to the best answer.\n",
    "    \"\"\"\n",
    "    for col in ['a)', 'b)', 'c)', 'd)', 'e)']:\n",
    "        if col in row:\n",
    "            tokens = tokenize_question(row[col])\n",
    "            result = True\n",
    "            count = 0\n",
    "            for mask_idx in mask_indices:\n",
    "                if tokens[mask_idx] != best_answer[count]:\n",
    "                    result = False\n",
    "                count += 1\n",
    "            if result:\n",
    "                return col.replace(\")\", \"\")\n",
    "        \n",
    "prediction = determine_prediction_based_on_best_answer(best_answer, test, mask_indices)\n",
    "print(\"The predicted answer is in column %s\" % prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(row):\n",
    "    # parse the incoming question\n",
    "    mask = \"_____\"\n",
    "    mask_indices = get_mask_indices(row, mask=mask)\n",
    "    tokenized_question = tokenize_question(row['question'])\n",
    "    possible_solutions = extract_possible_solutions(row, mask_indices)\n",
    "    windows = get_ngrams_with_mask(mask_indices, tokenized_question, n=3, mask=mask)\n",
    "    # Look against the model. Changing the freq_dist used should allow us to try different models.\n",
    "    # TODO: what if we try to use an ensemble of models with different maximum n-gram sizes?\n",
    "    best_answer = find_best_answer(possible_solutions, windows, freq_dist, mask=mask)  # note that freq_dist is a global here.\n",
    "    prediction = determine_prediction_based_on_best_answer(best_answer, row, mask_indices)\n",
    "    return prediction\n",
    "\n",
    "def score(y_true, y_pred):\n",
    "    print(\"%-15s: %.3f\" % (\"[ACCURACY]\", accuracy_score(y_true, y_pred)))\n",
    "    print(\"%-15s: %.3f\" % (\"[PRECISION]\", precision_score(y_true, y_pred, average='weighted')))\n",
    "    print(\"%-15s: %.3f\" % (\"[RECALL]\", recall_score(y_true, y_pred, average='weighted')))\n",
    "    print(\"%-15s: %.3f\" % (\"[F1-SCORE]\", f1_score(y_true, y_pred, average='weighted')))    \n",
    "\n",
    "# test the prediction method.\n",
    "prediction = [predict(test)]\n",
    "print(\"Prediction: %s\" % prediction)\n",
    "true_value = [test['ans']]\n",
    "print(\"True Value: %s\" % true_value)\n",
    "score(true_value, prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions\n",
    "This section is used to actually compute the predictions for our test data and our given input frequency distribution. The results of the prediction are then evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skf = StratifiedKFold(n_splits=10, random_state=42, shuffle=False)\n",
    "# for i, (train_index, test_index) in enumerate(skf.split(X, y)):\n",
    "predictions = sat.progress_apply(lambda row: predict(row), axis=1)\n",
    "predictions\n",
    "true_values = sat['ans']\n",
    "score(true_values, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_model_metrics_as_json(y_true, y_pred):\n",
    "    metrics = {}\n",
    "    metrics['accuracy_score'] = accuracy_score(y_true, y_pred)\n",
    "    metrics['precision_score'] = precision_score(y_true, y_pred, average='weighted')\n",
    "    metrics['recall_score'] = recall_score(y_true, y_pred, average='weighted')\n",
    "    metrics['f1_score'] = f1_score(y_true, y_pred, average='weighted')\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_metrics = record_model_metrics_as_json(true_values, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(true_values, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = confusion_matrix(true_values, predictions)\n",
    "fig, ax = plt.subplots(figsize=(8,6), dpi=100)\n",
    "display = ConfusionMatrixDisplay(conf, display_labels=['a', 'b', 'c', 'd', 'e'])\n",
    "ax.set(title='Confusion Matrix for SAT sentence completion')\n",
    "display.plot(ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Counter(sat['ans']))\n",
    "print(Counter(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x=predictions)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of Results\n",
    "Our model does not perform that well currently. We hypothesize that this is due to both the complexity of the words in the SAT question dataset as well as the size of our input dataset. Our input dataset is only comprised of around 600,000 sentences. Additionally, our input dataset comes from spoken lecture transcripts. While these transcripts should be on fairly complex topics, they still contain mostly spoken language. The SAT question dataset prompts are more complex than the language spoken in our input dataset. This leads to a mismatch between our corpus and training data where most of our n-grams either have low counts or do not exist in our corpus. This could be remedied by expanding the size of our input dataset and thusly our corpus. The research paper that our test dataset came from compiled their corpus on over 1.1 billion words in newspaper articles. Even with this large increase in corpus size they still only achieved a prediction accuracy of just over 50% on the testing dataset. This implies that the test dataset is fairly difficult to get high accuracy scores on.\n",
    "\n",
    "Note that we did not perform cross validation on our system. The primary reason for this is due to cross validation not making sense in our case. We have created a corpus and are trying to compute the accuracy of our corpus using a test set. As we are not directly training a model, it does not make sense to perform cross validation only over our test set.\n",
    "\n",
    "## Additionl Dataset Note\n",
    "[Note] One additional area of improvement that we should also test is improving the size of our test set. I found another dataset that will work for this from the paper [SC-Ques: A Sentence Completion Question Dataset for English as a Second Language Learners](https://arxiv.org/abs/2206.12036). They provided a [link to their code and their data](https://github.com/ai4ed/SC-Ques) for research purposes. This, in turn, gave a link to a [dropbox containing their data](https://www.dropbox.com/s/lzznin2hxt6rmft/SC-Ques.tar.gz?dl=0). The data that we would be looking to use to expand our training set would be found in test.jsons and train.jsons. These two files have been preprocessed in the SC-Ques-Preprocessing.ipynb notebook and saved to the individual processed_data_idx.csv files. They were stored as multiple files due to space limitations with GitHub repositories.\n",
    "\n",
    "The below section is used to read in and join each of the processed_data_idx.csv files. There should be no need to run the SC-Ques-Preprocessing.ipynb script and is only included for posterity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = Path(\"Datasets/SC-Ques\")\n",
    "sc_ques = pd.DataFrame()\n",
    "for data_path in directory_path.glob(\"**/Processed_data_*.csv\"):\n",
    "    print(data_path)\n",
    "    data = pd.read_csv(data_path)\n",
    "    sc_ques = pd.concat([sc_ques, data])\n",
    "sc_ques = sc_ques.drop(columns=['Unnamed: 0'])\n",
    "sc_ques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = sc_ques.iloc[89546]\n",
    "print(test['question'])\n",
    "print(test['a)'])\n",
    "print(test['b)'])\n",
    "print(test['c)'])\n",
    "print(test['d)'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = [predict(test)]\n",
    "print(\"Prediction: %s\" % prediction)\n",
    "true_value = [test['ans']]\n",
    "print(\"True Value: %s\" % true_value)\n",
    "score(true_value, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = sc_ques.progress_apply(lambda row: predict(row), axis=1)\n",
    "predictions\n",
    "true_values = sc_ques['ans']\n",
    "score(true_values, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_ques_metrics = record_model_metrics_as_json(true_values, predictions)\n",
    "sc_ques_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(true_values, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = confusion_matrix(true_values, predictions)\n",
    "fig, ax = plt.subplots(figsize=(8,6), dpi=100)\n",
    "display = ConfusionMatrixDisplay(conf, display_labels=['a', 'b', 'c', 'd'])\n",
    "ax.set(title='Confusion Matrix for SAT sentence completion')\n",
    "display.plot(ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store the most recent run of training metrics as a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {\"sat\": sat_metrics, \"sc_ques\": sc_ques_metrics}\n",
    "to_pickle(filename=\"metrics.pkl\", model=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO:\n",
    "- write up the readme with dependencies\n",
    "- write up a how to run\n",
    "- write up how to use.\n",
    "- pickle the model that we need. This notebook is only for training the model basically\n",
    "- Need to write up a python script that allows us to load the model (from the git repo so nobody needs to download it) and will suppport sentence completion as well as sentence writing.\n",
    "- do not want to need to download the csv's either.\n",
    "- Needs to be by Wednesday.\n",
    "- Play with removing punctuation during cleaning.\n",
    "- number to word conversion (look at \"ozone contains _____ oxygen atoms\")\n",
    "\n",
    "## Final Presentation\n",
    "- aimed towards our peers\n",
    "- need functioning copy of final project\n",
    "- what it does and how we did it\n",
    "- how we chose data\n",
    "- what we did to the data\n",
    "- how we trained the model\n",
    "- give or take 10 min long including demonstration\n",
    "- 12 groups.\n",
    "- can switch to giving presentations during the last week of class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "humanities_df = pd.read_csv(Path(\"Datasets/KhanAcademy/Humanities.csv\"))\n",
    "sample_lecture = humanities_df.iloc[0]['transcript']\n",
    "sample_lecture = remove_first_sentence(sample_lecture)\n",
    "sample_lecture = clean_base(sample_lecture)\n",
    "# sample_lecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_lecture_sentences = [word_tokenize(t) for t in sent_detector.tokenize(transcript)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at a sample process for a single sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import pad_sequence\n",
    "n = 4\n",
    "# this is the long way of doing things. there is a convenience method available.\n",
    "list(pad_sequence(sample_lecture_sentences[0],\n",
    "                  pad_left=True,\n",
    "                  left_pad_symbol=\"<s>\",\n",
    "                  pad_right=True,\n",
    "                  right_pad_symbol=\"</s>\",\n",
    "                  n=2)) # n denotes the padding for n-grams. probably do not want more than 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "list(pad_both_ends(sample_lecture_sentences[0], n=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import everygrams\n",
    "padded = list(pad_both_ends(sample_lecture_sentences[0], n=2))\n",
    "list(everygrams(padded, max_len=n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare all of the sentences in the document with flatten\n",
    "from nltk.lm.preprocessing import flatten\n",
    "list(flatten(pad_both_ends(sent, n=2) for sent in sample_lecture_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the padded_everygram_pipeline deals with all of the above steps for us.\n",
    "\n",
    "https://stackoverflow.com/a/54979617\n",
    "https://www.nltk.org/api/nltk.lm.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "train, vocab = padded_everygram_pipeline(n, sample_lecture_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm import MLE\n",
    "lm = MLE(n)\n",
    "lm.fit(train, vocab)\n",
    "print(lm.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.vocab.lookup(sample_lecture_sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lm.counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.counts['a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.counts[['<s>']]['The']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.score(\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.score(\"The\", [\"<s>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.logscore(\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.logscore(\"The\", [\"<s>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that this looks like we will need to perform steps manually instead of using padded \n",
    "# everygram pipeline so we do not have duplicate padding strings.\n",
    "lm.generate(15, random_seed=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.generate(15, text_seed=['a'], random_seed=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at generating text until it is satisfied\n",
    "start_token = '<s>'\n",
    "prior_token = start_token\n",
    "output = []\n",
    "satisfaction_threshold_log = -12\n",
    "satisfaction = 0\n",
    "count = 0\n",
    "while satisfaction > satisfaction_threshold_log and count < 10:\n",
    "    output.append(prior_token)\n",
    "    print(\"output sequence: %s\" % str(output))\n",
    "    generated = lm.generate(2, text_seed=output, random_seed=3)\n",
    "    print(\"generated sequence: %s\" % str(generated))\n",
    "    prior_token = generated[0]\n",
    "    print(\"new token: %s\" % prior_token)\n",
    "    print(\"last token: %s\" % output[-1])\n",
    "    satisfaction = lm.logscore(prior_token, [output[-1]])\n",
    "    print(\"satisfaction: %.2f\" % satisfaction)\n",
    "    \n",
    "print(\"output: %s\" % ' '.join(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create a list of all of the sentences with each sentence tokenized.\n",
    "# def tokenize_transcript(transcript):\n",
    "#     token_document = [word_tokenize(t) for t in sent_detector.tokenize(transcript)]\n",
    "#     sents = []\n",
    "#     start_of_sentence = \"<sent>\"\n",
    "#     end_of_sentence = \"<\\\\sent>\"\n",
    "#     for sent in token_document:\n",
    "#         sent.insert(0, start_of_sentence)\n",
    "#         sent.append(end_of_sentence)\n",
    "#         sents.append(sent)\n",
    "#     return sents\n",
    "# sample_lecture_sentences = tokenize_transcript(sample_lecture)\n",
    "# sample_lecture_sentences[1:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_lecture_sentences = [sent for sent in sent_detector.tokenize(sample_lecture)]\n",
    "# sample_lecture_sentences[1:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_of_sentence = \"<sent> \"\n",
    "# end_of_sentence = \" <\\\\sent>\"\n",
    "# for i in range(len(sample_lecture_sentences)):\n",
    "#     sample_lecture_sentences[i] = start_of_sentence + sample_lecture_sentences[i] + end_of_sentence\n",
    "# sample_lecture_sentences[1:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_of_sentence = \"<sent> \"\n",
    "# end_of_sentence = \" <\\\\sent>\"\n",
    "# for i in range(len(sample_lecture_sentences)):\n",
    "#     sample_lecture_sentences[i] = word_tokenize(sample_lecture_sentences[i])\n",
    "#     sample_lecture_sentences[i].insert(0, start_of_sentence)\n",
    "#     sample_lecture_sentences[i].append(end_of_sentence)\n",
    "# sample_lecture_sentences[1:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n = 4\n",
    "# from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "# train, vocab = padded_everygram_pipeline(n, sample_lecture_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.lm import MLE\n",
    "# n = 4\n",
    "# lm = MLE(n)\n",
    "# lm.fit(train, vocab)\n",
    "# print(lm.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lm.vocab.lookup(start_of_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lm.vocab.lookup(sample_lecture_sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lm.counts[start_of_sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# sents = []\n",
    "# for transcript in tqdm(khan['clean_transcript']):\n",
    "#     sents += tokenize_transcript(transcript)\n",
    "\n",
    "# print(len(sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n = 4\n",
    "# print(\"Joining sentence data of length %d together\" % len(sents))\n",
    "# tokens = []\n",
    "# for sent in tqdm(sents):\n",
    "#     tokens += sent\n",
    "# print(\"Computing all N-grams where N is %d or less\" % n)\n",
    "# from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "# train, vocab = padded_everygram_pipeline(n, tokens)\n",
    "\n",
    "\n",
    "# # grams = []\n",
    "# # for sent in tqdm(sents):\n",
    "# #     grams += list(everygrams(sent, min_len=2, max_len=n))\n",
    "# # print(\"Computing frequency distribution\")\n",
    "# # freq_dist = nltk.FreqDist(grams)\n",
    "# # freq_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/a/54979617\n",
    "# https://www.nltk.org/api/nltk.lm.html\n",
    "# from nltk.lm import MLE\n",
    "# start_token = \"<sent>\"\n",
    "# end_token = \"<\\sent>\"\n",
    "# output_sentence = [start_token]\n",
    "# n = 4\n",
    "# lm = MLE(n)\n",
    "# lm.fit(train, vocab)\n",
    "# print(lm.vocab)\n",
    "# possible_next_tokens = {}\n",
    "# for key in freq_dist.keys():\n",
    "#     if output_sentence[-1] in key:\n",
    "#         print(key)\n",
    "        # possible_next_tokens[key] = freq_dist[key]\n",
    "\n",
    "# freq_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
