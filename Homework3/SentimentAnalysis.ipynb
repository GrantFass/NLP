{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3\n",
    "## Warmup\n",
    "Given the following short movie reviews, each labeled with a genre, either comedy or action:\n",
    "1. fun, couple, love, love **comedy**\n",
    "2. fast, furious, shoot **action**\n",
    "3. couple, fly, fast, fun, fun **comedy**\n",
    "4. furious, shoot, shoot, fun **action**\n",
    "5. fly, fast, shoot, love **action**\n",
    "\n",
    "and a new document D:\n",
    "\n",
    "    fast, couple, shoot, fly\n",
    "\n",
    "compute the most likely class for D. Assume a naive Bayes classifier and use add-1 smoothing for the likelihoods.\n",
    "\n",
    "## Warmup Response\n",
    "**Total Vocab**  \n",
    "- fun: 4\n",
    "- couple: 2\n",
    "- love: 3\n",
    "- fast: 3\n",
    "- furious: 2\n",
    "- shoot: 4\n",
    "- fly: 2\n",
    "\n",
    "**Comedy Vocab**  \n",
    "- fun: 3 + 1 = 4\n",
    "- couple: 2 + 1 = 3\n",
    "- love: 2 + 1 = 3\n",
    "- fast: 1 + 1 = 2\n",
    "- furious: 0 + 1 = 1\n",
    "- shoot: 0 + 1 = 1\n",
    "- fly: 1 + 1 = 2\n",
    "\n",
    "**Action Vocab**  \n",
    "- fun: 1 + 1 = 2\n",
    "- couple: 0 + 1 = 1\n",
    "- love: 1 + 1 = 2\n",
    "- fast: 2 + 1 = 3\n",
    "- furious: 2 + 1 = 3\n",
    "- shoot: 4 + 1 = 5\n",
    "- fly: 1 + 1 = 2\n",
    "\n",
    "**Prediction**\n",
    "- the vocab has 7 words in it.\n",
    "- the comedy class has (4+3+3+2+1+1+2)=16 entries in it.\n",
    "- the action class has (2+1+2+3+3+5+2)=18 entries in it.\n",
    "- query\n",
    "    - fast\n",
    "        - comedy: log(2/(16+7))=-1.06\n",
    "        - action: log(3/(18+7))=-0.92\n",
    "    - couple\n",
    "        - comedy: log(3/(16+7))=-0.88\n",
    "        - action: log(1/(18+7))=-1.40\n",
    "    - shoot\n",
    "        - comedy: log(1/(16+7))=-1.36\n",
    "        - action: log(5/(18+7))=-0.70\n",
    "    - fly\n",
    "        - comedy: log(2/(16+7))=-1.06\n",
    "        - action: log(2/(18+7))=-1.10\n",
    "- sum\n",
    "    - comedy = (-1.06-0.88-1.36-1.06)=-4.37\n",
    "    - action = (-0.92-1.40-0.70-1.10)=-4.11\n",
    "- result\n",
    "    - because action > comedy this document is likely action.\n",
    "\n",
    "\n",
    "## Assignment\n",
    "Build a naive Bayes sentiment classifier that will assign reviews of an application as either **positive**, **neutral**, or **negative**.\n",
    "- You will need to do some basic preprocessing on the documents (normalization, etc).\n",
    "- Do not use a stop word list.\n",
    "- Ignore any Out-Of-Vocabulary (OOV) terms when classifying.\n",
    "\n",
    "You are provided a small set of pre-classified training data to build your model. The data is formatted such that each line of text contains a document (the title of a review). The first token of each line will be the classification of that review, either **POS**, **NEU**, or **NEG**. Below is a sample document:\n",
    "    \n",
    "    `POS The program was quite helpful with creating websites.`\n",
    "\n",
    "An example output of your system may look something like this:\n",
    "\n",
    "    ```\n",
    "    The program does what it should do. : POSITIVE\n",
    "    It functions adequately. : NEUTRAL\n",
    "    The program sucks. : NEGATIVE\n",
    "    This thing runs like a pregnant cow. : NEGATIVE\n",
    "    It was a little slow, but not too bad. : NEUTRAL\n",
    "    Slow. Slow. SLOW! : NEGATIVE\n",
    "    Great software! : POSITIVE\n",
    "    Worth the trouble to install. : NEUTRAL\n",
    "    ```\n",
    "\n",
    "## Report Instructions\n",
    "Once the model has been built, feed in the provided test documents and write a report detailing your results. In the report, address the following:\n",
    "- How accurate was the classifier? What was the Precision and Recall? The F-measure?\n",
    "- Choose one incorrectly classified document.\n",
    "    - Manually calculate the sentiment probabilities for the document (you can use your classifier to generate the likelihoods and prior probabilities, but do the classifying on paper)\n",
    "    - What is the difference of the probability sums of the correct class and the class assigned to the system?\n",
    "    - Identify the term or terms that caused the system to misclassify the document.\n",
    "    - Build a document (or documents) to add to the training set that would allow the system to correctly classify the document.\n",
    "        - Show the mathematical reasoning for your choice of words in the document.\n",
    "        - Rerun the tests with the additional information.\n",
    "        - Did adding the additional information change any other document classification? If so, how? Did it improve the overall accuracy of your system or make it worse?\n",
    "    - Add the MPQA Subjectivity Cues Lexicon to your system and run the tests again and report the results.\n",
    "        - Choose a document that was classified differently after adding the MPQA Subjectivity Cues Lexicon. Was it correctly or incorrectly classified? Discuss why.\n",
    "    - Finally use the provided collection of Amazon reviews from 2007 to train your classifier. Run the associated tests and report the Precision, Recall, and F-measure.\n",
    "    - Briefly discuss what you learned from this assignment, what you liked or disliked about the assignment and, optionally, anything you would like to see changed or added to improve the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package names to\n",
      "[nltk_data]     C:\\Users\\Grant\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package names is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Grant\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package state_union to\n",
      "[nltk_data]     C:\\Users\\Grant\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package state_union is already up-to-date!\n",
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     C:\\Users\\Grant\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n",
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\Grant\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Grant\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Grant\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Grant\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "## imports\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import contractions\n",
    "import unidecode\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score\n",
    "from sklearn.metrics import f1_score, recall_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "nltk.download([\n",
    "\"names\",\n",
    "\"stopwords\",\n",
    "\"state_union\",\n",
    "\"twitter_samples\",\n",
    "\"movie_reviews\",\n",
    "\"averaged_perceptron_tagger\",\n",
    "\"vader_lexicon\",\n",
    "\"punkt\",\n",
    "])\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "encoder = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(y_pred, y_true):\n",
    "    print(\"%-15s: %.3f\" % (\"[ACCURACY]\", accuracy_score(y_pred, y_true)))\n",
    "    print(\"%-15s: %.3f\" % (\"[PRECISION]\", precision_score(y_pred, y_true, average='weighted')))\n",
    "    print(\"%-15s: %.3f\" % (\"[RECALL]\", recall_score(y_pred, y_true, average='weighted')))\n",
    "    print(\"%-15s: %.3f\" % (\"[F1-SCORE]\", f1_score(y_pred, y_true, average='weighted')))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_string(x: str) -> list[str]:\n",
    "    x = contractions.fix(x) # expand contractions\n",
    "    x = unidecode.unidecode(x) # remove accents\n",
    "    x = ' '.join(x.strip().split()) # remove extra whitespace\n",
    "    x = re.sub(r'[\\.\\,\\?\\\\\\/\\<\\>\\;\\:\\[\\]\\{\\}]', r'', x) # punctuation\n",
    "    # # could do lemmatization using the WordNetLemmatizer\n",
    "    # x = ' '.join([lemmatizer.lemmatize(word) for word in x.split()])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cls</th>\n",
       "      <th>text</th>\n",
       "      <th>encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>POS</td>\n",
       "      <td>The program was quite helpful with creating we...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>POS</td>\n",
       "      <td>I really really really liked the cute icons!</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NEU</td>\n",
       "      <td>The program did its job but nothing special</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NEG</td>\n",
       "      <td>Why did they even bother releasing this software</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NEG</td>\n",
       "      <td>This program did not do anything it was promis...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NEU</td>\n",
       "      <td>The software was adequate</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NEU</td>\n",
       "      <td>I have used better programs I have used worse</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>POS</td>\n",
       "      <td>The pages it generated were just what I needed</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>POS</td>\n",
       "      <td>The software was intuitive and easy to use</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>POS</td>\n",
       "      <td>The program runs well on my laptop</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NEG</td>\n",
       "      <td>It was slow buggy and painful to use</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>NEG</td>\n",
       "      <td>This is the worst piece of garbage I have ever...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>NEG</td>\n",
       "      <td>I want my money back</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>POS</td>\n",
       "      <td>Best money I ever spent</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NEU</td>\n",
       "      <td>It is cheap software and you get what you pay for</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>NEU</td>\n",
       "      <td>The software generates web pages based on inpu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>NEU</td>\n",
       "      <td>It runs on all major platforms including iOS</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>NEG</td>\n",
       "      <td>I had nothing but trouble with the software</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>NEU</td>\n",
       "      <td>It was not too fast but not too slow either</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    cls                                               text encoded\n",
       "0   POS  The program was quite helpful with creating we...       2\n",
       "1   POS       I really really really liked the cute icons!       2\n",
       "2   NEU        The program did its job but nothing special       1\n",
       "3   NEG   Why did they even bother releasing this software       0\n",
       "4   NEG  This program did not do anything it was promis...       0\n",
       "5   NEU                          The software was adequate       1\n",
       "6   NEU      I have used better programs I have used worse       1\n",
       "7   POS     The pages it generated were just what I needed       2\n",
       "8   POS         The software was intuitive and easy to use       2\n",
       "9   POS                 The program runs well on my laptop       2\n",
       "10  NEG               It was slow buggy and painful to use       0\n",
       "11  NEG  This is the worst piece of garbage I have ever...       0\n",
       "12  NEG                               I want my money back       0\n",
       "13  POS                            Best money I ever spent       2\n",
       "14  NEU  It is cheap software and you get what you pay for       1\n",
       "15  NEU  The software generates web pages based on inpu...       1\n",
       "16  NEU       It runs on all major platforms including iOS       1\n",
       "17  NEG        I had nothing but trouble with the software       0\n",
       "18  NEU        It was not too fast but not too slow either       1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Read in the training data\n",
    "training = []\n",
    "strings: list[str] = []\n",
    "filename = \"trainingSet.txt\"\n",
    "with open(filename, \"r\") as f:\n",
    "    for line in f:\n",
    "        tokens = line.split(\" \")\n",
    "        classification = tokens.pop(0)\n",
    "        x = clean_string(str(line[line.index(\" \") + 1:-1]))\n",
    "        strings.append(x)\n",
    "        training +=[(classification, x)]\n",
    "text = '\\n'.join(strings)\n",
    "data = pd.DataFrame(training, columns=['cls', 'text'])\n",
    "data['cls'] = data['cls'].astype(\"category\")\n",
    "data['encoded'] = encoder.fit_transform(data['cls'])\n",
    "data['encoded'] = data['encoded'].astype(\"category\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cls</th>\n",
       "      <th>text</th>\n",
       "      <th>encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>POS</td>\n",
       "      <td>The program does what it should do</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NEU</td>\n",
       "      <td>It functions adequately</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NEG</td>\n",
       "      <td>The program sucks</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NEG</td>\n",
       "      <td>This thing runs like a pregnant cow</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NEU</td>\n",
       "      <td>It was a little slow but not too bad</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NEG</td>\n",
       "      <td>Slow Slow SLOW!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>POS</td>\n",
       "      <td>Great software!</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NEU</td>\n",
       "      <td>Worth the trouble to install</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cls                                  text encoded\n",
       "0  POS    The program does what it should do       2\n",
       "1  NEU               It functions adequately       1\n",
       "2  NEG                     The program sucks       0\n",
       "3  NEG   This thing runs like a pregnant cow       0\n",
       "4  NEU  It was a little slow but not too bad       1\n",
       "5  NEG                       Slow Slow SLOW!       0\n",
       "6  POS                       Great software!       2\n",
       "7  NEU          Worth the trouble to install       1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Read in the testing data\n",
    "training = []\n",
    "strings: list[str] = []\n",
    "filename = \"testSet.txt\"\n",
    "with open(filename, \"r\") as f:\n",
    "    for line in f:\n",
    "        tokens = line.split(\" \")\n",
    "        classification = tokens.pop(0)\n",
    "        x = clean_string(str(line[line.index(\" \") + 1:-1]))\n",
    "        strings.append(x)\n",
    "        training +=[(classification, x)]\n",
    "text = '\\n'.join(strings)\n",
    "test_data = pd.DataFrame(training, columns=['cls', 'text'])\n",
    "test_data['cls'] = test_data['cls'].astype(\"category\")\n",
    "test_data['encoded'] = encoder.transform(test_data['cls'])\n",
    "test_data['encoded'] = test_data['encoded'].astype(\"category\")\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>len</th>\n",
       "      <th>word1</th>\n",
       "      <th>pos1</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>priorpolarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>weaksubj</td>\n",
       "      <td>1</td>\n",
       "      <td>abandoned</td>\n",
       "      <td>adj</td>\n",
       "      <td>n</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>weaksubj</td>\n",
       "      <td>1</td>\n",
       "      <td>abandonment</td>\n",
       "      <td>noun</td>\n",
       "      <td>n</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>weaksubj</td>\n",
       "      <td>1</td>\n",
       "      <td>abandon</td>\n",
       "      <td>verb</td>\n",
       "      <td>y</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>strongsubj</td>\n",
       "      <td>1</td>\n",
       "      <td>abase</td>\n",
       "      <td>verb</td>\n",
       "      <td>y</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>strongsubj</td>\n",
       "      <td>1</td>\n",
       "      <td>abasement</td>\n",
       "      <td>anypos</td>\n",
       "      <td>y</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8217</th>\n",
       "      <td>strongsubj</td>\n",
       "      <td>1</td>\n",
       "      <td>zealot</td>\n",
       "      <td>noun</td>\n",
       "      <td>n</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8218</th>\n",
       "      <td>strongsubj</td>\n",
       "      <td>1</td>\n",
       "      <td>zealous</td>\n",
       "      <td>adj</td>\n",
       "      <td>n</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8219</th>\n",
       "      <td>strongsubj</td>\n",
       "      <td>1</td>\n",
       "      <td>zealously</td>\n",
       "      <td>anypos</td>\n",
       "      <td>n</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8220</th>\n",
       "      <td>strongsubj</td>\n",
       "      <td>1</td>\n",
       "      <td>zenith</td>\n",
       "      <td>noun</td>\n",
       "      <td>n</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8221</th>\n",
       "      <td>strongsubj</td>\n",
       "      <td>1</td>\n",
       "      <td>zest</td>\n",
       "      <td>noun</td>\n",
       "      <td>n</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8222 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            type len        word1    pos1 stemmed priorpolarity\n",
       "0       weaksubj   1    abandoned     adj       n      negative\n",
       "1       weaksubj   1  abandonment    noun       n      negative\n",
       "2       weaksubj   1      abandon    verb       y      negative\n",
       "3     strongsubj   1        abase    verb       y      negative\n",
       "4     strongsubj   1    abasement  anypos       y      negative\n",
       "...          ...  ..          ...     ...     ...           ...\n",
       "8217  strongsubj   1       zealot    noun       n      negative\n",
       "8218  strongsubj   1      zealous     adj       n      negative\n",
       "8219  strongsubj   1    zealously  anypos       n      negative\n",
       "8220  strongsubj   1       zenith    noun       n      positive\n",
       "8221  strongsubj   1         zest    noun       n      positive\n",
       "\n",
       "[8222 rows x 6 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in the MPQA Subjectivity Ques Lexicon\n",
    "filename = \"lexicon\\subjclueslen1-HLTEMNLP05.tff\"\n",
    "line_regex = r\"type=(\\w+)\\slen=(\\d+)\\sword1=([\\w-]+)\\spos1=(\\w+)\\sstemmed1=(\\w)(?:\\spolarity=(?:\\w+))*(?:\\sm)*\\spriorpolarity=(\\w+)\"\n",
    "mpqa = pd.DataFrame(columns=['type', 'len', 'word1', 'pos1', 'stemmed', 'priorpolarity'])\n",
    "with open(filename, 'r') as f:\n",
    "    for line in f:\n",
    "        # print(line)\n",
    "        result = re.search(line_regex, line)\n",
    "        mpqa = pd.concat([mpqa, pd.DataFrame.from_records([{\n",
    "            \"type\": result.group(1),\n",
    "            \"len\": result.group(2),\n",
    "            \"word1\": result.group(3),\n",
    "            \"pos1\": result.group(4),\n",
    "            \"stemmed\": result.group(5),\n",
    "            \"priorpolarity\": result.group(6)\n",
    "        }])]).reset_index(drop=True)\n",
    "mpqa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "{'neg': 0.556, 'neu': 0.444, 'pos': 0.0, 'compound': -0.3612}\n",
      "{'neg': 0.0, 'neu': 0.667, 'pos': 0.333, 'compound': 0.3612}\n",
      "{'neg': 0.0, 'neu': 0.65, 'pos': 0.35, 'compound': 0.5824}\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "{'neg': 0.0, 'neu': 0.185, 'pos': 0.815, 'compound': 0.6588}\n",
      "{'neg': 0.355, 'neu': 0.395, 'pos': 0.25, 'compound': -0.2023}\n"
     ]
    }
   ],
   "source": [
    "# NLTK polarity scores\n",
    "for string in strings:\n",
    "    print(sia.polarity_scores(string))\n",
    "# at a glance this seems to be performing poorly"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try building Naive Bayes Classifier\n",
    "Following the tutorial [Building Naive Bayes Classifier from Scratch to Perform Sentiment Analysis](https://www.analyticsvidhya.com/blog/2022/03/building-naive-bayes-classifier-from-scratch-to-perform-sentiment-analysis/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_tokenizer = nltk.tokenize.NLTKWordTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the train_test_split\n",
    "texts = data['text'].values\n",
    "labels = data['encoded'].values\n",
    "train_x, test_x, train_y, test_y = train_test_split(texts, \n",
    "                                                    labels, \n",
    "                                                    stratify=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = CountVectorizer(max_features = 5000)\n",
    "x = vec.fit_transform(train_x)\n",
    "vocab = vec.get_feature_names_out()\n",
    "x = x.toarray()\n",
    "word_counts = {}\n",
    "for l in range(3):\n",
    "    word_counts[l] = defaultdict(lambda: 0)\n",
    "for i in range(x.shape[0]):\n",
    "    l = train_y[i]\n",
    "    for j in range(len(vocab)):\n",
    "        word_counts[l][vocab[j]] += x[i][j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "so the word counts contains the count of words that occured in\n",
    "the sentences. The only difference here is that it is stratified\n",
    "based on the label. This means it has 3 separate sets of counts. \n",
    "Note that each set of counts still contains the words from the\n",
    "other sets just with a value of 0.\n",
    "\n",
    "the first is the encoded class. The second is the word to lookup.\n",
    "\"\"\"\n",
    "word_counts[2]['all']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def laplace_smoothing(n_label_items, vocab, word_counts, word, text_label):\n",
    "    a = word_counts[text_label][word] + 1\n",
    "    b = n_label_items[text_label] + len(vocab)\n",
    "    return math.log(a/b)\n",
    "\n",
    "\n",
    "def group_by_label(x, y, labels):\n",
    "    data = {}\n",
    "    for l in labels:\n",
    "        data[l] = x[np.where(y == l)]\n",
    "    return data\n",
    " \n",
    "\n",
    "def fit(x, y, labels):\n",
    "    n_label_items = {}\n",
    "    log_label_priors = {}\n",
    "    n = len(x)\n",
    "    grouped_data = group_by_label(x, y, labels)\n",
    "    for l, data in grouped_data.items():\n",
    "        n_label_items[l] = len(data)\n",
    "        log_label_priors[l] = math.log(n_label_items[l] / n)\n",
    "    return n_label_items, log_label_priors\n",
    "\n",
    "def predict(n_label_items, vocab, word_counts, log_label_priors, labels, x):\n",
    "    result = []\n",
    "    for text in x:\n",
    "        label_scores = {l: log_label_priors[l] for l in labels}\n",
    "        # print(label_scores)\n",
    "        words = set(w_tokenizer.tokenize(text))\n",
    "        for word in words:\n",
    "            if word not in vocab: continue\n",
    "            for l in labels:\n",
    "                log_w_given_l = laplace_smoothing(n_label_items, vocab, word_counts, word, l)\n",
    "                label_scores[l] += log_w_given_l\n",
    "        result.append(max(label_scores, key=label_scores.get))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 4, 1: 5, 2: 5}\n",
      "[ACCURACY]     : 0.200\n",
      "[PRECISION]    : 0.300\n",
      "[RECALL]       : 0.200\n",
      "[F1-SCORE]     : 0.240\n",
      "[1, 1, 2, 2, 1]\n",
      "[1, 0, 1, 0, 2]\n",
      "Categories (3, int64): [0, 1, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "labels = [0,1,2]\n",
    "n_label_items, log_label_priors = fit(train_x,train_y,labels)\n",
    "print(n_label_items)\n",
    "pred = predict(n_label_items, vocab, word_counts, log_label_priors, labels, test_x)\n",
    "score(pred, test_y)\n",
    "print(pred)\n",
    "print(test_y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-Implemment the Naive Bayes Model\n",
    "Now that I worked through the tutorial in the above code, I was able to rework things to use NLTK and pandas directly. This helped me understand the code better as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, test = train_test_split(data, \n",
    "#                                test_size=0.2, \n",
    "#                                shuffle=True,\n",
    "#                                stratify=data['encoded'])\n",
    "train = copy.deepcopy(data)\n",
    "test = copy.deepcopy(test_data)\n",
    "unique_labels = data['encoded'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{2: 135, 1: 150, 0: 144}\n",
      "{2: 1.9608357992719891, 1: 2.0661963149298153, 0: 2.02537432040956}\n",
      "                The program does what it should do: \tNEG: -20.95, \tNEU: -20.93, \tPOS: -20.82\n",
      "                           It functions adequately: \tNEG: -2.33, \tNEU: -1.80, \tPOS: -2.76\n",
      "                                 The program sucks: \tNEG: -6.80, \tNEU: -6.81, \tPOS: -5.97\n",
      "               This thing runs like a pregnant cow: \tNEG: -7.49, \tNEU: -8.19, \tPOS: -8.17\n",
      "              It was a little slow but not too bad: \tNEG: -26.40, \tNEU: -24.10, \tPOS: -28.72\n",
      "                                   Slow Slow SLOW!: \tNEG: -17.70, \tNEU: -17.76, \tPOS: -18.99\n",
      "                                   Great software!: \tNEG: -7.78, \tNEU: -7.50, \tPOS: -7.48\n",
      "                      Worth the trouble to install: \tNEG: -10.86, \tNEU: -12.98, \tPOS: -11.79\n",
      "[2, 1, 0, 0, 1, 0, 2, 1]\n",
      "[2, 1, 2, 0, 1, 0, 2, 0]\n",
      "[ACCURACY]     : 0.750\n",
      "[PRECISION]    : 0.792\n",
      "[RECALL]       : 0.750\n",
      "[F1-SCORE]     : 0.750\n"
     ]
    }
   ],
   "source": [
    "def gen_word_counts_with_laplace(dataframe: pd.DataFrame):\n",
    "    # get a list of all of the word keys\n",
    "    all_tokens = nltk.word_tokenize('\\n'.join(dataframe['text']))\n",
    "    all_text = nltk.Text([token.lower() for token in all_tokens])\n",
    "    all_freq_dist = all_text.vocab()\n",
    "    all_keys = all_freq_dist.keys()\n",
    "    word_counts = {}\n",
    "    # iterate through each of the label classes\n",
    "    for cls in unique_labels:\n",
    "        # for each label class compute the word frequency distribution\n",
    "        text = '\\n'.join(dataframe[dataframe['encoded'] == cls]['text'])\n",
    "        # print(\"CLS [%d]: %s\" % (cls, text))\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        tokens = [token.lower() for token in tokens]\n",
    "        n_text = nltk.Text(tokens)\n",
    "        fd = n_text.vocab()\n",
    "        dictionary = {}\n",
    "        # use the list of all keys and frequency distribution to make\n",
    "        # a frequency distribution for the class with all keys\n",
    "        # and laplacian smoothing\n",
    "        for key in all_keys:\n",
    "            count = fd[key] + 1 # add 1 for laplace smoothing\n",
    "            dictionary[key] = count\n",
    "        # add the distribution to the list for output\n",
    "        word_counts[cls] = dictionary\n",
    "    return word_counts, all_keys\n",
    "\n",
    "def fit(dataframe: pd.DataFrame):\n",
    "    num_entries_per_cls = {}\n",
    "    log_label_priors = {}\n",
    "    n = len(list(dataframe['text']))\n",
    "    grouped = [None] * len(unique_labels)\n",
    "    for cls in dataframe['encoded'].unique():\n",
    "        grouped[cls] = list(dataframe['text'][dataframe['encoded'] == cls])\n",
    "        # num_entries_per_cls[cls] = len(grouped[cls])\n",
    "        num_entries_per_cls[cls] = sum(word_counts[cls].values())\n",
    "        log_label_priors[cls] = math.log(num_entries_per_cls[cls] / n)\n",
    "    return(num_entries_per_cls, log_label_priors)\n",
    "\n",
    "def predict(x: list[str], num_entries_per_cls, log_label_priors, vocab):\n",
    "    result = []\n",
    "    for text in x:\n",
    "        label_scores = copy.deepcopy(log_label_priors)\n",
    "        text = clean_string(text)\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        tokens = [token.lower() for token in tokens]\n",
    "        # fd = nltk.Text(tokens).vocab()\n",
    "        for word in tokens:\n",
    "            if word not in vocab: \n",
    "                # print(\"[%s] missing from vocabulary\" % word)\n",
    "                continue\n",
    "            for cls in unique_labels:\n",
    "                count = word_counts[cls][word]\n",
    "                n_vocab_per_cls = num_entries_per_cls[cls] + len(vocab)\n",
    "                label_scores[cls] += math.log(count / n_vocab_per_cls)\n",
    "        # print(label_scores)\n",
    "        print(\"%50s: \\tNEG: %.2f, \\tNEU: %.2f, \\tPOS: %.2f\" % (text, label_scores[0], label_scores[1], label_scores[2]))\n",
    "        result.append(max(label_scores, key=lambda key: label_scores[key])) # ISSUE WAS HERE DUE TO USING np.argmax on a dict.\n",
    "    return result\n",
    "\n",
    "word_counts, vocab = gen_word_counts_with_laplace(train)\n",
    "num_entries_per_cls, log_label_priors = fit(train)\n",
    "print(num_entries_per_cls)\n",
    "print(log_label_priors)\n",
    "y_true = list(test['encoded'])\n",
    "y_pred = predict(test['text'], num_entries_per_cls, log_label_priors, vocab)\n",
    "print(y_true)\n",
    "print(y_pred)\n",
    "score(y_pred, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # compute the probabilities per class per word. Built into predict\n",
    "# log_word_counts = copy.deepcopy(word_counts)\n",
    "# for cls in range(len(word_counts)):\n",
    "#     vocab_for_cls = num_entries_per_cls[cls] + len(vocab)\n",
    "#     print(\"[%d] log denominator for class [%d]\" % (vocab_for_cls, cls))\n",
    "#     for word in log_word_counts[cls]:\n",
    "#         count = word_counts[cls][word]\n",
    "#         # print(\"Word \\\"%s\\\" in cls [%d] has count: %f\" % (word, cls, count))\n",
    "#         log_word_counts[cls][word] = math.log(count / vocab_for_cls)\n",
    "# # log_word_counts[2]['helpful']\n",
    "# # log_word_counts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "~~At this point I have run both models. They both have the same issue where there seems to be not enough training data to accuratly predict the outcome. When a random seed is not set it can predict one or two correctly every now and then. Most of the time though, the accuracy, precision, recall, and F1 scores were all 0. Occasionally the accuracy and recall would rise to 40%. This would raise the precision to 90% and the F1 score to 45.3%~~\n",
    "\n",
    "I left my above note for posterity. During further manual testing I determined that I was performing a numpy maximum argument check on a dictionary. This is slightly undefined and resulted in running the maximum argument check against the keys of the dictionary instead of the values. This is why the model was almost always predicting a value of $0$. After fixing this line the performance of the model increased to a $62.5%$ accuracy. The F1-Score was $0.648$ with the precision and recall being $0.750$ and $0.625$ respectively. Another issue that I fixed with the model was that the number of entries per class variable was recording the number of documents trained for each class instead of the total number of tokens for each class.\n",
    "\n",
    "## Incorrectly Classified Document Example:\n",
    "`NEU It was a little slow, but not too bad.` was supposed to be classified as Neutral, instead it was classified as Negative. To evaluate this manually we need to perform a breakdown of each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    it    was      a little   slow    but    not    too    bad \n",
      "     1      1      1      1      1      1      1      1      1 \n"
     ]
    }
   ],
   "source": [
    "sent = \"It was a little slow, but not too bad.\"\n",
    "sent = clean_string(sent)\n",
    "tkns = nltk.word_tokenize(sent)\n",
    "tkns = nltk.Text([token.lower() for token in tkns])\n",
    "fd = tkns.vocab()\n",
    "fd.tabulate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['        it']\tNEG: 3, NEU: 5, POS: 2\n",
      "['       was']\tNEG: 3, NEU: 3, POS: 3\n",
      "['         a']\tmissing from vocabulary\n",
      "['    little']\tmissing from vocabulary\n",
      "['      slow']\tNEG: 2, NEU: 2, POS: 1\n",
      "['       but']\tNEG: 2, NEU: 3, POS: 1\n",
      "['       not']\tNEG: 2, NEU: 3, POS: 1\n",
      "['       too']\tNEG: 1, NEU: 3, POS: 1\n",
      "['       bad']\tmissing from vocabulary\n",
      "\n",
      "Num Per Cls:\tNEG: 144, NEU: 150, POS: 135\n",
      "vocab size: 89\n"
     ]
    }
   ],
   "source": [
    "for word in fd.keys():\n",
    "    if word not in vocab: \n",
    "        print(\"[\\'%10s\\']\\tmissing from vocabulary\" % word)\n",
    "        continue\n",
    "    print(\"[\\'%10s\\']\\tNEG: %d, NEU: %d, POS: %d\" % (word, word_counts[0][word], word_counts[1][word], word_counts[2][word]))\n",
    "\n",
    "print(\"\\nNum Per Cls:\\tNEG: %d, NEU: %d, POS: %d\" % (num_entries_per_cls[0], num_entries_per_cls[1], num_entries_per_cls[2]))\n",
    "print(\"vocab size: %d\" % (len(vocab)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ignoring the missing words, we are left with the following 'it', 'was', 'slow', 'but', 'not'. According to the frequency distribution each of these words occurs once in the input sentence. The next step is to manually compute the log probability of each word for each class.\n",
    "- it\n",
    "    - NEG: $log(\\frac{3}{141+86})=-1.87890$\n",
    "    - NEU: $log(\\frac{4}{137+86})=-1.74624$\n",
    "    - POS: $log(\\frac{2}{132+86})=-2.03742$\n",
    "- was\n",
    "    - NEG: $log(\\frac{3}{141+86})=-1.87890$\n",
    "    - NEU: $log(\\frac{2}{137+86})=-2.04727$\n",
    "    - POS: $log(\\frac{3}{132+86})=-1.86133$\n",
    "- slow\n",
    "    - NEG: $log(\\frac{2}{141+86})=-2.05499$\n",
    "    - NEU: $log(\\frac{1}{137+86})=-2.34830$\n",
    "    - POS: $log(\\frac{1}{132+86})=-2.33845$\n",
    "- but\n",
    "    - NEG: $log(\\frac{2}{141+86})=-2.05499$\n",
    "    - NEU: $log(\\frac{2}{137+86})=-2.04727$\n",
    "    - POS: $log(\\frac{1}{132+86})=-2.33845$\n",
    "- not\n",
    "    - NEG: $log(\\frac{2}{141+86})=-2.05499$\n",
    "    - NEU: $log(\\frac{1}{137+86})=-2.34830$\n",
    "    - POS: $log(\\frac{1}{132+86})=-2.33845$\n",
    "\n",
    "We now need to sum these probabilities per class\n",
    "- NEG: $2*(-1.87890) + 3*(-2.05499)=-9.92277$\n",
    "- NEU: $(-1.74624)+2*(-2.04727)+2*(-2.34830)=-10.53738$\n",
    "- POS: $(-2.03742)+(-1.86133)+3*(-2.33845)=-10.9141$\n",
    "\n",
    "From these we can see that the NEG class value is the largest. This explains why the algorithm is predicting NEG instead of NEU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['        it']\tNEG: 3, NEU: 5, POS: 2\n",
      "['        it']\tNEG: -4.35, NEU: -3.87, POS: -4.72\n",
      "\n",
      "['       was']\tNEG: 3, NEU: 3, POS: 3\n",
      "['       was']\tNEG: -4.35, NEU: -4.38, POS: -4.31\n",
      "\n",
      "['         a']\tmissing from vocabulary\n",
      "\n",
      "['    little']\tmissing from vocabulary\n",
      "\n",
      "['      slow']\tNEG: 2, NEU: 2, POS: 1\n",
      "['      slow']\tNEG: -4.76, NEU: -4.78, POS: -5.41\n",
      "\n",
      "['       but']\tNEG: 2, NEU: 3, POS: 1\n",
      "['       but']\tNEG: -4.76, NEU: -4.38, POS: -5.41\n",
      "\n",
      "['       not']\tNEG: 2, NEU: 3, POS: 1\n",
      "['       not']\tNEG: -4.76, NEU: -4.38, POS: -5.41\n",
      "\n",
      "['       too']\tNEG: 1, NEU: 3, POS: 1\n",
      "['       too']\tNEG: -5.45, NEU: -4.38, POS: -5.41\n",
      "\n",
      "['       bad']\tmissing from vocabulary\n",
      "\n",
      "\n",
      "Num Per Cls:\tNEG: 144, NEU: 150, POS: 135\n",
      "vocab size: 89\n"
     ]
    }
   ],
   "source": [
    "for word in fd.keys():\n",
    "    if word not in vocab: \n",
    "        print(\"[\\'%10s\\']\\tmissing from vocabulary\\n\" % word)\n",
    "        continue\n",
    "    print(\"[\\'%10s\\']\\tNEG: %d, NEU: %d, POS: %d\" % (word, word_counts[0][word], word_counts[1][word], word_counts[2][word]))\n",
    "    prob_neg = math.log(word_counts[0][word] / (len(vocab) + num_entries_per_cls[0]))\n",
    "    prob_neu = math.log(word_counts[1][word] / (len(vocab) + num_entries_per_cls[1]))\n",
    "    prob_pos = math.log(word_counts[2][word] / (len(vocab) + num_entries_per_cls[2]))\n",
    "    print(\"[\\'%10s\\']\\tNEG: %.2f, NEU: %.2f, POS: %.2f\\n\" % (word, prob_neg, prob_neu, prob_pos))\n",
    "\n",
    "print(\"\\nNum Per Cls:\\tNEG: %d, NEU: %d, POS: %d\" % (num_entries_per_cls[0], num_entries_per_cls[1], num_entries_per_cls[2]))\n",
    "print(\"vocab size: %d\" % (len(vocab)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After adding the sentence \"NEU It was not too fast, but not too slow either.\" the new probabilities are:\n",
    "- NEG: $(-4.35) + (-4.35) + (-4.76) + (-4.76) + (-4.76) + (-5.45) = (-28.43)$\n",
    "- NEU: $(-3.87) + (-4.38) + (-4.78) + (-4.38) + (-4.38) + (-4.38) = (-26.17)$\n",
    "- POS: $(-4.72) + (-4.31) + (-5.41) + (-5.41) + (-5.41) + (-5.41) = (-30.67)$\n",
    "\n",
    "The program now correctly calculates the proper class for the sentence we manually tested. The reasoning behind this specific sentence was that using the word 'not' usually negates whatever is being said. Therefore it would probably be more often in neurtral sentences. Including both 'fast' and 'slow' in the sentence helped to counter eachother out as well. Adding this sentence dramatically improved the model performance. It went from $62.5%$ accuracy to $75.0%$ accuracy. The recall and F1-Score both jumped to $0.750$. The precision jumped to $0.792$.\n",
    "\n",
    "## Adding the MPQA Subjectivity Cues Lexicon to the system\n",
    "The MPQA Subjectivity Cues Lexicon was already loaded above into a pandas dataframe. The next steps are to evaluate how many words there are in each category. There are 5 categories total. 'negative', 'positive', 'neutral', 'both', and 'weakneg'. For simplicity the 'weakneg' will be treated as 'negative'. The 'both' will be treated as 'neutral'. This leaves us our 3 classes we are categorizing against. The following step would be to add the data to our word list and then retrain the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "negative    4912\n",
       "positive    2718\n",
       "neutral      570\n",
       "both          21\n",
       "weakneg        1\n",
       "Name: priorpolarity, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mpqa['priorpolarity'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NEG    4913\n",
       "POS    2718\n",
       "NEU     591\n",
       "Name: priorpolarity, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mpqa['priorpolarity'] = mpqa['priorpolarity'].\\\n",
    "    apply(lambda x: 'neutral' if x == 'both' else x).\\\n",
    "    apply(lambda x: 'negative' if x == 'weakneg' else x).\\\n",
    "    apply(lambda x: 'NEG' if x == 'negative' else x).\\\n",
    "    apply(lambda x: 'NEU' if x == 'neutral' else x).\\\n",
    "    apply(lambda x: 'POS' if x == 'positive' else x)\n",
    "mpqa['priorpolarity'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = copy.deepcopy(data)\n",
    "test = copy.deepcopy(test_data)\n",
    "unique_labels = data['encoded'].unique()\n",
    "\n",
    "for word in list(mpqa[mpqa['priorpolarity'] == 'NEG']['word1']):\n",
    "    train = pd.concat([train, pd.DataFrame.from_records([{\n",
    "        \"cls\": \"NEG\",\n",
    "        \"text\": word,\n",
    "        \"encoded\": 0\n",
    "    }])]).reset_index(drop=True)\n",
    "for word in list(mpqa[mpqa['priorpolarity'] == 'NEU']['word1']):\n",
    "    train = pd.concat([train, pd.DataFrame.from_records([{\n",
    "        \"cls\": \"NEU\",\n",
    "        \"text\": word,\n",
    "        \"encoded\": 1\n",
    "    }])]).reset_index(drop=True)\n",
    "for word in list(mpqa[mpqa['priorpolarity'] == 'POS']['word1']):\n",
    "    train = pd.concat([train, pd.DataFrame.from_records([{\n",
    "        \"cls\": \"POS\",\n",
    "        \"text\": word,\n",
    "        \"encoded\": 2\n",
    "    }])]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{2: 135, 1: 150, 0: 144}\n",
      "{2: 1.9608357992719891, 1: 2.0661963149298153, 0: 2.02537432040956}\n",
      "                The program does what it should do: \tNEG: -20.95, \tNEU: -20.93, \tPOS: -20.82\n",
      "                           It functions adequately: \tNEG: -2.33, \tNEU: -1.80, \tPOS: -2.76\n",
      "                                 The program sucks: \tNEG: -6.80, \tNEU: -6.81, \tPOS: -5.97\n",
      "               This thing runs like a pregnant cow: \tNEG: -7.49, \tNEU: -8.19, \tPOS: -8.17\n",
      "              It was a little slow but not too bad: \tNEG: -26.40, \tNEU: -24.10, \tPOS: -28.72\n",
      "                                   Slow Slow SLOW!: \tNEG: -17.70, \tNEU: -17.76, \tPOS: -18.99\n",
      "                                   Great software!: \tNEG: -7.78, \tNEU: -7.50, \tPOS: -7.48\n",
      "                      Worth the trouble to install: \tNEG: -10.86, \tNEU: -12.98, \tPOS: -11.79\n",
      "[2, 1, 0, 0, 1, 0, 2, 1]\n",
      "[2, 1, 2, 0, 1, 0, 2, 0]\n",
      "[ACCURACY]     : 0.750\n",
      "[PRECISION]    : 0.792\n",
      "[RECALL]       : 0.750\n",
      "[F1-SCORE]     : 0.750\n"
     ]
    }
   ],
   "source": [
    "train = copy.deepcopy(data)\n",
    "test = copy.deepcopy(test_data)\n",
    "unique_labels = data['encoded'].unique()\n",
    "\n",
    "def gen_word_counts_with_laplace(dataframe: pd.DataFrame):\n",
    "    # get a list of all of the word keys\n",
    "    all_tokens = nltk.word_tokenize('\\n'.join(dataframe['text']))\n",
    "    all_text = nltk.Text([token.lower() for token in all_tokens])\n",
    "    all_freq_dist = all_text.vocab()\n",
    "    all_keys = all_freq_dist.keys()\n",
    "    word_counts = {}\n",
    "    # iterate through each of the label classes\n",
    "    for cls in unique_labels:\n",
    "        # for each label class compute the word frequency distribution\n",
    "        text = '\\n'.join(dataframe[dataframe['encoded'] == cls]['text'])\n",
    "        # print(\"CLS [%d]: %s\" % (cls, text))\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        tokens = [token.lower() for token in tokens]\n",
    "        n_text = nltk.Text(tokens)\n",
    "        fd = n_text.vocab()\n",
    "        dictionary = {}\n",
    "        # use the list of all keys and frequency distribution to make\n",
    "        # a frequency distribution for the class with all keys\n",
    "        # and laplacian smoothing\n",
    "        for key in all_keys:\n",
    "            count = fd[key] + 1 # add 1 for laplace smoothing\n",
    "            dictionary[key] = count\n",
    "        # add the distribution to the list for output\n",
    "        word_counts[cls] = dictionary\n",
    "    return word_counts, all_keys\n",
    "\n",
    "def fit(dataframe: pd.DataFrame):\n",
    "    num_entries_per_cls = {}\n",
    "    log_label_priors = {}\n",
    "    n = len(list(dataframe['text']))\n",
    "    grouped = [None] * len(unique_labels)\n",
    "    for cls in dataframe['encoded'].unique():\n",
    "        grouped[cls] = list(dataframe['text'][dataframe['encoded'] == cls])\n",
    "        # num_entries_per_cls[cls] = len(grouped[cls])\n",
    "        num_entries_per_cls[cls] = sum(word_counts[cls].values())\n",
    "        log_label_priors[cls] = math.log(num_entries_per_cls[cls] / n)\n",
    "    return(num_entries_per_cls, log_label_priors)\n",
    "\n",
    "def predict(x: list[str], num_entries_per_cls, log_label_priors, vocab):\n",
    "    result = []\n",
    "    for text in x:\n",
    "        label_scores = copy.deepcopy(log_label_priors)\n",
    "        text = clean_string(text)\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        tokens = [token.lower() for token in tokens]\n",
    "        # fd = nltk.Text(tokens).vocab()\n",
    "        for word in tokens:\n",
    "            if word not in vocab: \n",
    "                # print(\"[%s] missing from vocabulary\" % word)\n",
    "                continue\n",
    "            for cls in unique_labels:\n",
    "                count = word_counts[cls][word]\n",
    "                n_vocab_per_cls = num_entries_per_cls[cls] + len(vocab)\n",
    "                label_scores[cls] += math.log(count / n_vocab_per_cls)\n",
    "        # print(label_scores)\n",
    "        print(\"%50s: \\tNEG: %.2f, \\tNEU: %.2f, \\tPOS: %.2f\" % (text, label_scores[0], label_scores[1], label_scores[2]))\n",
    "        result.append(max(label_scores, key=lambda key: label_scores[key])) # ISSUE WAS HERE DUE TO USING np.argmax on a dict.\n",
    "    return result\n",
    "\n",
    "word_counts, vocab = gen_word_counts_with_laplace(train)\n",
    "\n",
    "num_entries_per_cls, log_label_priors = fit(train)\n",
    "print(num_entries_per_cls)\n",
    "print(log_label_priors)\n",
    "y_true = list(test['encoded'])\n",
    "y_pred = predict(test['text'], num_entries_per_cls, log_label_priors, vocab)\n",
    "print(y_true)\n",
    "print(y_pred)\n",
    "score(y_pred, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: [the], Polarity: Series([], Name: priorpolarity, dtype: object)\n",
      "Word: [program], Polarity: Series([], Name: priorpolarity, dtype: object)\n",
      "Word: [does], Polarity: Series([], Name: priorpolarity, dtype: object)\n",
      "Word: [what], Polarity: Series([], Name: priorpolarity, dtype: object)\n",
      "Word: [it], Polarity: Series([], Name: priorpolarity, dtype: object)\n",
      "Word: [should], Polarity: 6665    NEU\n",
      "Name: priorpolarity, dtype: object\n",
      "Word: [do], Polarity: Series([], Name: priorpolarity, dtype: object)\n"
     ]
    }
   ],
   "source": [
    "print(\"Word: [%s], Polarity: %s\" % ('the', mpqa[mpqa['word1'] == 'the']['priorpolarity']))\n",
    "print(\"Word: [%s], Polarity: %s\" % ('program', mpqa[mpqa['word1'] == 'program']['priorpolarity']))\n",
    "print(\"Word: [%s], Polarity: %s\" % ('does', mpqa[mpqa['word1'] == 'does']['priorpolarity']))\n",
    "print(\"Word: [%s], Polarity: %s\" % ('what', mpqa[mpqa['word1'] == 'what']['priorpolarity']))\n",
    "print(\"Word: [%s], Polarity: %s\" % ('it', mpqa[mpqa['word1'] == 'it']['priorpolarity']))\n",
    "print(\"Word: [%s], Polarity: %s\" % ('should', mpqa[mpqa['word1'] == 'should']['priorpolarity']))\n",
    "print(\"Word: [%s], Polarity: %s\" % ('do', mpqa[mpqa['word1'] == 'do']['priorpolarity']))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the lexicon ended up hurting my score. This is likely due to how I added the lexicon to the program. Document 1 used to be classified correctly as a positive document. Now that the lexicon was added it is now classified as a neutral document. The document in question is \"POS The program does what it should do.\" After checking the mpqa for the words in that document it appears that the inclusion of the word 'should' under 'Neutral' ended up changing the classification. This is in addition to the increase in words for each class as well with the 'Neutral' class not having as many words added to it.\n",
    "\n",
    "## Amazon Reviews Documents\n",
    "The last step is to use the colleciton of amazon reviews documents to train the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cls</th>\n",
       "      <th>text</th>\n",
       "      <th>encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>__label__2</td>\n",
       "      <td>Stuning even for the non-gamer This sound trac...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>__label__2</td>\n",
       "      <td>The best soundtrack ever to anything I am read...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>__label__2</td>\n",
       "      <td>Amazing! This soundtrack is my favorite music ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>__label__2</td>\n",
       "      <td>Excellent Soundtrack I truly like this soundtr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>__label__2</td>\n",
       "      <td>Remember Pull Your Jaw Off The Floor After Hea...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3599995</th>\n",
       "      <td>__label__1</td>\n",
       "      <td>Do not do it!! The high chair looks great when...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3599996</th>\n",
       "      <td>__label__1</td>\n",
       "      <td>Looks nice low functionality I have used this ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3599997</th>\n",
       "      <td>__label__1</td>\n",
       "      <td>compact but hard to clean We have a small hous...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3599998</th>\n",
       "      <td>__label__1</td>\n",
       "      <td>what is it saying not sure what this book is s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3599999</th>\n",
       "      <td>__label__2</td>\n",
       "      <td>Makes My Blood Run Red-White-And-Blue I agree ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3600000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                cls                                               text encoded\n",
       "0        __label__2  Stuning even for the non-gamer This sound trac...       1\n",
       "1        __label__2  The best soundtrack ever to anything I am read...       1\n",
       "2        __label__2  Amazing! This soundtrack is my favorite music ...       1\n",
       "3        __label__2  Excellent Soundtrack I truly like this soundtr...       1\n",
       "4        __label__2  Remember Pull Your Jaw Off The Floor After Hea...       1\n",
       "...             ...                                                ...     ...\n",
       "3599995  __label__1  Do not do it!! The high chair looks great when...       0\n",
       "3599996  __label__1  Looks nice low functionality I have used this ...       0\n",
       "3599997  __label__1  compact but hard to clean We have a small hous...       0\n",
       "3599998  __label__1  what is it saying not sure what this book is s...       0\n",
       "3599999  __label__2  Makes My Blood Run Red-White-And-Blue I agree ...       1\n",
       "\n",
       "[3600000 rows x 3 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Read in the training data\n",
    "training = []\n",
    "strings: list[str] = []\n",
    "filename = \"Amazon Reviews\\\\train.ft.txt\"\n",
    "with open(filename, mode=\"r\", encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        tokens = line.split(\" \")\n",
    "        classification = tokens.pop(0)\n",
    "        x = clean_string(str(line[line.index(\" \") + 1:-1]))\n",
    "        strings.append(x)\n",
    "        training +=[(classification, x)]\n",
    "text = '\\n'.join(strings)\n",
    "data = pd.DataFrame(training, columns=['cls', 'text'])\n",
    "data['cls'] = data['cls'].astype(\"category\")\n",
    "data['encoded'] = encoder.fit_transform(data['cls'])\n",
    "data['encoded'] = data['encoded'].astype(\"category\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cls</th>\n",
       "      <th>text</th>\n",
       "      <th>encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>__label__2</td>\n",
       "      <td>Great CD My lovely Pat has one of the GREAT vo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>__label__2</td>\n",
       "      <td>One of the best game music soundtracks - for a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>__label__1</td>\n",
       "      <td>Batteries died within a year  I bought this ch...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>__label__2</td>\n",
       "      <td>works fine but Maha Energy is better Check out...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>__label__2</td>\n",
       "      <td>Great for the non-audiophile Reviewed quite a ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399995</th>\n",
       "      <td>__label__1</td>\n",
       "      <td>Unbelievable- In a Bad Way We bought this Thom...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399996</th>\n",
       "      <td>__label__1</td>\n",
       "      <td>Almost Great Until it Broke My son recieved th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399997</th>\n",
       "      <td>__label__1</td>\n",
       "      <td>Disappointed !!! I bought this toy for my son ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399998</th>\n",
       "      <td>__label__2</td>\n",
       "      <td>Classic Jessica Mitford This is a compilation ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399999</th>\n",
       "      <td>__label__1</td>\n",
       "      <td>Comedy Scene and Not Heard This DVD will be a ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               cls                                               text encoded\n",
       "0       __label__2  Great CD My lovely Pat has one of the GREAT vo...       1\n",
       "1       __label__2  One of the best game music soundtracks - for a...       1\n",
       "2       __label__1  Batteries died within a year  I bought this ch...       0\n",
       "3       __label__2  works fine but Maha Energy is better Check out...       1\n",
       "4       __label__2  Great for the non-audiophile Reviewed quite a ...       1\n",
       "...            ...                                                ...     ...\n",
       "399995  __label__1  Unbelievable- In a Bad Way We bought this Thom...       0\n",
       "399996  __label__1  Almost Great Until it Broke My son recieved th...       0\n",
       "399997  __label__1  Disappointed !!! I bought this toy for my son ...       0\n",
       "399998  __label__2  Classic Jessica Mitford This is a compilation ...       1\n",
       "399999  __label__1  Comedy Scene and Not Heard This DVD will be a ...       0\n",
       "\n",
       "[400000 rows x 3 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Read in the testing data\n",
    "training = []\n",
    "strings: list[str] = []\n",
    "filename = \"Amazon Reviews\\\\test.ft.txt\"\n",
    "with open(filename, mode=\"r\", encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        tokens = line.split(\" \")\n",
    "        classification = tokens.pop(0)\n",
    "        x = clean_string(str(line[line.index(\" \") + 1:-1]))\n",
    "        strings.append(x)\n",
    "        training +=[(classification, x)]\n",
    "text = '\\n'.join(strings)\n",
    "test_data = pd.DataFrame(training, columns=['cls', 'text'])\n",
    "test_data['cls'] = test_data['cls'].astype(\"category\")\n",
    "test_data['encoded'] = encoder.transform(test_data['cls'])\n",
    "test_data['encoded'] = test_data['encoded'].astype(\"category\")\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 144870616, 0: 156199131}\n",
      "{1: 3.6949071951705346, 0: 3.7701978285477638}\n",
      "[ACCURACY]     : 0.850\n",
      "[PRECISION]    : 0.851\n",
      "[RECALL]       : 0.850\n",
      "[F1-SCORE]     : 0.850\n"
     ]
    }
   ],
   "source": [
    "train = copy.deepcopy(data)\n",
    "test = copy.deepcopy(test_data)\n",
    "unique_labels = data['encoded'].unique()\n",
    "\n",
    "def gen_word_counts_with_laplace(dataframe: pd.DataFrame):\n",
    "    # get a list of all of the word keys\n",
    "    all_tokens = nltk.word_tokenize('\\n'.join(dataframe['text']))\n",
    "    all_text = nltk.Text([token.lower() for token in all_tokens])\n",
    "    all_freq_dist = all_text.vocab()\n",
    "    all_keys = all_freq_dist.keys()\n",
    "    word_counts = {}\n",
    "    # iterate through each of the label classes\n",
    "    for cls in unique_labels:\n",
    "        # for each label class compute the word frequency distribution\n",
    "        text = '\\n'.join(dataframe[dataframe['encoded'] == cls]['text'])\n",
    "        # print(\"CLS [%d]: %s\" % (cls, text))\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        tokens = [token.lower() for token in tokens]\n",
    "        n_text = nltk.Text(tokens)\n",
    "        fd = n_text.vocab()\n",
    "        dictionary = {}\n",
    "        # use the list of all keys and frequency distribution to make\n",
    "        # a frequency distribution for the class with all keys\n",
    "        # and laplacian smoothing\n",
    "        for key in all_keys:\n",
    "            count = fd[key] + 1 # add 1 for laplace smoothing\n",
    "            dictionary[key] = count\n",
    "        # add the distribution to the list for output\n",
    "        word_counts[cls] = dictionary\n",
    "    return word_counts, all_keys\n",
    "\n",
    "def fit(dataframe: pd.DataFrame):\n",
    "    num_entries_per_cls = {}\n",
    "    log_label_priors = {}\n",
    "    n = len(list(dataframe['text']))\n",
    "    grouped = [None] * len(unique_labels)\n",
    "    for cls in dataframe['encoded'].unique():\n",
    "        grouped[cls] = list(dataframe['text'][dataframe['encoded'] == cls])\n",
    "        # num_entries_per_cls[cls] = len(grouped[cls])\n",
    "        num_entries_per_cls[cls] = sum(word_counts[cls].values())\n",
    "        log_label_priors[cls] = math.log(num_entries_per_cls[cls] / n)\n",
    "    return(num_entries_per_cls, log_label_priors)\n",
    "\n",
    "def predict(x: list[str], num_entries_per_cls, log_label_priors, vocab):\n",
    "    result = []\n",
    "    for text in x:\n",
    "        label_scores = copy.deepcopy(log_label_priors)\n",
    "        text = clean_string(text)\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        tokens = [token.lower() for token in tokens]\n",
    "        # fd = nltk.Text(tokens).vocab()\n",
    "        for word in tokens:\n",
    "            if word not in vocab: \n",
    "                # print(\"[%s] missing from vocabulary\" % word)\n",
    "                continue\n",
    "            for cls in unique_labels:\n",
    "                count = word_counts[cls][word]\n",
    "                n_vocab_per_cls = num_entries_per_cls[cls] + len(vocab)\n",
    "                label_scores[cls] += math.log(count / n_vocab_per_cls)\n",
    "        # print(label_scores)\n",
    "        # print(\"%50s: \\tNEG: %.2f, \\tNEU: %.2f, \\tPOS: %.2f\" % (text, label_scores[0], label_scores[1], label_scores[2]))\n",
    "        result.append(max(label_scores, key=lambda key: label_scores[key])) # ISSUE WAS HERE DUE TO USING np.argmax on a dict.\n",
    "    return result\n",
    "\n",
    "word_counts, vocab = gen_word_counts_with_laplace(train)\n",
    "num_entries_per_cls, log_label_priors = fit(train)\n",
    "print(num_entries_per_cls)\n",
    "print(log_label_priors)\n",
    "y_true = list(test['encoded'])\n",
    "y_pred = predict(test['text'], num_entries_per_cls, log_label_priors, vocab)\n",
    "# print(y_true)\n",
    "# print(y_pred)\n",
    "score(y_pred, y_true)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Amazon reviews dataset achieved a $85.0%$ accuracy. It also got a $0.850$ recall and F1-Score. The precision was slightly higher at $0.851$. Note that this cell take about 50min to execute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
