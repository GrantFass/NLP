{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3\n",
    "## Warmup\n",
    "Given the following short movie reviews, each labeled with a genre, either comedy or action:\n",
    "1. fun, couple, love, love **comedy**\n",
    "2. fast, furious, shoot **action**\n",
    "3. couple, fly, fast, fun, fun **comedy**\n",
    "4. furious, shoot, shoot, fun **action**\n",
    "5. fly, fast, shoot, love **action**\n",
    "\n",
    "and a new document D:\n",
    "\n",
    "    fast, couple, shoot, fly\n",
    "\n",
    "compute the most likely class for D. Assume a naive Bayes classifier and use add-1 smoothing for the likelihoods.\n",
    "\n",
    "## Warmup Response\n",
    "TODO\n",
    "\n",
    "## Assignment\n",
    "Build a naive Bayes sentiment classifier that will assign reviews of an application as either **positive**, **neutral**, or **negative**.\n",
    "- You will need to do some basic preprocessing on the documents (normalization, etc).\n",
    "- Do not use a stop word list.\n",
    "- Ignore any Out-Of-Vocabulary (OOV) terms when classifying.\n",
    "\n",
    "You are provided a small set of pre-classified training data to build your model. The data is formatted such that each line of text contains a document (the title of a review). The first token of each line will be the classification of that review, either **POS**, **NEU**, or **NEG**. Below is a sample document:\n",
    "    \n",
    "    `POS The program was quite helpful with creating websites.`\n",
    "\n",
    "An example output of your system may look something like this:\n",
    "\n",
    "    ```\n",
    "    The program does what it should do. : POSITIVE\n",
    "    It functions adequately. : NEUTRAL\n",
    "    The program sucks. : NEGATIVE\n",
    "    This thing runs like a pregnant cow. : NEGATIVE\n",
    "    It was a little slow, but not too bad. : NEUTRAL\n",
    "    Slow. Slow. SLOW! : NEGATIVE\n",
    "    Great software! : POSITIVE\n",
    "    Worth the trouble to install. : NEUTRAL\n",
    "    ```\n",
    "\n",
    "## Report Instructions\n",
    "Once the model has been built, feed in the provided test documents and write a report detailing your results. In the report, address the following:\n",
    "- How accurate was the classifier? What was the Precision and Recall? The F-measure?\n",
    "- Choose one incorrectly classified document.\n",
    "    - Manually calculate the sentiment probabilities for the document (you can use your classifier to generate the likelihoods and prior probabilities, but do the classifying on paper)\n",
    "    - What is the difference of the probability sums of the correct class and the class assigned to the system?\n",
    "    - Identify the term or terms that caused the system to misclassify the document.\n",
    "    - Build a document (or documents) to add to the training set that would allow the system to correctly classify the document.\n",
    "        - Show the mathematical reasoning for your choice of words in the document.\n",
    "        - Rerun the tests with the additional information.\n",
    "        - Did adding the additional information change any other document classification? If so, how? Did it improve the overall accuracy of your system or make it worse?\n",
    "    - Add the MPQA Subjectivity Cues Lexicon to your system and run the tests again and report the results.\n",
    "        - Choose a document that was classified differently after adding the MPQA Subjectivity Cues Lexicon. Was it correctly or incorrectly classified? Discuss why.\n",
    "    - Finally use the provided collection of Amazon reviews from 2007 to train your classifier. Run the associated tests and report the Precision, Recall, and F-measure.\n",
    "    - Briefly discuss what you learned from this assignment, what you liked or disliked about the assignment and, optionally, anything you would like to see changed or added to improve the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package names to\n",
      "[nltk_data]     C:\\Users\\Grant\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package names is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Grant\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package state_union to\n",
      "[nltk_data]     C:\\Users\\Grant\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package state_union is already up-to-date!\n",
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     C:\\Users\\Grant\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n",
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\Grant\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Grant\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Grant\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Grant\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "## imports\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import contractions\n",
    "import unidecode\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "nltk.download([\n",
    "\"names\",\n",
    "\"stopwords\",\n",
    "\"state_union\",\n",
    "\"twitter_samples\",\n",
    "\"movie_reviews\",\n",
    "\"averaged_perceptron_tagger\",\n",
    "\"vader_lexicon\",\n",
    "\"punkt\",\n",
    "])\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "encoder = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cls</th>\n",
       "      <th>text</th>\n",
       "      <th>encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>POS</td>\n",
       "      <td>The program was quite helpful with creating we...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>POS</td>\n",
       "      <td>I really, really, really liked the cute icons!</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NEU</td>\n",
       "      <td>The program did its job, but nothing special.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NEG</td>\n",
       "      <td>Why did they even bother releasing this software?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NEG</td>\n",
       "      <td>This program did not do anything it was promis...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NEU</td>\n",
       "      <td>The software was adequate.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NEU</td>\n",
       "      <td>I have used better programs, I have used worse.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>POS</td>\n",
       "      <td>The pages it generated were just what I needed.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>POS</td>\n",
       "      <td>The software was intuitive and easy to use</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>POS</td>\n",
       "      <td>The program runs well on my laptop</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NEG</td>\n",
       "      <td>It was slow, buggy, and painful to use.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>NEG</td>\n",
       "      <td>This is the worst piece of garbage I have ever...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>NEG</td>\n",
       "      <td>I want my money back.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>POS</td>\n",
       "      <td>Best money I ever spent.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NEU</td>\n",
       "      <td>It is cheap software, and you get what you pay...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>NEU</td>\n",
       "      <td>The software generates web pages based on inpu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>NEU</td>\n",
       "      <td>It runs on all major platforms, including iOS.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>NEG</td>\n",
       "      <td>I had nothing but trouble with the software</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    cls                                               text encoded\n",
       "0   POS  The program was quite helpful with creating we...       2\n",
       "1   POS     I really, really, really liked the cute icons!       2\n",
       "2   NEU      The program did its job, but nothing special.       1\n",
       "3   NEG  Why did they even bother releasing this software?       0\n",
       "4   NEG  This program did not do anything it was promis...       0\n",
       "5   NEU                         The software was adequate.       1\n",
       "6   NEU    I have used better programs, I have used worse.       1\n",
       "7   POS    The pages it generated were just what I needed.       2\n",
       "8   POS         The software was intuitive and easy to use       2\n",
       "9   POS                 The program runs well on my laptop       2\n",
       "10  NEG            It was slow, buggy, and painful to use.       0\n",
       "11  NEG  This is the worst piece of garbage I have ever...       0\n",
       "12  NEG                              I want my money back.       0\n",
       "13  POS                           Best money I ever spent.       2\n",
       "14  NEU  It is cheap software, and you get what you pay...       1\n",
       "15  NEU  The software generates web pages based on inpu...       1\n",
       "16  NEU     It runs on all major platforms, including iOS.       1\n",
       "17  NEG        I had nothing but trouble with the software       0"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Read in the training data\n",
    "training = []\n",
    "strings: list[str] = []\n",
    "filename = \"trainingSet.txt\"\n",
    "with open(filename, \"r\") as f:\n",
    "    for line in f:\n",
    "        tokens = line.split(\" \")\n",
    "        classification = tokens.pop(0)\n",
    "        x = str(line[line.index(\" \") + 1:-1])\n",
    "        x = contractions.fix(x) # expand contractions\n",
    "        x = unidecode.unidecode(x) # remove accents\n",
    "        x = ' '.join(x.strip().split()) # remove extra whitespace\n",
    "        # # could do lemmatization using the WordNetLemmatizer\n",
    "        # x = ' '.join([lemmatizer.lemmatize(word) for word in x.split()])\n",
    "        strings.append(x)\n",
    "        training +=[(classification, x)]\n",
    "text = '\\n'.join(strings)\n",
    "data = pd.DataFrame(training, columns=['cls', 'text'])\n",
    "data['cls'] = data['cls'].astype(\"category\")\n",
    "data['encoded'] = encoder.fit_transform(data['cls'])\n",
    "data['encoded'] = data['encoded'].astype(\"category\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       .      the        i        , software       it      was \n",
      "      13       11        8        8        6        6        5 \n",
      "None\n",
      "Displaying 1 of 1 matches:\n",
      "rogram runs well on my laptop it was slow , buggy , and painful to use . this \n"
     ]
    }
   ],
   "source": [
    "# ## Build Frequency Distribution\n",
    "# fd = nltk.FreqDist(words)\n",
    "# print(fd)\n",
    "# print(fd.most_common(3))\n",
    "# print(fd.tabulate(3))\n",
    "\n",
    "## Better method of making frequency distribution\n",
    "tokens = nltk.word_tokenize(text)\n",
    "tokens = [token.lower() for token in tokens]\n",
    "n_text = nltk.Text(tokens)\n",
    "fd = n_text.vocab()\n",
    "print(fd.tabulate(7))\n",
    "n_text.concordance(\"slow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 0.0, 'neu': 0.519, 'pos': 0.481, 'compound': 0.6764}\n",
      "{'neg': 0.0, 'neu': 0.41, 'pos': 0.59, 'compound': 0.8015}\n",
      "{'neg': 0.292, 'neu': 0.708, 'pos': 0.0, 'compound': -0.438}\n",
      "{'neg': 0.255, 'neu': 0.745, 'pos': 0.0, 'compound': -0.34}\n",
      "{'neg': 0.0, 'neu': 0.8, 'pos': 0.2, 'compound': 0.3612}\n",
      "{'neg': 0.0, 'neu': 0.612, 'pos': 0.388, 'compound': 0.2263}\n",
      "{'neg': 0.282, 'neu': 0.455, 'pos': 0.264, 'compound': -0.0516}\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "{'neg': 0.0, 'neu': 0.707, 'pos': 0.293, 'compound': 0.4404}\n",
      "{'neg': 0.0, 'neu': 0.741, 'pos': 0.259, 'compound': 0.2732}\n",
      "{'neg': 0.293, 'neu': 0.707, 'pos': 0.0, 'compound': -0.4404}\n",
      "{'neg': 0.24, 'neu': 0.76, 'pos': 0.0, 'compound': -0.6249}\n",
      "{'neg': 0.0, 'neu': 0.698, 'pos': 0.302, 'compound': 0.0772}\n",
      "{'neg': 0.0, 'neu': 0.417, 'pos': 0.583, 'compound': 0.6369}\n",
      "{'neg': 0.123, 'neu': 0.877, 'pos': 0.0, 'compound': -0.1027}\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "{'neg': 0.0, 'neu': 0.675, 'pos': 0.325, 'compound': 0.438}\n"
     ]
    }
   ],
   "source": [
    "for string in strings:\n",
    "    print(sia.polarity_scores(string))\n",
    "# at a glance this seems to be performing poorly"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try building Naive Bayes Classifier\n",
    "Following the tutorial [Building Naive Bayes Classifier from Scratch to Perform Sentiment Analysis](https://www.analyticsvidhya.com/blog/2022/03/building-naive-bayes-classifier-from-scratch-to-perform-sentiment-analysis/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_tokenizer = nltk.tokenize.NLTKWordTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the train_test_split\n",
    "texts = data['text'].values\n",
    "labels = data['encoded'].values\n",
    "train_x, test_x, train_y, test_y = train_test_split(texts, \n",
    "                                                    labels, \n",
    "                                                    stratify=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = CountVectorizer(max_features = 5000)\n",
    "x = vec.fit_transform(train_x)\n",
    "vocab = vec.get_feature_names_out()\n",
    "x = x.toarray()\n",
    "word_counts = {}\n",
    "for l in range(3):\n",
    "    word_counts[l] = defaultdict(lambda: 0)\n",
    "for i in range(x.shape[0]):\n",
    "    l = train_y[i]\n",
    "    for j in range(len(vocab)):\n",
    "        word_counts[l][vocab[j]] += x[i][j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "so the word counts contains the count of words that occured in\n",
    "the sentences. The only difference here is that it is stratified\n",
    "based on the label. This means it has 3 separate sets of counts. \n",
    "Note that each set of counts still contains the words from the\n",
    "other sets just with a value of 0.\n",
    "\n",
    "the first is the encoded class. The second is the word to lookup.\n",
    "\"\"\"\n",
    "word_counts[2]['all']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def laplace_smoothing(n_label_items, vocab, word_counts, word, text_label):\n",
    "    a = word_counts[text_label][word] + 1\n",
    "    b = n_label_items[text_label] + len(vocab)\n",
    "    return math.log(a/b)\n",
    "\n",
    "\n",
    "def group_by_label(x, y, labels):\n",
    "    data = {}\n",
    "    for l in labels:\n",
    "        data[l] = x[np.where(y == l)]\n",
    "    return data\n",
    " \n",
    "\n",
    "def fit(x, y, labels):\n",
    "    n_label_items = {}\n",
    "    log_label_priors = {}\n",
    "    n = len(x)\n",
    "    grouped_data = group_by_label(x, y, labels)\n",
    "    for l, data in grouped_data.items():\n",
    "        n_label_items[l] = len(data)\n",
    "        log_label_priors[l] = math.log(n_label_items[l] / n)\n",
    "    return n_label_items, log_label_priors\n",
    "\n",
    "def predict(n_label_items, vocab, word_counts, log_label_priors, labels, x):\n",
    "    result = []\n",
    "    for text in x:\n",
    "        label_scores = {l: log_label_priors[l] for l in labels}\n",
    "        words = set(w_tokenizer.tokenize(text))\n",
    "        for word in words:\n",
    "            if word not in vocab: continue\n",
    "            for l in labels:\n",
    "                log_w_given_l = laplace_smoothing(n_label_items, vocab, word_counts, word, l)\n",
    "                label_scores[l] += log_w_given_l\n",
    "        result.append(max(label_scores, key=label_scores.get))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of prediction on test set :  0.2\n",
      "[1, 2, 0, 0, 2]\n",
      "[2, 0, 0, 1, 1]\n",
      "Categories (3, int64): [0, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "labels = [0,1,2]\n",
    "n_label_items, log_label_priors = fit(train_x,train_y,labels)\n",
    "pred = predict(n_label_items, vocab, word_counts, log_label_priors, labels, test_x)\n",
    "print(\"Accuracy of prediction on test set : \", accuracy_score(test_y,pred))\n",
    "print(pred)\n",
    "print(test_y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes\n",
    "\n",
    "    The program does what it should do. : POSITIVE\n",
    "    It functions adequately. : NEUTRAL\n",
    "    The program sucks. : NEGATIVE\n",
    "    This thing runs like a pregnant cow. : NEGATIVE\n",
    "    It was a little slow, but not too bad. : NEUTRAL\n",
    "    Slow. Slow. SLOW! : NEGATIVE\n",
    "    Great software! : POSITIVE\n",
    "    Worth the trouble to install. : NEUTRAL\n",
    "\n",
    "ignore terms not in vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(data, \n",
    "                               test_size=0.2, \n",
    "                               random_state=42, \n",
    "                               shuffle=True,\n",
    "                               stratify=data['encoded'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def gen_word_counts_with_laplace(dataframe: pd.DataFrame):\n",
    "    # get a list of all of the word keys\n",
    "    all_tokens = nltk.word_tokenize('\\n'.join(dataframe['text']))\n",
    "    all_text = nltk.Text([token.lower() for token in all_tokens])\n",
    "    all_freq_dist = all_text.vocab()\n",
    "    all_keys = all_freq_dist.keys()\n",
    "    word_counts = [None] * len(dataframe['encoded'].unique())\n",
    "    # iterate through each of the label classes\n",
    "    for cls in dataframe['encoded'].unique():\n",
    "        # for each label class compute the word frequency distribution\n",
    "        text = '\\n'.join(dataframe[dataframe['encoded'] == cls]['text'])\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        tokens = [token.lower() for token in tokens]\n",
    "        n_text = nltk.Text(tokens)\n",
    "        fd = n_text.vocab()\n",
    "        dictionary = {}\n",
    "        # use the list of all keys and frequency distribution to make\n",
    "        # a frequency distribution for the class with all keys\n",
    "        # and laplacian smoothing\n",
    "        for key in all_keys:\n",
    "            count = fd[key] + 1 # add 1 for laplace smoothing\n",
    "            dictionary[key] = count\n",
    "        # add the distribution to the list for output\n",
    "        word_counts[cls] = dictionary\n",
    "    return word_counts\n",
    "word_counts = gen_word_counts_with_laplace(train)\n",
    "word_counts[2]['all']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([5, 5, 4], [-0.9555114450274363, -0.9555114450274363, -1.1786549963416462])"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fit(dataframe: pd.DataFrame):\n",
    "    num_entries_per_cls = [None] * len(dataframe['encoded'].unique())\n",
    "    log_label_priors = [None] * len(dataframe['encoded'].unique())\n",
    "    n = len(x)\n",
    "    grouped = [None] * len(dataframe['encoded'].unique())\n",
    "    for cls in dataframe['encoded'].unique():\n",
    "        grouped[cls] = list(dataframe['text'][dataframe['encoded'] == cls])\n",
    "        num_entries_per_cls[cls] = len(grouped[cls])\n",
    "        log_label_priors[cls] = math.log(num_entries_per_cls[cls] / n)\n",
    "    return(num_entries_per_cls, log_label_priors)\n",
    "\n",
    "fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
